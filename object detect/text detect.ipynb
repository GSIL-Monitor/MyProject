{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/skyfsm/p/6806246.html 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN\n",
    "    \n",
    "    https://blog.csdn.net/u013293750/article/details/64904681 CNN+LSTM深度学习文字检测\n",
    "    \n",
    "    https://blog.csdn.net/forest_world/article/details/78566737 主流ocr算法：CNN+BLSTM+CTC架构\n",
    "    \n",
    "    https://blog.csdn.net/slade_ruan/article/details/78301842?utm_source=blogxgwz1 场景文本检测，CTPN tensorflow版本\n",
    "    \n",
    "    https://blog.csdn.net/Quincuntial/article/details/79475339?utm_source=blogxgwz1 CTPN论文翻译——中英文对照\n",
    "    \n",
    "    http://lib.csdn.net/article/deeplearning/61632  通过代码理解faster-RCNN中的RPN\n",
    "    \n",
    "    https://slade-ruan.me/2017/10/22/text-detection-ctpn/  论文阅读与实现--CTPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/freeweb/p/6548208.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://deepsense.ai/region-of-interest-pooling-in-tensorflow-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/king-lps/p/9031568.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_train\n",
      "voc_2007_val\n",
      "voc_2007_trainval\n",
      "voc_2007_test\n",
      "<bound method imdb.default_roidb of <lib.datasets.pascal_voc.pascal_voc object at 0x000002B5D45EF390>>\n",
      "voc_2007_trainval gt roidb loaded from D:\\PROJECT_TW\\git\\data\\voc_2007_trainval_gt_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "from lib.datasets.factory import get_imdb\n",
    "from lib.datasets.pascal_voc import pascal_voc\n",
    "from lib.roi_data_layer.roidb import prepare_roidb\n",
    "from lib.roi_data_layer.layer import RoIDataLayer\n",
    "\n",
    "\n",
    "imdb = pascal_voc('trainval', '2007')\n",
    "# roidb ROI框的坐标位置信息, 信息来源于Annotations目录下对图片的XML定义\n",
    "prepare_roidb(imdb)   #  为方便训练，在原roidb信息基础上增加象image等等信息\n",
    "roidb = imdb.roidb \n",
    "data_layer = RoIDataLayer(roidb, imdb.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     39
    ]
   },
   "outputs": [],
   "source": [
    "RPN_CHANNELS = 512\n",
    "TRUNCATED = False\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._gt_image = None\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "\n",
    "    def create_architecture(self, num_classes, tag=None,anchor_scales=(16,), anchor_ratios=(0.5, 1, 2)):\n",
    "        self._tag = tag\n",
    "        self._num_classes = num_classes\n",
    "        self._anchor_scales = anchor_scales\n",
    "        self._num_scales = len(anchor_scales)\n",
    "        self._anchor_ratios = anchor_ratios\n",
    "        self._num_ratios = len(anchor_ratios)\n",
    "        self._num_anchors = 10\n",
    "        assert tag != None\n",
    "        # Initialize layers\n",
    "        self._init_modules()\n",
    "        \n",
    "    def _init_modules(self):\n",
    "        self._init_head_tail()\n",
    "        # rpn\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, RPN_CHANNELS, [3, 3], padding=1)\n",
    "        self.rpn_bi_net = nn.LSTM(RPN_CHANNELS, 256, batch_first=True, bidirectional=True)\n",
    "        self.rpn_cls_score_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 2, batch_first=True, bidirectional=False)\n",
    "        self.rpn_bbox_pred_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 4, batch_first=True, bidirectional=False)\n",
    "        self.init_weights()    \n",
    "        \n",
    "    # 对构建的网络参数（weight, bias）进行正则、初始化\n",
    "    def init_weights(self):\n",
    "        def normal_init(m, mean, stddev, truncated=False):\n",
    "            \"\"\"\n",
    "                weight initalizer: truncated normal and random normal.\n",
    "            \"\"\"\n",
    "            # x is a parameter\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                init.xavier_normal_(m.all_weights[0][0])\n",
    "                init.xavier_normal_(m.all_weights[0][1])\n",
    "                if len(m.all_weights) == 2:   # 双向  LSTM\n",
    "                    init.xavier_normal_(m.all_weights[1][0])\n",
    "                    init.xavier_normal_(m.all_weights[1][1])\n",
    "            else:\n",
    "                if truncated:\n",
    "                    m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "                else:\n",
    "                    m.weight.data.normal_(mean, stddev)\n",
    "                m.bias.data.zero_()\n",
    "        normal_init(self.rpn_net, 0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_cls_score_net,0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_bbox_pred_net,0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_bi_net,0, 0.01, TRUNCATED)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class vgg16(Network):\n",
    "    def __init__(self):\n",
    "        Network.__init__(self)\n",
    "        self._feat_stride = [16, ]\n",
    "        self._feat_compress = [1. / float(self._feat_stride[0]), ]\n",
    "        self._net_conv_channels = 512\n",
    "        self._fc7_channels = 4096\n",
    "\n",
    "    def _init_head_tail(self):\n",
    "        # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要load_pretrained_cnn从外部载入已训练好的权重信息\n",
    "        # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，无需再加载了\n",
    "        # 注意预加载的是识别图像的（对于识字的需做更改）\n",
    "        self.vgg = models.vgg16_bn(pretrained=True)\n",
    "        # Remove fc8\n",
    "        self.vgg.classifier = nn.Sequential(*list(self.vgg.classifier._modules.values())[:-1])\n",
    "\n",
    "        # Fix the layers before conv3:\n",
    "        for layer in range(12):\n",
    "          for p in self.vgg.features[layer].parameters(): p.requires_grad = False\n",
    "\n",
    "        # not using the last maxpool layer\n",
    "        self._layers['head'] = nn.Sequential(*list(self.vgg.features._modules.values())[:-1])\n",
    "#         print(self._layers['head'])\n",
    "\n",
    "\n",
    "    # 通过卷积网络VG16的feature层，抽取图片的特征\n",
    "    def _image_to_head(self):\n",
    "        net_conv = self._layers['head'](self._image)\n",
    "        self._act_summaries['conv'] = net_conv\n",
    "        return net_conv\n",
    "\n",
    "    def _head_to_tail(self, pool5):\n",
    "        pool5_flat = pool5.view(pool5.size(0), -1)\n",
    "        fc7 = self.vgg.classifier(pool5_flat)\n",
    "        return fc7\n",
    "\n",
    "\n",
    "    # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要该方法从外部载入权重信息\n",
    "    # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，\n",
    "    # 无需再加载了\n",
    "    def load_pretrained_cnn(self, state_dict):\n",
    "        self.vgg.load_state_dict({k:v for k,v in state_dict.items() if k in self.vgg.state_dict()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from lib.layutils.generate_anchors import generate_anchors\n",
    "import numpy as np\n",
    "def generate_anchors_pre(height, width, feat_stride, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n",
    "  \"\"\" A wrapper function to generate anchors given different scales\n",
    "    Also return the number of anchors in variable 'length'\n",
    "  \"\"\"\n",
    "  anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n",
    "  A = anchors.shape[0]\n",
    "  shift_x = np.arange(0, width) * feat_stride\n",
    "  shift_y = np.arange(0, height) * feat_stride\n",
    "  shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "  shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "  K = shifts.shape[0]\n",
    "  # width changes faster, so here it is H, W, C\n",
    "  anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "  anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "  length = np.int32(anchors.shape[0])\n",
    "\n",
    "  return anchors, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [
     13,
     18
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "torch.Size([1, 3, 600, 878])\n",
      "net conv size --> torch.Size([1, 512, 37, 54])\n",
      "tensor([ 0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# feat_stride：[16]VGG中conv5_3相比于输入图像缩小了16倍，也就是相邻两个点之间的stride=16 \n",
    "feat_stride = [16, ]\n",
    "ANCHOR_SCALES = [16]\n",
    "ANCHOR_RATIOS = [0.5,1,2]\n",
    "MOMENTUM = 0.9\n",
    "lr = 0.001\n",
    "DOUBLE_BIAS = True\n",
    "BIAS_DECAY = False\n",
    "WEIGHT_DECAY = 0.0005\n",
    "EPCHO = 1\n",
    "\n",
    "net = vgg16()\n",
    "# print(imdb.num_classes)\n",
    "net.create_architecture(imdb.num_classes, tag='default',\n",
    "                                            anchor_scales=ANCHOR_SCALES,\n",
    "                                            anchor_ratios=ANCHOR_RATIOS)\n",
    "params = []\n",
    "\n",
    "for key, value in dict(net.named_parameters()).items():\n",
    "  if value.requires_grad:\n",
    "    if 'bias' in key:\n",
    "      params += [{'params':[value],'lr':lr*(DOUBLE_BIAS + 1), \n",
    "                  'weight_decay': BIAS_DECAY and WEIGHT_DECAY or 0}]\n",
    "    else:\n",
    "      params += [{'params':[value],'lr':lr, \n",
    "                  'weight_decay': WEIGHT_DECAY}]\n",
    "\n",
    "optimizer = torch.optim.SGD(params,lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for _ in range(EPCHO):\n",
    "    blobs = data_layer.forward()\n",
    "    image = torch.from_numpy(blobs['data'].transpose([0,3,1,2]))\n",
    "    im_info = blobs['im_info']\n",
    "    gt_boxes = torch.from_numpy(blobs['gt_boxes'])\n",
    "    print(image.size())\n",
    "    net_conv = net._layers['head'](image)\n",
    "    print('net conv size --> {}'.format(net_conv.size()))\n",
    "    \n",
    "    # height, width   build the anchors for the image\n",
    "    anchors, length = generate_anchors_pre(net_conv.size(2), net_conv.size(3),feat_stride=feat_stride,anchor_scales=(16,))\n",
    "    anchors = torch.from_numpy(anchors)\n",
    "    rpn = F.relu(net.rpn_net(net_conv))  # ( N , C, H, W）\n",
    "    \n",
    "    # ( N , C, H, W）  --》 （N * H, W, C)\n",
    "    rpn_reshape = rpn.permute(0,2,3,1).squeeze(0)\n",
    "    \n",
    "    # 双向LSTM网络   -->  (N*H, W, C)\n",
    "    rpn_blstm,_ = net.rpn_bi_net(rpn_reshape)\n",
    "    rpn_blstm = F.relu(rpn_blstm)  # 注意另外可以考虑采用batch normal方法对数据进行整理\n",
    "    \n",
    "    # test detect 采用随机生成 偏移变量数组和得分初始化数组  \n",
    "    # 与rpn_blsm[N*H*W,C]矩阵相乘方式得到其偏移和分类得分 [N,H,W,4*num anchor或2]\n",
    "    # 这里暂时用lstm 来代替，后面需改成上述方案实现\n",
    "    rpn_cls_score,_ = net.rpn_cls_score_net(rpn_blstm)   # [W H, num_anchors*2], num_anchors = 10\n",
    "\n",
    "    \n",
    "    rpn_cls_score = rpn_cls_score.permute(2,0,1).unsqueeze(0)\n",
    "    rpn_cls_score_reshape = rpn_cls_score.contiguous().view(1,2,-1,rpn_cls_score.size()[3])  # N , 2*10, H, W   --->  N, 2, 10*H , W\n",
    "       \n",
    "    # 得到坐标点的10个分类概率（二分类方法)\n",
    "    rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape,dim=1)           # N 2 H*NUM_ANCHORS W\n",
    "    rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score) # N 2 H W  -- > N 2*NUM_ANCHORS  H  W\n",
    "    rpn_cls_prob = rpn_cls_prob.permute(0,2,3,1)\n",
    "    \n",
    "    rpn_cls_pred = rpn_cls_score_reshape.permute(0,2,3,1).contiguous()   # N H*NUM_ANCHORS W 2\n",
    "    rpn_cls_pred = torch.max(rpn_cls_pred.view(-1,2),1)[1]\n",
    "    print(rpn_cls_pred[0:3])\n",
    "    \n",
    "    # 注意采用下面方式进行比较时，发现cls prob 和 cls pred的值不一致\n",
    "#     print(rpn_cls_score.size())\n",
    "#     print(rpn_cls_pred[0:10])\n",
    "#     # rpn_cls_prob = rpn_cls_prob.contiguous().view(-1,2)  # 这种方式取值好象有问题，两项值相加不为1 \n",
    "#     print(rpn_cls_prob[0][0][0])    \n",
    "    \n",
    "    \n",
    "    \n",
    "    rpn_bbox_pred,_ = net.rpn_bbox_pred_net(rpn_blstm)   # [W H, num_anchors*4], num_anchors = 10\n",
    "    rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 37, 54])\n",
      "tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  0])\n",
      "tensor([ 0.4888,  0.5129,  0.4874,  0.5079,  0.4996,  0.5237,  0.5470,\n",
      "         0.4949,  0.4822,  0.4929,  0.5112,  0.4871,  0.5126,  0.4921,\n",
      "         0.5004,  0.4763,  0.4530,  0.5051,  0.5178,  0.5071])\n",
      "tensor([ 0.4888,  0.5129])\n",
      "torch.Size([1, 37, 54, 40])\n"
     ]
    }
   ],
   "source": [
    "# rpn_cls_score = rpn_cls_score.permute(2,0,1).unsqueeze(0)\n",
    "# print(rpn_cls_score.permute(2,0,1).unsqueeze(0).size())\n",
    "# nnn = rpn_cls_score.contiguous().view(1,2,-1,48)\n",
    "# print(nnn.size())\n",
    "print(rpn_cls_score.size())\n",
    "print(rpn_cls_pred[0:10])\n",
    "print(rpn_cls_prob[0][0][0])\n",
    "print(rpn_cls_prob.contiguous() .view(-1,2)[0])\n",
    "\n",
    "print(rpn_bbox_pred.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

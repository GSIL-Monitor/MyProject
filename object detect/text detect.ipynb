{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/skyfsm/p/6806246.html 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN\n",
    "    \n",
    "    https://blog.csdn.net/u013293750/article/details/64904681 CNN+LSTM深度学习文字检测\n",
    "    \n",
    "    https://blog.csdn.net/forest_world/article/details/78566737 主流ocr算法：CNN+BLSTM+CTC架构\n",
    "    \n",
    "    https://blog.csdn.net/slade_ruan/article/details/78301842?utm_source=blogxgwz1 场景文本检测，CTPN tensorflow版本\n",
    "    \n",
    "    https://blog.csdn.net/Quincuntial/article/details/79475339?utm_source=blogxgwz1 CTPN论文翻译——中英文对照\n",
    "    \n",
    "    http://lib.csdn.net/article/deeplearning/61632  通过代码理解faster-RCNN中的RPN\n",
    "    \n",
    "    https://slade-ruan.me/2017/10/22/text-detection-ctpn/  论文阅读与实现--CTPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/freeweb/p/6548208.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://deepsense.ai/region-of-interest-pooling-in-tensorflow-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/king-lps/p/9031568.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_train\n",
      "voc_2007_val\n",
      "voc_2007_trainval\n",
      "voc_2007_test\n",
      "<bound method imdb.default_roidb of <lib.datasets.pascal_voc.pascal_voc object at 0x0000025AB10275F8>>\n",
      "voc_2007_trainval gt roidb loaded from D:\\PROJECT_TW\\git\\data\\voc_2007_trainval_gt_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "from lib.datasets.factory import get_imdb\n",
    "from lib.datasets.pascal_voc import pascal_voc\n",
    "from lib.roi_data_layer.roidb import prepare_roidb\n",
    "from lib.roi_data_layer.layer import RoIDataLayer\n",
    "\n",
    "\n",
    "imdb = pascal_voc('trainval', '2007')\n",
    "# roidb ROI框的坐标位置信息, 信息来源于Annotations目录下对图片的XML定义\n",
    "prepare_roidb(imdb)   #  为方便训练，在原roidb信息基础上增加象image等等信息\n",
    "roidb = imdb.roidb \n",
    "data_layer = RoIDataLayer(roidb, imdb.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     2,
     39
    ]
   },
   "outputs": [],
   "source": [
    "RPN_CHANNELS = 512\n",
    "TRUNCATED = False\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._gt_image = None\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "\n",
    "    def create_architecture(self, num_classes, tag=None,anchor_scales=(16,), anchor_ratios=(0.5, 1, 2)):\n",
    "        self._tag = tag\n",
    "        self._num_classes = num_classes\n",
    "        self._anchor_scales = anchor_scales\n",
    "        self._num_scales = len(anchor_scales)\n",
    "        self._anchor_ratios = anchor_ratios\n",
    "        self._num_ratios = len(anchor_ratios)\n",
    "        self._num_anchors = 10\n",
    "        assert tag != None\n",
    "        # Initialize layers\n",
    "        self._init_modules()\n",
    "        \n",
    "    def _init_modules(self):\n",
    "        self._init_head_tail()\n",
    "        # rpn\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, RPN_CHANNELS, [3, 3], padding=1)\n",
    "        self.rpn_bi_net = nn.LSTM(RPN_CHANNELS, 256, batch_first=True, bidirectional=True)\n",
    "        self.rpn_cls_score_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 2, batch_first=True, bidirectional=False)\n",
    "        self.rpn_bbox_pred_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 4, batch_first=True, bidirectional=False)\n",
    "        self.init_weights()    \n",
    "        \n",
    "    # 对构建的网络参数（weight, bias）进行正则、初始化\n",
    "    def init_weights(self):\n",
    "        def normal_init(m, mean, stddev, truncated=False):\n",
    "            \"\"\"\n",
    "                weight initalizer: truncated normal and random normal.\n",
    "            \"\"\"\n",
    "            # x is a parameter\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                init.xavier_normal_(m.all_weights[0][0])\n",
    "                init.xavier_normal_(m.all_weights[0][1])\n",
    "                if len(m.all_weights) == 2:   # 双向  LSTM\n",
    "                    init.xavier_normal_(m.all_weights[1][0])\n",
    "                    init.xavier_normal_(m.all_weights[1][1])\n",
    "            else:\n",
    "                if truncated:\n",
    "                    m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "                else:\n",
    "                    m.weight.data.normal_(mean, stddev)\n",
    "                m.bias.data.zero_()\n",
    "        normal_init(self.rpn_net, 0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_cls_score_net,0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_bbox_pred_net,0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_bi_net,0, 0.01, TRUNCATED)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class vgg16(Network):\n",
    "    def __init__(self):\n",
    "        Network.__init__(self)\n",
    "        self._feat_stride = [16, ]\n",
    "        self._feat_compress = [1. / float(self._feat_stride[0]), ]\n",
    "        self._net_conv_channels = 512\n",
    "        self._fc7_channels = 4096\n",
    "\n",
    "    def _init_head_tail(self):\n",
    "        # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要load_pretrained_cnn从外部载入已训练好的权重信息\n",
    "        # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，无需再加载了\n",
    "        # 注意预加载的是识别图像的（对于识字的需做更改）\n",
    "        self.vgg = models.vgg16_bn(pretrained=True)\n",
    "        # Remove fc8\n",
    "        self.vgg.classifier = nn.Sequential(*list(self.vgg.classifier._modules.values())[:-1])\n",
    "\n",
    "        # Fix the layers before conv3:\n",
    "        for layer in range(12):\n",
    "          for p in self.vgg.features[layer].parameters(): p.requires_grad = False\n",
    "\n",
    "        # not using the last maxpool layer\n",
    "        self._layers['head'] = nn.Sequential(*list(self.vgg.features._modules.values())[:-1])\n",
    "#         print(self._layers['head'])\n",
    "\n",
    "\n",
    "    # 通过卷积网络VG16的feature层，抽取图片的特征\n",
    "    def _image_to_head(self):\n",
    "        net_conv = self._layers['head'](self._image)\n",
    "        self._act_summaries['conv'] = net_conv\n",
    "        return net_conv\n",
    "\n",
    "    def _head_to_tail(self, pool5):\n",
    "        pool5_flat = pool5.view(pool5.size(0), -1)\n",
    "        fc7 = self.vgg.classifier(pool5_flat)\n",
    "        return fc7\n",
    "\n",
    "\n",
    "    # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要该方法从外部载入权重信息\n",
    "    # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，\n",
    "    # 无需再加载了\n",
    "    def load_pretrained_cnn(self, state_dict):\n",
    "        self.vgg.load_state_dict({k:v for k,v in state_dict.items() if k in self.vgg.state_dict()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/garfielder007/article/details/51378296  Python numpy函数hstack() vstack() stack() dstack() vsplit() concatenate()\n",
    "from lib.layutils.generate_anchors import generate_anchors\n",
    "import lib.layutils.anchor_target_layer as atl\n",
    "import numpy as np\n",
    "\n",
    "def generate_anchors_pre(height, width, feat_stride, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n",
    "  \"\"\" A wrapper function to generate anchors given different scales\n",
    "    Also return the number of anchors in variable 'length'\n",
    "  \"\"\"\n",
    "  anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n",
    "  A = anchors.shape[0]\n",
    "  shift_x = np.arange(0, width) * feat_stride\n",
    "  shift_y = np.arange(0, height) * feat_stride\n",
    "  shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "  shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "  K = shifts.shape[0]\n",
    "  # width changes faster, so here it is H, W, C\n",
    "  anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "  anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "  length = np.int32(anchors.shape[0])\n",
    "\n",
    "  return anchors, length\n",
    "\n",
    "def _anchor_target_layer(rpn_cls_score, gt_boxes, im_info, feat_stride, anchors, num_anchors):\n",
    "    print('_anchor_target_layer begin .... 开始 。。。。')\n",
    "    rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = atl.anchor_target_layer(\n",
    "        rpn_cls_score.data,\n",
    "        gt_boxes.data.numpy(),\n",
    "        im_info,\n",
    "        feat_stride,\n",
    "        anchors.data.numpy(),\n",
    "        num_anchors)\n",
    "\n",
    "    rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "    rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_labels = rpn_labels.long()\n",
    "#     self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "#     self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "#     self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "#     self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "#     for k in self._anchor_targets.keys():\n",
    "#         self._score_summaries[k] = self._anchor_targets[k]\n",
    "\n",
    "    return rpn_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     13,
     18
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "torch.Size([1, 3, 600, 878])\n",
      "net conv size --> torch.Size([1, 512, 37, 54])\n",
      "tensor([ 1,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# feat_stride：[16]VGG中conv5_3相比于输入图像缩小了16倍，也就是相邻两个点之间的stride=16 \n",
    "feat_stride = [16, ]\n",
    "ANCHOR_SCALES = [16]\n",
    "ANCHOR_RATIOS = [0.5,1,2]\n",
    "MOMENTUM = 0.9\n",
    "lr = 0.001\n",
    "DOUBLE_BIAS = True\n",
    "BIAS_DECAY = False\n",
    "WEIGHT_DECAY = 0.0005\n",
    "EPCHO = 1\n",
    "num_anchors = 10\n",
    "\n",
    "net = vgg16()\n",
    "# print(imdb.num_classes)\n",
    "net.create_architecture(imdb.num_classes, tag='default',\n",
    "                                            anchor_scales=ANCHOR_SCALES,\n",
    "                                            anchor_ratios=ANCHOR_RATIOS)\n",
    "params = []\n",
    "\n",
    "for key, value in dict(net.named_parameters()).items():\n",
    "  if value.requires_grad:\n",
    "    if 'bias' in key:\n",
    "      params += [{'params':[value],'lr':lr*(DOUBLE_BIAS + 1), \n",
    "                  'weight_decay': BIAS_DECAY and WEIGHT_DECAY or 0}]\n",
    "    else:\n",
    "      params += [{'params':[value],'lr':lr, \n",
    "                  'weight_decay': WEIGHT_DECAY}]\n",
    "\n",
    "optimizer = torch.optim.SGD(params,lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for _ in range(EPCHO):\n",
    "    blobs = data_layer.forward()\n",
    "    image = torch.from_numpy(blobs['data'].transpose([0,3,1,2]))\n",
    "    im_info = blobs['im_info']\n",
    "    gt_boxes = torch.from_numpy(blobs['gt_boxes'])\n",
    "    print(image.size())\n",
    "    net_conv = net._layers['head'](image)\n",
    "    print('net conv size --> {}'.format(net_conv.size()))\n",
    "    \n",
    "    # height, width   build the anchors for the image\n",
    "    anchors, length = generate_anchors_pre(net_conv.size(2), net_conv.size(3),feat_stride=feat_stride,anchor_scales=(16,))\n",
    "    anchors = torch.from_numpy(anchors)\n",
    "    rpn = F.relu(net.rpn_net(net_conv))  # ( N , C, H, W）\n",
    "    \n",
    "    # ( N , C, H, W）  --》 （N * H, W, C)\n",
    "    rpn_reshape = rpn.permute(0,2,3,1).squeeze(0)\n",
    "    \n",
    "    # 双向LSTM网络   -->  (N*H, W, C)\n",
    "    rpn_blstm,_ = net.rpn_bi_net(rpn_reshape)\n",
    "    rpn_blstm = F.relu(rpn_blstm)  # 注意另外可以考虑采用batch normal方法对数据进行整理\n",
    "    \n",
    "    # test detect 采用随机生成 偏移变量数组和得分初始化数组  \n",
    "    # 与rpn_blsm[N*H*W,C]矩阵相乘方式得到其偏移和分类得分 [N,H,W,4*num anchor或2]\n",
    "    # 这里暂时用lstm 来代替，后面需改成上述方案实现\n",
    "    rpn_cls_score,_ = net.rpn_cls_score_net(rpn_blstm)   # [W H, num_anchors*2], num_anchors = 10\n",
    "\n",
    "    \n",
    "    rpn_cls_score = rpn_cls_score.permute(2,0,1).unsqueeze(0)\n",
    "    rpn_cls_score_reshape = rpn_cls_score.contiguous().view(1,2,-1,rpn_cls_score.size()[3])  # N , 2*10, H, W   --->  N, 2, 10*H , W\n",
    "       \n",
    "    # 得到坐标点的10个分类概率（二分类方法)\n",
    "    rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape,dim=1)           # N 2 H*NUM_ANCHORS W\n",
    "    rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score) # N 2 H W  -- > N 2*NUM_ANCHORS  H  W\n",
    "    rpn_cls_prob = rpn_cls_prob.permute(0,2,3,1)\n",
    "    \n",
    "    rpn_cls_pred = rpn_cls_score_reshape.permute(0,2,3,1).contiguous()   # N H*NUM_ANCHORS W 2\n",
    "    rpn_cls_pred = torch.max(rpn_cls_pred.view(-1,2),1)[1]\n",
    "    print(rpn_cls_pred[0:3])\n",
    "    \n",
    "    # 注意采用下面方式进行比较时，发现cls prob 和 cls pred的值不一致\n",
    "#     print(rpn_cls_score.size())\n",
    "#     print(rpn_cls_pred[0:10])\n",
    "#     # rpn_cls_prob = rpn_cls_prob.contiguous().view(-1,2)  # 这种方式取值好象有问题，两项值相加不为1 \n",
    "#     print(rpn_cls_prob[0][0][0])    \n",
    "    \n",
    "    \n",
    "    \n",
    "    rpn_bbox_pred,_ = net.rpn_bbox_pred_net(rpn_blstm)   # [W H, num_anchors*4], num_anchors = 10\n",
    "    rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_anchor_target_layer begin .... 开始 。。。。\n",
      "AnchorTargetLayer: height 20 width 37\n",
      "\n",
      "im_size: (600.0, 878.0)\n",
      "scale: 1.0\n",
      "height, width: (20, 37)\n",
      "rpn: gt_boxes.shape (833, 5)\n",
      "rpn: gt_boxes [[242. 574. 256. 595.   1.]\n",
      " [257. 574. 260. 595.   1.]\n",
      " [260. 574. 272. 595.   1.]\n",
      " ...\n",
      " [321.   3. 332.  24.   1.]\n",
      " [332.   3. 336.  24.   1.]\n",
      " [337.   3. 350.  24.   1.]]\n",
      "total_anchors 19980\n",
      "inds_inside 17172\n",
      "anchors.shape (17172, 4)\n",
      "anchors , gt boxes --> overlaps: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "arg max over laps -> [  0   0 797 ...   0   0   0]\n",
      "max over laps -> [0.         0.         0.20446097 ... 0.         0.         0.        ]\n",
      "gt argmax overlaps -> 917\n",
      "Number FG -> 128  Number BG -> 128\n",
      "anchors size -> (17172, 4) cgt boxes size -> (17172, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "min() missing 1 required positional arguments: \"dim\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-93faff319d82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                                   \u001b[0mfeat_stride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                                   \u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                                   \u001b[0mnum_anchors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                                  )\n",
      "\u001b[1;32m<ipython-input-36-345909fc9d90>\u001b[0m in \u001b[0;36m_anchor_target_layer\u001b[1;34m(rpn_cls_score, gt_boxes, im_info, feat_stride, anchors, num_anchors)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mfeat_stride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0manchors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         num_anchors)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mrpn_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrpn_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.set_shape([1, 1, None, None])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\object detect\\lib\\layutils\\anchor_target_layer.py\u001b[0m in \u001b[0;36manchor_target_layer\u001b[1;34m(rpn_cls_score, gt_boxes, im_info, _feat_stride, all_anchors, num_anchors)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'anchors size -> {} cgt boxes size -> {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcgt_boxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mbbox_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compute_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcgt_boxes\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;31m# zz = bbox_transform_inv(torch.from_numpy(anchors[4506]).view(1,4),torch.from_numpy(bbox_targets[4506]).view(1,4))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\object detect\\lib\\layutils\\anchor_target_layer.py\u001b[0m in \u001b[0;36m_compute_targets\u001b[1;34m(ex_rois, gt_rois)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mgt_rois\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbbox_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_rois\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_rois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\object detect\\lib\\layutils\\anchor_target_layer.py\u001b[0m in \u001b[0;36mbbox_transform\u001b[1;34m(ex_rois, gt_rois)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mex_ctr_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex_rois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mex_heights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_widths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_heights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;34m'Invalid boxes found: {} {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_rois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_widths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex_rois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_heights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2415\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2417\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mamin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m     return _methods._amin(a, axis=axis,\n",
      "\u001b[1;31mTypeError\u001b[0m: min() missing 1 required positional arguments: \"dim\""
     ]
    }
   ],
   "source": [
    "# print(rpn_cls_score.size())\n",
    "# print(rpn_cls_pred[0:10])\n",
    "# print(rpn_cls_prob[0][0][0])\n",
    "# print(rpn_cls_prob.contiguous() .view(-1,2)[0])\n",
    "# print(type(gt_boxes.data))\n",
    "# print(anchors.size())\n",
    "# from lib.layutils.anchor_target_layer import anchor_target_layer\n",
    "\n",
    "# 重新加载某模块\n",
    "import lib.layutils.anchor_target_layer as atl\n",
    "import importlib\n",
    "importlib.reload(atl)\n",
    "\n",
    "import lib.utils.bbox as bbox\n",
    "importlib.reload(bbox)\n",
    "\n",
    "# print('anchors -->', anchors)\n",
    "# print('gt_boxes ->', gt_boxes)\n",
    "# anchors_a = anchors.data.numpy()\n",
    "# gt_boxes_a = gt_boxes.data.numpy()\n",
    "\n",
    "# overlaps = bbox.bbox_overlaps(\n",
    "#     np.ascontiguousarray(anchors_a,dtype=np.float),\n",
    "#     np.ascontiguousarray(gt_boxes_a,dtype=np.float)\n",
    "# )\n",
    "\n",
    "# argmax_overlaps = overlaps.argmax(axis=1)\n",
    "# oidx = np.where(argmax_overlaps > 0)\n",
    "\n",
    "rpn_labels = _anchor_target_layer(rpn_cls_score,\n",
    "                                  gt_boxes,\n",
    "                                  im_info,\n",
    "                                  feat_stride,\n",
    "                                  anchors,\n",
    "                                  num_anchors\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12280,)\n",
      "(19980, 833)\n"
     ]
    }
   ],
   "source": [
    "print(oidx[0].shape)\n",
    "print(overlaps.shape)\n",
    "# print(overlaps[:,oidx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

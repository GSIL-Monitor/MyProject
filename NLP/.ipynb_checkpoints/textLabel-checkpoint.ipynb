{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "家居 家居\n",
      "彩票 彩票\n",
      "房产 房产\n",
      "教育 教育\n",
      "股票 股票\n",
      "财经 财经\n"
     ]
    }
   ],
   "source": [
    "import lib.data.dataset as ds\n",
    "import importlib\n",
    "from torch.utils import data\n",
    "importlib.reload(ds)\n",
    "import torch.nn as nn\n",
    "from easydict import EasyDict as edict\n",
    "import torch\n",
    "data_path = 'D:/PROJECT_TW/git/data/nlp/w2v/data/'\n",
    "model_path = 'D:/PROJECT_TW/git/data/nlp/w2v/'\n",
    "eds = ds.EduData(data_path, model_path)\n",
    "dl = data.DataLoader(eds,batch_size=50,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 64])\n"
     ]
    }
   ],
   "source": [
    "#检测 nn.Emmeding 使用训练好的词向量结果\n",
    "import torch.nn as nn\n",
    "_,in_data = next(enumerate(dl))\n",
    "words, labels = in_data\n",
    "print(words.size())\n",
    "embedding_matrix = torch.from_numpy(eds.embedding_matrix)\n",
    "embed = nn.Embedding(40000, 20,_weight=embedding_matrix)\n",
    "out_data = embed(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(out_data[0][5])\n",
    "# print(words[0])\n",
    "# print(eds.idx_to_word[4237])\n",
    "# print(eds.word_vec_mod.wv.get_vector('布莱克本'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "opt =  edict()\n",
    "opt.vocab_size = 40000\n",
    "opt.embedding_dim = 20\n",
    "opt.inception_dim = 40\n",
    "opt.static = False\n",
    "opt.num_classes = 6\n",
    "opt.content_seq_len = 64\n",
    "opt.linear_hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于conv1D 参看： https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
    "import lib.models.CNNText_inception as cni\n",
    "import torch\n",
    "importlib.reload(cni)\n",
    "\n",
    "embedding_matrix = torch.from_numpy(eds.embedding_matrix)\n",
    "model = cni.CNNText_inception(opt, embedding_matrix)\n",
    "# 注意下面这个， 将model设置成model.double\n",
    "# https://stackoverflow.com/questions/49407303/runtimeerror-expected-object-of-type-torch-doubletensor-but-found-type-torch-fl\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss -->  6.386157853517816e-05 \n",
      "20 loss -->  3.6354154207884903e-06 \n",
      "40 loss -->  7.560421738604717e-05 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-475aa2537388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPCHO = 100\n",
    "loss_fun =  nn.CrossEntropyLoss()\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for step in range(EPCHO):\n",
    "    for i, (words, labels) in enumerate(dl):\n",
    "        out = model(content=words)\n",
    "        loss = loss_fun(out,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0:\n",
    "            print('{} loss -->  {} '.format(i, loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12350,   8294,  13002,  ...,  11481,  11481,  11481],\n",
      "        [ 11263,   8010,   3722,  ...,  11481,  11481,  11481],\n",
      "        [  4733,   5008,  13054,  ...,  11481,  11481,  11481],\n",
      "        ...,\n",
      "        [  5871,  12416,   4349,  ...,  11481,  11481,  11481],\n",
      "        [  2939,  10100,  13096,  ...,  11481,  11481,  11481],\n",
      "        [  8581,   8264,   7777,  ...,   9705,   9838,  11759]])\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "testDoc = '''\n",
    "130平田园风小复式 楼住有情人(组图)\n",
    "　　拥有阁楼应该是件幸福的事，它不但增加了家居空间，还给家的设计提供了更为多样化的选择。\n",
    "　　这位网友的130平小复式采用都市田园风格，明朗透亮且温馨浪漫，甜蜜的小两口带着浓浓爱意携手打造温馨爱窝。\n",
    "　　阁楼自成观景台，可尽情享受一览众“山”小的精彩，将阁楼变身为卧室，打破常规在阁楼中体验别样的灵动。\n",
    "\n",
    "　'''    \n",
    "# 提取中文, 分词， 得到词向量\n",
    "line = ds.extract_chinese(testDoc)\n",
    "ws = jieba.lcut(line, cut_all=False, HMM=True)\n",
    "words_ids = [eds.word_to_idx[x] for x in ws if x in eds.word_to_idx]\n",
    "words_ids = words_ids[0:64]\n",
    "words_ids = torch.LongTensor(words_ids)\n",
    "words_ids = words_ids.unsqueeze(0)\n",
    "twords = words[0]\n",
    "twords = twords.unsqueeze(0)\n",
    "print(twords.size())\n",
    "model.eval()\n",
    "out = model(content=words_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.4055,  -7.1522,  -8.3775,  -7.0593, -10.3726,  -5.2682]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# {0: '家居', 1: '彩票', 2: '房产', 3: '教育', 4: '股票', 5: '财经'}\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    http://www.dataguru.cn/article-13488-1.html\n",
    "    \n",
    "    https://github.com/Adoni/word2vec_pytorch\n",
    "    \n",
    "    https://cloud.tencent.com/developer/news/157676\n",
    "    \n",
    "    https://blog.csdn.net/BBZZ2/article/details/71123150?utm_source=blogxgwz1 高级词向量表达\n",
    "    \n",
    "    Word2vec，是为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系，该向量为神经网络之隐藏层。Word2vec 可以根据给定的语料库，通过优化后的训练模型快速有效地将一个词语表达成向量形式，为自然语言处理领域的应用研究提供了新的工具。Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。Word2vec为托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造。该算法渐渐被其他人所分析和解释。\n",
    "    依赖\n",
    "    1) 词袋模型\n",
    "        词袋模型（Bag-of-words model）是个在自然语言处理和信息检索(IR)下被简化的表达模型。此模型下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。最近词袋模型也被应用在计算机视觉领域。词袋模型被广泛应用在文件分类，词出现的频率可以用来当作训练分类器的特征。关于\"词袋\"这个用字的由来可追溯到泽里格·哈里斯于1954年在Distributional Structure的文章。\n",
    "        \n",
    "    2) Skip-gram 模型\n",
    "        Skip-gram 模型是一个简单但却非常实用的模型。在自然语言处理中，语料的选取是一个相当重要的问题: 第一，语料必须充分。一方面词典的词量要足够大，另一方面要尽可能多地包含反映词语之间关系的句子，例如，只有“鱼在水中游”这种句式在语料中尽可能地多，模型才能够学习到该句中的语义和语法关系，这和人类学习自然语言一个道理，重复的次数多了，也就会模仿了； 第二，语料必须准确。 也就是说所选取的语料能够正确反映该语言的语义和语法关系，这一点似乎不难做到，例如中文里，《人民日报》的语料比较准确。 但是，更多的时候，并不是语料的选取引发了对准确性问题的担忧，而是处理的方法。 n元模型中，因为窗口大小的限制，导致超出窗口范围的词语与当前词之间的关系不能被正确地反映到模型之中，如果单纯扩大窗口大小又会增加训练的复杂度。Skip-gram 模型的提出很好地解决了这些问题。顾名思义，Skip-gram 就是“跳过某些符号”，例如，句子“中国足球踢得真是太烂了”有4个3元词组，分别是“中国足球踢得”、“足球踢得真是”、“踢得真是太烂”、“真是太烂了”，可是我们发现，这个句子的本意就是“中国足球太烂”可是上述 4个3元词组并不能反映出这个信息。Skip-gram 模型却允许某些词被跳过，因此可以组成“中国足球太烂”这个3元词组。 如果允许跳过2个词，即 2-Skip-gram\n",
    "        \n",
    "        \n",
    "    word2vec和word embedding的区别 简言之，word embedding 是一个将词向量化的概念，中文译名为\"词嵌入\"。 word2vec是谷歌提出的一种word embedding的具体手段，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。\n",
    "    \n",
    "    典型代表：word2vec和auto-encoder （这里解释一下AutoEncoder，AutoEncoder也可以用于训练词向量，先将one hot映射成一个hidden state，再映射回原来的维度，令输入等于输出，取中间的hidden vector作为词向量，在不损耗原表达能力的前提下压缩向量维度，得到一个压缩的向量表达形式。） \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

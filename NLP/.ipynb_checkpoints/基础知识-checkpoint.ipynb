{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://blog.csdn.net/pipisorry/article/details/76095118?utm_source=blogxgwz0\n",
    "    \n",
    "    https://blog.csdn.net/mylove0414/article/details/61616617?utm_source=blogxgwz1\n",
    "    \n",
    "    https://baijiahao.baidu.com/s?id=1602069319915188130&wfr=spider&for=pc\n",
    "    \n",
    "    https://blog.csdn.net/appleml/article/details/78595724 pytorch加载已训练好的word-embedding\n",
    "    \n",
    "    那么我们为什么要使用嵌入层 Embedding呢? 主要有这两大原因:\n",
    "\n",
    "    1、使用One-hot 方法编码的向量会很高维也很稀疏。假设我们在做自然语言处理（NLP）中遇到了一个包含2000个词的字典，当时用One-hot编码时，每一个词会被一个包含2000个整数的向量来表示，其中1999个数字是0，要是我的字典再大一点的话这种方法的计算效率岂不是大打折扣？\n",
    "\n",
    "    2、训练神经网络的过程中，每个嵌入的向量都会得到更新。如果你看到了博客上面的图片你就会发现在多维空间中词与词之间有多少相似性，这使我们能可视化的了解词语之间的关系，不仅仅是词语，任何能通过嵌入层 Embedding 转换成向量的内容都可以这样做。\n",
    "    \n",
    "    \n",
    "    embedding 算法主要用于处理稀疏特征，应用于NLP、推荐、广告等领域。所以word2vec 只是embbeding 思想的一个应用，而不是全部。\n",
    "    \n",
    "    常见的特征降维方法主要有PCA、SVD等。而embedding的主要目的也是对（稀疏）特征进行降维，它降维的方式可以类比为一个全连接层（没有激活函数），通过 embedding 层的权重矩阵计算来降低维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hecong/venv/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/hecong/venv/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.348613\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.287601\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.227542\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.168027\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.108868\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 5.050111\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.991744\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.933736\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.875984\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.818320\n",
      "tensor([44])\n",
      "real word is Then, predict word is Then\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# import torch.Value.Variable as Variable\n",
    "# https://my.oschina.net/earnp/blog/1113897  使用word embedding做自然语言处理的词语预测\n",
    "# 数据预处理 首先我们给出了一段文章作为我们的训练集\n",
    "\n",
    "CONTEXT_SIZE = 2  # 表示我们想由前面的几个单词来预测这个单词，这里设置为2，就是说我们希望通过这个单词的前两个单词来预测这一个单词。\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2])\n",
    "           for i in range(len(test_sentence)-2)]\n",
    "\n",
    "vocb = set(test_sentence) # 通过set将重复的单词去掉\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}\n",
    "\n",
    "# 定义模型\n",
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        # 注意Embedding初始化后， 不参与回归，每次的词的词向量是相同的。\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size*n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1)\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "#         print(out)\n",
    "        log_prob = F.log_softmax(out)\n",
    "#         print(log_prob)\n",
    "        return log_prob\n",
    "\n",
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('*'*10)\n",
    "    running_loss = 0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data[0]\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))\n",
    "    \n",
    "word, label = trigram[30]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngrammodel(word)\n",
    "_, predict_label = torch.max(out, 1)\n",
    "print(predict_label)\n",
    "predict_word = idx_to_word[predict_label.item()]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(predict_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Log-Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062\n",
    "    \n",
    "    https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral pytorch loss function 总结\n",
    "    \n",
    "    http://yangli.name/2017/12/25/20171225pytorch2/  pytorch:激活函数、损失函数小结\n",
    "    \n",
    "    S型函数（Sigmoid function）是BP神经网络中常用的非线性作用函数，即sigmoid函数，公式是f(x)=1/(1+e^-x)（-x是幂数）。Sigmoid函数又分为Log-Sigmoid函数和Tan-Sigmoid函数。\n",
    "    由于BP神经网络的传递函数必须可微，所以感知器的传递函数--二值函数在这里不可用，故BP神经网络一般使用Sigmoid函数或者线性函数作为传递函数。而Sigmoid函数又分为Log-Sigmoid函数（一般所说的S型函数就是这个的简称）和Tan-Sigmoid函数（又称为双曲正切S型函数），前者的值域为（0，1），后者的值域为（-1,1）。\n",
    "    \n",
    "    m = nn.LogSigmoid()\n",
    "    input = torch.randn(2)\n",
    "    print(input)\n",
    "    output = m(input)\n",
    "    print(output)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

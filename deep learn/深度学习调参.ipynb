{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 权重初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html\n",
    "    \n",
    "    https://www.cnblogs.com/bamtercelboo/p/7469005.html\n",
    "    \n",
    "    param.data = param.data + lr*(param.grad.data)\n",
    "    \n",
    "    可行的几种初始化方式\n",
    "    \n",
    "    1、pre-training\n",
    "        pre-training是早期训练神经网络的有效初始化方法，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。具体过程可以参见UFLDL的一个tutorial，因为这不是本文重点，就在这里简略的说一下：\n",
    "\n",
    "        pre-training阶段，将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值 \n",
    "        fine-tuning阶段，将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。\n",
    "\n",
    "        随着数据量的增加以及activation function (参见我的另一篇文章) 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。不想从零开始训练神经网络时，我们往往选择一个已经训练好的在任务A上的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）。\n",
    "        \n",
    "        \n",
    "    2、random initialization\n",
    "        随机初始化是很多人目前经常使用的方法，然而这是有弊端的，一旦随机分布选择不当，就会导致网络优化陷入困境。\n",
    "        \n",
    "    3、Xavier initialization\n",
    "        Xavier initialization可以解决上面的问题！其初始化方式也并不复杂。Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。注意，为了问题的简便，Xavier初始化的推导过程是基于线性函数的，但是它在一些非线性神经元中也很有效。输出值在很多层之后依然保持着良好的分布，这很有利于我们优化神经网络！之前谈到Xavier initialization是在线性函数上推导得出，这说明它对非线性函数并不具有普适性，所以这个例子仅仅说明它对tanh很有效，那么对于目前最常用的ReLU神经元呢。前面看起来还不错，后面的趋势却是越来越接近0。幸运的是，He initialization可以用来解决ReLU初始化的问题。\n",
    "    \n",
    "    4、He initialization\n",
    "        He initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2：\n",
    "    \n",
    "    5、Batch Normalization Layer\n",
    "        Batch Normalization是一种巧妙而粗暴的方法来削弱bad initialization的影响，其基本思想是：If you want it, just make it!我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换。Batch Normalization中所有的操作都是平滑可导，这使得back propagation可以有效运行并学到相应的参数γ，β。需要注意的一点是Batch Normalization在training和testing时行为有所差别。Training时μβ和σβ由当前batch计算得出；在Testing时μβ和σβ应使用Training时保存的均值或类似的经过处理的值，而不是由当前batch计算。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 深度学习的训练和调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/cvtoEyes/p/8603490.html\n",
    "    \n",
    "    https://blog.csdn.net/liyuan123zhouhui/article/details/61190436\n",
    "    \n",
    "    https://blog.csdn.net/Statham_stone/article/details/79748037  神经网络自动化调参框架MultiRunner详解\n",
    "    \n",
    "    神经网络经典五大超参数:\n",
    "\n",
    "    学习率(Leraning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)\n",
    "    \n",
    "    单层神经元数(Units)、正则惩罚项（Regularizer|Normalization)\n",
    "\n",
    "\n",
    "\n",
    "---------------------\n",
    "\n",
    "本文来自 跬步达千里 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/liyuan123zhouhui/article/details/61190436?utm_source=copy \n",
    "    准备数据集到训练到理想的模型的过程顺序，我们把其分成如下几个部分分别叙述。\n",
    "    1、Data augmentation（数据增强）。个人理解数据增强主要是在准备数据集时，由于需要的data较多又得不到满足，则可以通过对颜色（color）、尺度（scale）、裁剪（crop）、翻转（Flip）、添加噪声（Noise）、旋转（Rotation）等等，这样就增加了数据集的数目，解决的是data不足的问题，近几年GAN模型的研究也取得了较大的发展，其主要的出发点就是解决了监督学习data不足的问题，经常可用于虚拟场景的仿真等等，感兴趣的可以深入研究。\n",
    "\n",
    "    2、Pre-processing（数据预处理）。个人理解的是有了数据之后，难道就一定能很好的利用这些数据吗？如何评价这些数据的好坏？那要对数据进行预处理，预处理的前提是要正确的理解数据。数据之间存在相关性吗？假设你用了data augmentation，那显然数据集之间的相关性是较大的，说直白点，你用了两张一模一样的数据在训练集里，意义何在？所以接下来要讲的数据预处理很重要。常用的方法有：normalization（归一化）、PCA（主成分分析）、Whitening（白化）等。\n",
    "        （1）Normalization。可以这样认为，归一化主要在干这样一件事：把数据从一个一般的分布，变成0均值、单位方差的分布，为什么这么干呢？原因是这么做更容易收敛，这种方法在Caffe框架中普遍使用（mean value或者mean binaryproto 文件）。Batch Normalization（BN）是一个升级版本，作者主要考虑当使用了饱和的激活函数时，例如sigmoid函数变成0均值、单位方差则会出现在该函数中间近似线性的那一段，这是非常糟糕的，中间线性的部分的拟合能力最差，因此降低了模型的表达capacity，所以BN应运而生，实验表明，其效果sigmoid函数比Relu函数要好。\n",
    "        （2）    PCA。研究如何以最少的信息丢失将众多原有的变量信息浓缩到少数几个维度上，即所谓的主成分。首先计算出协方差矩阵，然后求出协方差矩阵的特征向量，并用其对原特征进行线性变换，实现降维。\n",
    "        （3）    Whitening。去除特征向量中各个特征之间的相关性，同时保证每个特征的方差一致。 设特征向量 X = (X1，X2，X3)，对于向量 X，可以计算出相应的协方差矩阵(根据已有数据集来估计)。我们希望协方差矩阵是一个对角矩阵，因为这意味着 X 的每个元素之间都是互不关联的，但是我们的数据并不具备这样的性质。为了解耦数据，我们需要对原始特征向量执行一个变换，从而使得变换后的向量 Y 的各个特征之间相关性为0。设 Σ 是 X 的协方差矩阵，有：ΣΦ=ΦΛ， 那么 Λ 中的每个元素都是协方差矩阵的特征值，Φ 的每个列向量是相应的特征向量。如果对特征向量做变换：Y = XΦ = X(Φ1，Φ2，Φ3)，此时根据向量 Y 计算出来的协方差矩阵是一个对角矩阵。对角矩阵 Λ 的特征值就是 Y 的每个元素的方差，可以全部相同，也可能不相同，如果对变换后的特征向量的某些维度进行缩放，使得 Λ 的每个元素都相等，那么整个过程就是 whitening。\n",
    "\n",
    "    3、Initialization（初始化）。当前两步完成之后，可以考虑模型参数的初始化方式了。此处举出实例，Caffe中的参数初始化方式有7种方法，分别为：constant、gaussian、positive_unitball、uniform、xavier、msra和bilinear。用的较多的是xavier用在权重初始化上，constant用在偏置初始化上。\n",
    "    \n",
    "    4、Activation Functions（激活函数）。深度学习之所以具有丰富的表达能力，非常关键的一点是激活函数，这就相当于一系列叠加在一起的非线性处理单元，可以想象出这一系列叠加的非线性处理单元原则上可以逼近任意函数（这指的是从输入到输出效果）。几种常用的激活函数：sigmoid、tanh和Relu，但是我们又介绍过之前广泛使用的sigmoid和tanh等饱和激活函数，使用它们在很深网络模型中的训练效果往往很不好，因为存在梯度消失的问题，例如下图中是一个sigmoid函数的例子，由于神经网络在反向传播时，需要乘以激活函数的一阶导数，这样逐层往前传，可想0.930=0.042，这就产生了两个极端，出现了如下图所示的梯度消失区，一旦梯度都已经很小了，还怎么学习？我们在Caffe中常用Relu函数有效地避免这一问题。\n",
    "    \n",
    "    5、During training（训练过程中）。在训练过程中，要掌握学习率的变化策略，一般而言Caffe定义学习率在超参数配置文件中（solver.prototxt），并选择了学习速率的衰减策略（学习速率都是开始的时候大，然后之后变小，如何变，怎么变，我们将其称为策略，所以在论文中一般都会谈到这一问题），更为重要的是，可以在网络层定义中指定lr_mult选择某一层的学习率，该技巧也可为之后的调参做准备。另外一点非常重要的是fine-tune，微调的用处通常情况下就是你选择了一个较为深的model，也就是较为复杂的model，你并不需要把所有的layers都重新训练，而只是训练了其中的some layers，此时我们完全可以站在巨人的肩膀上（利用预训练模型的weights初始化），可以省去很多工作，更为重要的是，加上合适的调参还会提高模型的泛化能力（因为预训练的模型往往还未收敛）\n",
    "    \n",
    "    6、 Regularizations（正则化）。正则化也称为Weight-decay（限制权值）。正则化应该讲是一种避免over-fitting的有效方法，这里我们插入一段对over-fitting的分析，就我的认识而言，从事机器学习的工程师们经常会遇到很多问题，很多bug，但是可以这样说over-fitting是所有工程师都必须面对的一个问题，其具有很强的通用性，这是由于方法本身所决定的。既然大家都会遇到这个问题，又该如何解决呢？回头看，我们说过深度学习的本质就是数据和模型，那解决过拟合的根本途径也必须从这两个方向出发，那什么是过拟合呢？形象一点说就是你认为你的model在训练集上已经表现很好了，可是当你把它使用在验证集上的时候，效果则很差，进一步说就是数据集太少或者模型太复杂，两者显然不匹配。现在我们开始从这两个方向分析，解决方法两个：增加数据集和减小模型的复杂度（限制网络的capacity）。此处正则化就是从减小模型的复杂度出发的一项技术，其本质就是控制模型学习的特征数目，使其最小化，从而防止在训练过程中引入训练集的抽样误差，正则化包括L2正则化和L1正则化。\n",
    "    \n",
    "    7、Dropout。Dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，如下图所示。对于随机梯度下降来说，由于是随机丢弃，因此每一个mini-batch都在训练不同的网络（对于一个有N个节点的神经网络，采用dropout后，可以认为其是2n个模型的集合），同时每个网络只见过一个训练数据（每次都是随机的新网络），从而将这些多个模型组合起来，以每个模型的平均输出作为结果，\n",
    "    \n",
    "    8、Insights from Figures。如果说通过上面的方法，你都做了，还是存在问题，那就需要仔细的检查了，检查的方法有很多，其中最为形象生动的，也就是这里要说的就是画图，从图中进行推断。我们知道Caffe也给我们提供了很多画图的tools（称其为可视化），这对写论文、科研分析还是挺好的。言归正传，下面从网上找到几张图片，这些图片都可以从log中通过tools画出，让我们来看一看。\n",
    "      图3表示的是不同学习率下的loss变化曲线，很明显图中的四条曲线随着迭代次数的增加表现出不同的性能，黄色的曲线随着迭代次数的增加，loss先减少而后剧烈增加，往往引发loss等于Nan，这是由于选择的学习率太大的缘故（ps：本人亲测，有几次我在修改一些模型时，开始的loss就很大，然后选择了较大的学习率，一下子就Nan了）；蓝色的曲线随着迭代次数的增加，loss的减少速率很慢很慢，而且设置的最大迭代次数已经很大，但网络并没有收敛，这说明选择的学习率太小了；绿色的曲线随着迭代次数的增加，loss的很快减少，并且网络收敛在一个loss较高的地方居高不下，这说明选择的学习率有点大了，已达到局部最优，可观察在网络loss不降时降低学习率；红色的曲线随着迭代的次数的增加，loss缓慢下降，曲线相对平滑，最终收敛在loss很低的水平上，说明选择的学习率较好。当然图中是理想的曲线，只能说明变化趋势，实际情况下曲线是有波动的，有些毛刺感（ps：大量的实践证明可以接受的就是局部最优和全局最优了，也就是红色和绿色曲线代表的过程，当然大多数同志们遇到的都是局部最优，此时我们考虑在局部最优的基础上减小学习率继续训练，两者的区别就是局部最优会保持在一个较高的loss上，当然怎么衡量loss高低没有标准，所以局部最优不代表训练结果就差，局部最优的结果也可以媲美全局最优，因为我们根本不知道全局最优在哪个地方）。\n",
    "      图4表示的是不同迭代次数的loss变化曲线，从图中可以看到随着迭代次数的增加，loss的变化趋势是减小的，注意图中标注出的“宽度”，如果曲线的宽度太大了，则说明有可能你选择的batch太小了，而其实batch的选择在深度学习中也不是随便来的，太大了不好，太小了也不好，太大了会有显存溢出的错误，太小了有可能某个label很难被学到，这往往导致模型不收敛，或者出现loss为Nan等错误。这个时候可以用accum_batch_size来解决由于硬件不足不能选择较大batch的问题。\n",
    "      图5是模型在训练集和验证集上的精度曲线。红色曲线表示的是模型在训练集上的分类精度，可以看到，还不错，随着迭代次数增加，分类的精度也在增加，并且有收敛的趋势；绿色曲线表示的是模型在验证集上的分类精度，可以看出，与训练集的精度相比，差距很大，说明模型over-fitting了，应该运用上面说到过的解决方法解决。如果图中两者之间没什么大的差距而且精度都很低，应该增加模型的capacity，提升性能。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 调参的参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.jianshu.com/p/f3a9dc6107c7\n",
    "    \n",
    "    如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。\n",
    "    先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。\n",
    "    1、超参数范围\n",
    "        建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。\n",
    "    2、经验参数\n",
    "        1） learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。\n",
    "        2）网络层数： 先从1层开始。每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。\n",
    "        3）clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15\n",
    "        4）dropout： 0.5\n",
    "\n",
    "        5）L2正则：1.0，超过10的很少见。\n",
    "\n",
    "        6）词向量embedding大小：128，256\n",
    "\n",
    "        7）正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 自动调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    1、Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。\n",
    "    2、Random Search。Bengio在Random Search for Hyper-Parameter Optimization中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。\n",
    "    3、Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文：Practical Bayesian Optimization of Machine Learning Algorithms，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：\n",
    "    jaberg/hyperopt, 比较简单。\n",
    "    fmfn/BayesianOptimization， 比较复杂，支持并行调参\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 深度学习网络调试技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    1、遇到Nan怎么办？\n",
    "        Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：\n",
    "        1）除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。后面会介绍Theano的处理办法。\n",
    "        2）梯度过大，造成更新后的值为Nan。特别是RNN，在序列比较长的时候，很容易出现梯度爆炸的问题。一般有以下几个解决办法。\n",
    "        2.1）对梯度做clip(梯度裁剪），限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。\n",
    "        2.2）减少学习率。初始学习率过大，也有可能造成这个问题。需要注意的是，即使使用adam之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。\n",
    "        3）初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。具体方法可以参考我之前的一篇文章：深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏（http://zhuanlan.zhihu.com/p/20767428）\n",
    "        \n",
    "    2、神经网络学不出东西怎么办？\n",
    "        1）请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。 \n",
    "        2）请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。\n",
    "        3）如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。\n",
    "        4）先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://blog.csdn.net/Lucifer_zzq/article/details/80537579 Pytorch 0.3 调参指南 Debug大法\n",
    "    \n",
    "    Deeplearning4j 指出了权重和偏差柱状图的期望值应该是什么样的： https://deeplearning4j.org/visualization#usingui\n",
    "    \n",
    "    https://cs231n.github.io/neural-networks-3/#baby Convolutional Neural Networks for Visual Recognition\n",
    "    \n",
    "    检查隐藏层的激活值。Deeplearning4j 中有一个很好的指导方针：“一个好的激活值标准差大约在 0.5 到 2.0 之间。明显超过这一范围可能就代表着激活值消失或爆炸。 https://deeplearning4j.org/visualization#usingui\n",
    "    \n",
    "    当你发现你的loss在训练过程中居然还上升了，那么一般来讲，是你此时的学习率设置过大了。这时候我们需要动态调整我们的学习率：\n",
    "    \n",
    "    \n",
    "    def adjust_learning_rate(optimizer, epoch, t=10):    \n",
    "        \"\"\"Sets the learning rate to the initial LR decayed by 10 every t epochs，default=10\"\"\"    \n",
    "        new_lr = lr * (0.1 ** (epoch // t))    \n",
    "        for param_group in optimizer.param_groups:        \n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开源软件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://autokeras.com/\n",
    "    \n",
    "    http://www.sohu.com/a/158622727_610300\n",
    "    \n",
    "    https://shiring.github.io/machine_learning/2017/03/07/grid_search   Hyper-parameter Tuning with Grid Search for Deep Learning\n",
    "    https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/ Bayesian Optimization for Hyperparameter Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

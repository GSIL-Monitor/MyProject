{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://en.wikipedia.org/wiki/Vanishing_gradient_problem\n",
    "    \n",
    "    http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/\n",
    "    \n",
    "    https://blog.csdn.net/comway_li/article/details/81878400\n",
    "    \n",
    "    https://blog.csdn.net/xlbryant/article/details/48470987 深度学习之收敛问题\n",
    "    \n",
    "    http://neuralnetworksanddeeplearning.com/chap5.html\n",
    "    \n",
    "    https://cn.bing.com/search?q=vanishing+gradient+problem&qs=n&form=QBLHCN&sp=-1&pq=vanishing+gradient+problem&sc=5-26&sk=&cvid=A35BE0FA7E264E38A4F4963E16969292\n",
    "    \n",
    "    http://harinisuresh.com/2016/10/09/lstms/\n",
    "    \n",
    "    In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
    "\n",
    "    Back-propagation allowed researchers to train supervised deep artificial neural networks from scratch, initially with little success. Hochreiter's diploma thesis of 1991[1][2] formally identified the reason for this failure in the \"vanishing gradient problem\", which not only affects many-layered feedforward networks,[3] but also recurrent networks.[4] The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network.\n",
    "\n",
    "    When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    record weight, grad\n",
    "    \n",
    "    def _log_gradients(self, num_layers):\n",
    "        gr = tf.get_default_graph()\n",
    "        for i in range(num_layers):\n",
    "            weight = gr.get_tensor_by_name('layer{}/kernel:0'.format(i + 1))\n",
    "            grad = tf.gradients(self.loss, weight)[0]\n",
    "            mean = tf.reduce_mean(tf.abs(grad))\n",
    "            tf.summary.scalar('mean_{}'.format(i + 1), mean)\n",
    "            tf.summary.histogram('histogram_{}'.format(i + 1), grad)\n",
    "            tf.summary.histogram('hist_weights_{}'.format(i + 1), grad)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

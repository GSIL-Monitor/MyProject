{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简析深度学习、概率建模、特征表示-灌水篇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://www.jianshu.com/p/fcfb35667058\n",
    "    \n",
    "    深度学习/神经网络和许多现有的统计学、特征表示之间关系的理解和观点。\n",
    "    \n",
    "    1. 三层架构-框架抽象\n",
    "    从模型架构的角度，我们似乎可以粗略地把绝大部分的建模架构抽象为三个组件/三大层：输入数据/特征层、特征表示/学习层和任务层。输入层可以理解成原始的特征或简单清洗后的特征，不同算法或框架的区别在第二大层和第三层上（这里的第二层是为了抽象的框架的概念，中间可以包含多层子层），比如最简单的是没有中间层，直接把输入层输出给任务层做分类；传统的人工特征工程在第二大层表现为求sin、平方、开方等简单的数学方法，产生新的特征，然后输出给第三层做分类；SVM第二层通过Kernel Method将输入层特征“升维”到高维空间张成新的特征向量，以便在第三层更容易分类；同理GBDT也是在第二层进行树形特征学习和表示，然后经过第三层的修正优化得到漂亮的分类结果；而今天的主角深度学习（网络）则是在第二层通过多层子层进行特征的学习和传递，同样通过第三层进行任务的修正优化，得到更优的结果。再进一步抽象，第三层的表示大部分可以通过GLM（广义线性模型）进行表示。特别地，深度学习网络中间的每一子层也都可以看成上一层的输入和权重的线性组合，并把结果封装传入一个简单的非线性函数（GLM的连接函数），最后通过第三大层的进行统一的优化。可以发现随着研究的进步，总的框架其实没有巨大变化，只是在中间特征学习的智能性或线性可分性的转化上是在不断的进化，本质都在做特征的抽取和学习。\n",
    "    深度学习其实很关键的一点就是得到好的特征表示（representation），通过深度学习第二层学习的出来的网络，即使抛弃第三层的分类/回归模型，直接把学习到的网络（参数）当做新的特征，把这堆特征丢到普通的 LR之类的分类器里，往往也会得到分类性能提高。\n",
    "    \n",
    "    2. 特征学习/表示-杂谈\n",
    "    我们在建模的时候，非常重要且往往决定结果好坏的环节是特征的提取，也就是我们谈的第二层的内容，而特征的提取无非就人工和机器自动提取两种，有一个有意思的观点认为无论是深度学习还是人工特征浅层学习，目的都是有目的的进行“信息丢失”的过程，其实这个可以从数学上严格证明(参见：Data Processing Inequality)，因为在不添加新数据的条件下，原始数据包含的信息量是最大的，之后对其进行处理和特征工程，信息就是在逐步损失掉，那么有人可能要疑问了：既然只要做特征，信息就减少，岂不是效果会变差吗？其实，并不矛盾，我们容易想到，无论建立何种模型、进行何种特征学习都是为了完成某个领域的任务，那么进一步可以想到原始数据中一定包含该任务下不需要的信息，所以理论上，我们做建模和特征工程理想情况是努力把为完成某项任务不需要的信息损失掉，从而使得模型效果反而提高，其实对应人脑信息损失的过程也可以理解为“抽象”的过程。例如，我们大脑储存了大量的信息和记忆，如果给定一个任务去判断某动物是不是狗，那么人通常会是从脑中记忆的大量信息中丢弃与狗无关的信息，并把狗想关的多种扁平特征抽象到一个高维“立体”空间来进行判断，最终得到该动物是否为狗的识别。\n",
    "    “抽象”在数学概念里（与我们今天讨论话题相关的概念）可以用一个词叫“线性可分性”来约等于，很面熟的一个概念，很多人对这个概念的了解可能会出自于流行多年的SVM，大概的过程是判断原始特征是否“线性可分”，如果不可分，则采用核方法将原始特征“升维”以获得更丰富的数据表达，以实现在新的空间里“线性可分”，核方法是非参数的思想，基本思路是通过一个正定核，得到一个线性映射将数据映射到一个RKHS（ Reproducing Kernel Hilbert Space）中，然后使用RKHS中的线性模型来处理数据，可能有人要疑问了，为啥一定要“线性可分”而不是直接找一个非线性的表达来做。道理很简单，线性函数非常容易表达和估计，我们无法找到一个具体的非线性表达来对原始数据进行抽象，所以不论是SVM也好还是神经网络，本质都是通过可以抽象出来的通用变换来产生新的空间，并使得输入层可以在新的空间找到线性表达完成模型任务。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 金融中的三种深度学习用例及这些模型优劣的证据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    http://www.woshipm.com/it/895926.html\n",
    "    \n",
    "    虽然金融是计算密集型最多的领域，但广泛使用的金融模型：监督和无监督模型、基于状态的模型、计量经济学模型甚至随机模型都受到过度拟合和启发式问题带来的影响，抽样结果很差。因为金融生态圈异常复杂，其非线性充斥着大量的相互影响的因素。\n",
    "    \n",
    "    一、收益预测\n",
    "    以预测每日黄金价格的抽样问题为例，我们首先看看传统的方法\n",
    "    1、ARIMA 模型\n",
    "        ARIMA 模型（Autoregressive Integrated Moving Average model），差分整合移动平均自回归模型，又称整合移动平均自回归模型（移动也可称作滑动），时间序列预测分析方法之一。ARIMA（p，d，q）中，AR 是“自回归”，p 为自回归项数；MA 为“滑动平均”，q 为滑动平均项数，d 为使之成为平稳序列所做的差分次数（阶数）。“差分”一词虽未出现在 ARIMA 的英文名称中，却是关键步骤。\n",
    "    ARIMA 模型的基本思想是：将预测对象随时间推移而形成的数据序列视为一个随机序列，用一定的数学模型来近似描述这个序列。这个模型一旦被识别后就可以从时间序列的过去值及现在值来预测未来值。现代统计方法、计量经济模型在某种程度上已经能够帮助企业对未来进行预测。利用整合移动平均自回归模型，来尝试预测季节性平稳时间序列，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    机器学习模型无法得到较低训练误差\n",
    "    \n",
    "    拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。。但是如果真的还是存在的话，可以通过增加网络复杂度或者在模型中增加多点特征点，这些都是很好解决欠拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://blog.csdn.net/liuy9803/article/details/81611402\n",
    "    \n",
    "    https://www.zhihu.com/question/59201590  机器学习中用来防止过拟合的方法有哪些\n",
    "    \n",
    "    \n",
    "    \n",
    "    机器学习模型的训练误差远小于其在测试数据集上的误差\n",
    "    \n",
    "    产生过拟合的根本原因在于：\n",
    "    \n",
    "    （1）观测值与真实值之间的误差：\n",
    "        训练样本=真实值+随机误差，学习时尽可能地拟合了训练样本，而不是真实值，即学到了真实规律以外的随机误差。\n",
    "    （2）数据太少，无法反映真实分布；\n",
    "    （3）数据含有噪声，模型覆盖了噪音点；\n",
    "    （4）模型训练过度，非常复杂。    \n",
    "    \n",
    "    解决方法如下：\n",
    "    \n",
    "    （1）获取更多数据：从数据源获得更多数据，或数据增强；\n",
    "    （2）数据预处理：清洗数据、减少特征维度、类别平衡；\n",
    "    （3）增加噪声：输入时+权重上（高斯初始化）；\n",
    "    （4）正则化：限制权重过大、网络层数过多，避免模型过于复杂；\n",
    "    （5）多种模型结合：集成学习的思想；\n",
    "    （6）Dropout：随机从网络中去掉一部分隐神经元；\n",
    "    （7）限制训练时间、次数，及早停止。\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

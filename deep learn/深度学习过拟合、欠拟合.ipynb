{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简析深度学习、概率建模、特征表示-灌水篇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://www.jianshu.com/p/fcfb35667058\n",
    "    \n",
    "    深度学习/神经网络和许多现有的统计学、特征表示之间关系的理解和观点。\n",
    "    \n",
    "    1. 三层架构-框架抽象\n",
    "    从模型架构的角度，我们似乎可以粗略地把绝大部分的建模架构抽象为三个组件/三大层：输入数据/特征层、特征表示/学习层和任务层。输入层可以理解成原始的特征或简单清洗后的特征，不同算法或框架的区别在第二大层和第三层上（这里的第二层是为了抽象的框架的概念，中间可以包含多层子层），比如最简单的是没有中间层，直接把输入层输出给任务层做分类；传统的人工特征工程在第二大层表现为求sin、平方、开方等简单的数学方法，产生新的特征，然后输出给第三层做分类；SVM第二层通过Kernel Method将输入层特征“升维”到高维空间张成新的特征向量，以便在第三层更容易分类；同理GBDT也是在第二层进行树形特征学习和表示，然后经过第三层的修正优化得到漂亮的分类结果；而今天的主角深度学习（网络）则是在第二层通过多层子层进行特征的学习和传递，同样通过第三层进行任务的修正优化，得到更优的结果。再进一步抽象，第三层的表示大部分可以通过GLM（广义线性模型）进行表示。特别地，深度学习网络中间的每一子层也都可以看成上一层的输入和权重的线性组合，并把结果封装传入一个简单的非线性函数（GLM的连接函数），最后通过第三大层的进行统一的优化。可以发现随着研究的进步，总的框架其实没有巨大变化，只是在中间特征学习的智能性或线性可分性的转化上是在不断的进化，本质都在做特征的抽取和学习。\n",
    "    深度学习其实很关键的一点就是得到好的特征表示（representation），通过深度学习第二层学习的出来的网络，即使抛弃第三层的分类/回归模型，直接把学习到的网络（参数）当做新的特征，把这堆特征丢到普通的 LR之类的分类器里，往往也会得到分类性能提高。\n",
    "    \n",
    "    2. 特征学习/表示-杂谈\n",
    "    我们在建模的时候，非常重要且往往决定结果好坏的环节是特征的提取，也就是我们谈的第二层的内容，而特征的提取无非就人工和机器自动提取两种，有一个有意思的观点认为无论是深度学习还是人工特征浅层学习，目的都是有目的的进行“信息丢失”的过程，其实这个可以从数学上严格证明(参见：Data Processing Inequality)，因为在不添加新数据的条件下，原始数据包含的信息量是最大的，之后对其进行处理和特征工程，信息就是在逐步损失掉，那么有人可能要疑问了：既然只要做特征，信息就减少，岂不是效果会变差吗？其实，并不矛盾，我们容易想到，无论建立何种模型、进行何种特征学习都是为了完成某个领域的任务，那么进一步可以想到原始数据中一定包含该任务下不需要的信息，所以理论上，我们做建模和特征工程理想情况是努力把为完成某项任务不需要的信息损失掉，从而使得模型效果反而提高，其实对应人脑信息损失的过程也可以理解为“抽象”的过程。例如，我们大脑储存了大量的信息和记忆，如果给定一个任务去判断某动物是不是狗，那么人通常会是从脑中记忆的大量信息中丢弃与狗无关的信息，并把狗想关的多种扁平特征抽象到一个高维“立体”空间来进行判断，最终得到该动物是否为狗的识别。\n",
    "    “抽象”在数学概念里（与我们今天讨论话题相关的概念）可以用一个词叫“线性可分性”来约等于，很面熟的一个概念，很多人对这个概念的了解可能会出自于流行多年的SVM，大概的过程是判断原始特征是否“线性可分”，如果不可分，则采用核方法将原始特征“升维”以获得更丰富的数据表达，以实现在新的空间里“线性可分”，核方法是非参数的思想，基本思路是通过一个正定核，得到一个线性映射将数据映射到一个RKHS（ Reproducing Kernel Hilbert Space）中，然后使用RKHS中的线性模型来处理数据，可能有人要疑问了，为啥一定要“线性可分”而不是直接找一个非线性的表达来做。道理很简单，线性函数非常容易表达和估计，我们无法找到一个具体的非线性表达来对原始数据进行抽象，所以不论是SVM也好还是神经网络，本质都是通过可以抽象出来的通用变换来产生新的空间，并使得输入层可以在新的空间找到线性表达完成模型任务。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 金融中的三种深度学习用例及这些模型优劣的证据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    http://www.woshipm.com/it/895926.html\n",
    "    \n",
    "    虽然金融是计算密集型最多的领域，但广泛使用的金融模型：监督和无监督模型、基于状态的模型、计量经济学模型甚至随机模型都受到过度拟合和启发式问题带来的影响，抽样结果很差。因为金融生态圈异常复杂，其非线性充斥着大量的相互影响的因素。\n",
    "    \n",
    "    一、收益预测\n",
    "    以预测每日黄金价格的抽样问题为例，我们首先看看传统的方法\n",
    "    1、ARIMA 模型\n",
    "        ARIMA 模型（Autoregressive Integrated Moving Average model），差分整合移动平均自回归模型，又称整合移动平均自回归模型（移动也可称作滑动），时间序列预测分析方法之一。ARIMA（p，d，q）中，AR 是“自回归”，p 为自回归项数；MA 为“滑动平均”，q 为滑动平均项数，d 为使之成为平稳序列所做的差分次数（阶数）。“差分”一词虽未出现在 ARIMA 的英文名称中，却是关键步骤。\n",
    "    ARIMA 模型的基本思想是：将预测对象随时间推移而形成的数据序列视为一个随机序列，用一定的数学模型来近似描述这个序列。这个模型一旦被识别后就可以从时间序列的过去值及现在值来预测未来值。现代统计方法、计量经济模型在某种程度上已经能够帮助企业对未来进行预测。利用整合移动平均自回归模型，来尝试预测季节性平稳时间序列。\n",
    "    2、VAR 模型\n",
    "        （Vector Autoregression model）向量自回归模型，是一种常用的计量经济模型，由计量经济学家和宏观经济学家 Christopher Sims 提出。它扩充了只能使用一个变量的自回归模型（简称：AR 模型），使容纳大于 1 个变量，因此经常用在多变量时间序列模型的分析上。\n",
    "    3、深度回归模型\n",
    "        如果在数据上使用简单的深度回归模型，使用相同的输入，会得到更好的结果，\n",
    "    4、卷积神经网络\n",
    "    卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。修改我的架构，使用卷积神经网络来解决同一个问题\n",
    "    5、长短期记忆网络\n",
    "    \n",
    "    二、投资组合构建\n",
    "    我们尝试使用深度学习解决的第二个金融问题是投资组合构建。在这个问题上，深度学习的实际应用效果很好。我的研究灵感来自这篇论文：《深度投资组合》（https://arxiv.org/pdf/1602.06561v2.pdf）\n",
    "    这篇论文的作者尝试构建自动编码器，将时间序列映射到自身。使用这些自动编码器的预测误差成为股票测试版（与市场相关）的代用指标，自动编码器用作市场的模型。自动编码器（auto-encoder），是一种无监督的学习算法，主要用于数据的降维或者特征的抽取，在深度学习中，自动编码器可用于在训练阶段开始前，确定权重矩阵 W 的初始值。基于上述自动编码器的误差选择不同的股票，我们可以使用另一个深度神经网络来构建深度指标，结果相当不错，深度神经网络已成为利用股票复制指数的指数构建方法。但这只是它的开始！如果我们应用智能索引，在我去掉指数的极端下降期，并在智能索引上训练我的指数映射深度神经网络时，我就能以惊人的速度超过指数！\n",
    "    这种技术在证券投资组合领域有着巨大的潜力！\n",
    "    \n",
    "    译者感言：\n",
    "    \n",
    "    深度学习的一大优势在于可以大幅减少人工参与的特征工程去“拟合”训练数据，但这也不是说完全不需要人去参与特征的选取，尤其是金融市场，数据简直是海量，并且大都高噪声，非稳定，所以除非你能够清楚哪些数据具有潜在价值、如何做适当的预处理和如何转化并达成哪些目标，否则深度学习在金融领域是无法应用的。\n",
    "    \n",
    "    如果设计得当，增加神经网络的深度可以对更复杂的模式进行映射，因此可对金融数据的训练产生更好效果。\n",
    "    例如 CNN 适合处理图像一类的大数据，不一定直接适用于金融数据。金融数据虽然也可以很大，但通常不在一个数量级上——一幅图像中的样本数（像素、颜色）很容易上百万，要训练一个模型又要用到成千上万个图像，为了找到高效办法提取特征，于是计算机科学家想出了卷积神经网络通过抽取特征值来简化运算复杂度。而我们处理的金融时间序列数据，单组样本的数量常在百万以下，规模上往往不需要卷积。进一步比较，相对于金融数据，图像识别的特点是数据量庞大、特征明显、特征重复次数多、特征组合性强、出现的次序性、特征呈现的方向性有时不强，这与金融数据噪音大、数量较小的特点有所不同。因此，在训练金融数据时很有必要设计有针对性的深度机器学习模型。\n",
    "\n",
    "    本文涉及到的代码可在作者的 GitHub Repo 中找到https://github.com/sonaam1234/DeepLearningInFinance\n",
    "    \n",
    "    \n",
    "    Portfolio Construction - A smart indexing based portfolio construction exercise was undertaken using deep neural networks and autoencoders based on http://onlinelibrary.wiley.com/doi/10.1002/asmb.2209/pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    机器学习模型无法得到较低训练误差\n",
    "    \n",
    "    拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。。但是如果真的还是存在的话，可以通过增加网络复杂度或者在模型中增加多点特征点，这些都是很好解决欠拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://blog.csdn.net/liuy9803/article/details/81611402\n",
    "    \n",
    "    https://www.zhihu.com/question/59201590  机器学习中用来防止过拟合的方法有哪些\n",
    "    \n",
    "    \n",
    "    \n",
    "    机器学习模型的训练误差远小于其在测试数据集上的误差\n",
    "    \n",
    "    产生过拟合的根本原因在于：\n",
    "    \n",
    "    （1）观测值与真实值之间的误差：\n",
    "        训练样本=真实值+随机误差，学习时尽可能地拟合了训练样本，而不是真实值，即学到了真实规律以外的随机误差。\n",
    "    （2）数据太少，无法反映真实分布；\n",
    "    （3）数据含有噪声，模型覆盖了噪音点；\n",
    "    （4）模型训练过度，非常复杂。    \n",
    "    \n",
    "    解决方法如下：\n",
    "    \n",
    "    （1）获取更多数据：从数据源获得更多数据，或数据增强；\n",
    "    （2）数据预处理：清洗数据、减少特征维度、类别平衡；\n",
    "    （3）增加噪声：输入时+权重上（高斯初始化）；\n",
    "    （4）正则化：限制权重过大、网络层数过多，避免模型过于复杂；\n",
    "    （5）多种模型结合：集成学习的思想；\n",
    "    （6）Dropout：随机从网络中去掉一部分隐神经元；\n",
    "    （7）限制训练时间、次数，及早停止。\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 卷积网络 P350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "  1、卷积网络反向传播  P377\n",
    "  2、结构化输出    P378\n",
    "  \n",
    "  \n",
    "  P385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 序列模型和循环网络  P394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    1、导师驱动过程和输出循环网络 P400\n",
    "    2、基于编码--解码的序列到序列结构  P413\n",
    "    3、深度循环网络  P415\n",
    "    4、递归神经网络  P417\n",
    "    5、LSTM\n",
    "    6、截断梯度\n",
    "    7、外显记忆   P432\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 实践方法论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    1、性能度量  P437\n",
    "    2、默认的基准模型 P439\n",
    "    3、调试策略 P450\n",
    "       3.1 监控激活函数值和梯度的直方图  P453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用  P455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1、大规模的分布实现  P458\n",
    "    2、模型压缩  P459\n",
    "    3、计算机视觉  P464\n",
    "    4、图像数据集增强  P469\n",
    "    \n",
    "    5、语音识别  P469\n",
    "    \n",
    "    6、自然语言处理  P473\n",
    "       6.1、N-GRAM\n",
    "       6.2、神经语文模型  P475\n",
    "       6.3、解决词汇量大的情部下 softmax. P478   .注：可参看一下词向量那部分的实现\n",
    "       6.4、噪声对比估计和排名损失  P482\n",
    "       6.5、结合n-gram和神经语言模型 P483\n",
    "               N-Gram是大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型(CLM, Chinese Language Model)。汉语语言模型利用上下文中相邻词间的搭配信息，可以实现到汉字的自动转换，中文名 汉语语言模型 外文名 N-Gram 定    义 计算出具有最大概率的句子 基    于 该模型基于这样一种假设汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔划，或代表字母或笔划的数字，转换成汉字串(即句子)时，可以计算出具有最大概率的句子，从而实现到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音(或笔划串，或数字串)的重码问题。该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。\n",
    "       \n",
    "       6.6、神经机器翻译  P484\n",
    "       6.7、使用注意力机制  P486\n",
    "       \n",
    "           RNN  http://ai.51cto.com/art/201711/559441.htm  https://blog.csdn.net/zhaojc1995/article/details/80572098  https://blog.csdn.net/m0_37306360/article/details/79316013\n",
    "           \n",
    "    7、推荐系统  P488\n",
    "    8、知识表示、推理及回答  P492\n",
    "       \n",
    "       \n",
    "P479"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

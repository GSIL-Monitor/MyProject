{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 需用到的知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    1、Wavelet transform 小波变换  https://www.zhihu.com/question/22864189/answer/40772083\n",
    "        Consequently, wavelet is useful in handling highly irregular financial time series\n",
    "            https://www.researchgate.net/publication/223249583_Forecasting_stock_markets_using_wavelet_transforms_and_recurrent_neural_networks_An_integrated_system_based_on_artificial_bee_colony_algorithm\n",
    "            \n",
    "        https://pywavelets.readthedocs.io/en/latest/regression/index.html\n",
    "        \n",
    "        https://blog.csdn.net/zhaoyuxia517/article/details/78005713 基于小波变换的时间序列预测，Python实现\n",
    "        \n",
    "        http://www.docin.com/p-1546109075.html\n",
    "        \n",
    "        http://www.docin.com/p-1072414208.html?docfrom=rrela\n",
    "        \n",
    "        https://www.zhihu.com/question/19725983/answer/13856998\n",
    "        \n",
    "        http://www.360doc.com/content/13/0614/15/10724725_292827411.shtml\n",
    "            \n",
    "    2、Stacked autoencoders\n",
    "        https://blog.csdn.net/jningwei/article/details/78836823\n",
    "    \n",
    "    另注意： \n",
    "    什么Autoencoder啦、RBM啦，现在都已经 没人用了 。\n",
    "    现在所常说的 pre-training (预训练) ，其实 专指 migration learning (迁移学习)，那是一种无比强大又省事儿的trick。\n",
    "    \n",
    "        https://blog.csdn.net/hai008007/article/details/79676994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# 包含部分指标数据\n",
    "from stock.common.sdata import StockData\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "code = '600016'\n",
    "sd = StockData(code)\n",
    "data = sd.combine_income(ndays=5)\n",
    "key_defines = ['RSI','MACD','ADOSC','OBV','MFI','BBANDS','BOP','ADX','EMA','MA']\n",
    "# 指标数据\n",
    "\n",
    "data = StockData.combine_index_data(data,index_keys=key_defines)\n",
    "\n",
    "use_column = [x not in ['Flag','INCOME'] for x in data.columns.get_level_values(0)]\n",
    "data.iloc[:,use_column] =  \\\n",
    "     data.iloc[:,use_column].apply(lambda x: (x-np.mean(x))/np.std(x))\n",
    "target= data.loc[:,'Flag'].values\n",
    "\n",
    "data_values = data.iloc[:,use_column].values\n",
    "dlen = int(len(data_values)*0.8)\n",
    "train_data = torch.from_numpy(np.array(data_values[0:dlen])).float()\n",
    "train_target = target[0:dlen]\n",
    "\n",
    "test_data = torch.from_numpy(np.array(data_values[dlen:])).float()\n",
    "test_target = target[dlen:]\n",
    "dataset = TensorDataset(train_data)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "n_feature = len(data.columns) - 2\n",
    "print(n_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencode模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自编码主要用于数据降维\n",
    "import torch.nn as nn\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_feature, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3,8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8,16),\n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(16, n_feature)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载上次训练模型...\n",
      "epcho --> 0 ： loss 0.088\n",
      "epcho --> 5 ： loss 0.088\n",
      "epcho --> 10 ： loss 0.088\n",
      "epcho --> 15 ： loss 0.089\n",
      "epcho --> 20 ： loss 0.088\n",
      "epcho --> 25 ： loss 0.089\n",
      "epcho --> 30 ： loss 0.088\n",
      "epcho --> 35 ： loss 0.089\n",
      "epcho --> 40 ： loss 0.088\n",
      "epcho --> 45 ： loss 0.089\n",
      "epcho --> 50 ： loss 0.088\n",
      "epcho --> 55 ： loss 0.088\n",
      "epcho --> 60 ： loss 0.088\n",
      "epcho --> 65 ： loss 0.088\n",
      "epcho --> 70 ： loss 0.088\n",
      "epcho --> 75 ： loss 0.088\n",
      "epcho --> 80 ： loss 0.089\n",
      "epcho --> 85 ： loss 0.088\n",
      "epcho --> 90 ： loss 0.088\n",
      "epcho --> 95 ： loss 0.088\n",
      "epcho --> 100 ： loss 0.088\n",
      "epcho --> 105 ： loss 0.088\n",
      "epcho --> 110 ： loss 0.088\n",
      "epcho --> 115 ： loss 0.088\n",
      "epcho --> 120 ： loss 0.088\n",
      "epcho --> 125 ： loss 0.088\n",
      "epcho --> 130 ： loss 0.088\n",
      "epcho --> 135 ： loss 0.088\n",
      "epcho --> 140 ： loss 0.088\n",
      "epcho --> 145 ： loss 0.088\n",
      "epcho --> 150 ： loss 0.089\n",
      "epcho --> 155 ： loss 0.088\n",
      "epcho --> 160 ： loss 0.089\n",
      "epcho --> 165 ： loss 0.088\n",
      "epcho --> 170 ： loss 0.088\n",
      "epcho --> 175 ： loss 0.088\n",
      "epcho --> 180 ： loss 0.088\n",
      "epcho --> 185 ： loss 0.089\n",
      "epcho --> 190 ： loss 0.088\n",
      "epcho --> 195 ： loss 0.088\n",
      "epcho --> 200 ： loss 0.088\n",
      "epcho --> 205 ： loss 0.088\n",
      "epcho --> 210 ： loss 0.088\n",
      "epcho --> 215 ： loss 0.088\n",
      "epcho --> 220 ： loss 0.088\n",
      "epcho --> 225 ： loss 0.088\n",
      "epcho --> 230 ： loss 0.088\n",
      "epcho --> 235 ： loss 0.088\n",
      "epcho --> 240 ： loss 0.088\n",
      "epcho --> 245 ： loss 0.088\n",
      "epcho --> 250 ： loss 0.088\n",
      "epcho --> 255 ： loss 0.088\n",
      "epcho --> 260 ： loss 0.088\n",
      "epcho --> 265 ： loss 0.088\n",
      "epcho --> 270 ： loss 0.088\n",
      "epcho --> 275 ： loss 0.088\n",
      "epcho --> 280 ： loss 0.088\n",
      "epcho --> 285 ： loss 0.088\n",
      "epcho --> 290 ： loss 0.088\n",
      "epcho --> 295 ： loss 0.088\n",
      "epcho --> 300 ： loss 0.088\n",
      "epcho --> 305 ： loss 0.088\n",
      "epcho --> 310 ： loss 0.088\n",
      "epcho --> 315 ： loss 0.088\n",
      "epcho --> 320 ： loss 0.088\n",
      "epcho --> 325 ： loss 0.088\n",
      "epcho --> 330 ： loss 0.088\n",
      "epcho --> 335 ： loss 0.088\n",
      "epcho --> 340 ： loss 0.088\n",
      "epcho --> 345 ： loss 0.089\n",
      "epcho --> 350 ： loss 0.088\n",
      "epcho --> 355 ： loss 0.088\n",
      "epcho --> 360 ： loss 0.088\n",
      "epcho --> 365 ： loss 0.088\n",
      "epcho --> 370 ： loss 0.089\n",
      "epcho --> 375 ： loss 0.088\n",
      "epcho --> 380 ： loss 0.088\n",
      "epcho --> 385 ： loss 0.088\n",
      "epcho --> 390 ： loss 0.088\n",
      "epcho --> 395 ： loss 0.088\n",
      "epcho --> 400 ： loss 0.088\n",
      "epcho --> 405 ： loss 0.089\n",
      "epcho --> 410 ： loss 0.088\n",
      "epcho --> 415 ： loss 0.088\n",
      "epcho --> 420 ： loss 0.088\n",
      "epcho --> 425 ： loss 0.088\n",
      "epcho --> 430 ： loss 0.088\n",
      "epcho --> 435 ： loss 0.088\n",
      "epcho --> 440 ： loss 0.089\n",
      "epcho --> 445 ： loss 0.088\n",
      "epcho --> 450 ： loss 0.088\n",
      "epcho --> 455 ： loss 0.088\n",
      "epcho --> 460 ： loss 0.088\n",
      "epcho --> 465 ： loss 0.088\n",
      "epcho --> 470 ： loss 0.088\n",
      "epcho --> 475 ： loss 0.088\n",
      "epcho --> 480 ： loss 0.088\n",
      "epcho --> 485 ： loss 0.088\n",
      "epcho --> 490 ： loss 0.088\n",
      "epcho --> 495 ： loss 0.088\n",
      "epcho --> 500 ： loss 0.088\n",
      "epcho --> 505 ： loss 0.088\n",
      "epcho --> 510 ： loss 0.088\n",
      "epcho --> 515 ： loss 0.088\n",
      "epcho --> 520 ： loss 0.088\n",
      "epcho --> 525 ： loss 0.088\n",
      "epcho --> 530 ： loss 0.088\n",
      "epcho --> 535 ： loss 0.088\n",
      "epcho --> 540 ： loss 0.088\n",
      "epcho --> 545 ： loss 0.088\n",
      "epcho --> 550 ： loss 0.088\n",
      "epcho --> 555 ： loss 0.088\n",
      "epcho --> 560 ： loss 0.088\n",
      "epcho --> 565 ： loss 0.088\n",
      "epcho --> 570 ： loss 0.088\n",
      "epcho --> 575 ： loss 0.088\n",
      "epcho --> 580 ： loss 0.088\n",
      "epcho --> 585 ： loss 0.088\n",
      "epcho --> 590 ： loss 0.088\n",
      "epcho --> 595 ： loss 0.088\n",
      "epcho --> 600 ： loss 0.088\n",
      "epcho --> 605 ： loss 0.088\n",
      "epcho --> 610 ： loss 0.088\n",
      "epcho --> 615 ： loss 0.088\n",
      "epcho --> 620 ： loss 0.088\n",
      "epcho --> 625 ： loss 0.088\n",
      "epcho --> 630 ： loss 0.088\n",
      "epcho --> 635 ： loss 0.088\n",
      "epcho --> 640 ： loss 0.088\n",
      "epcho --> 645 ： loss 0.088\n",
      "epcho --> 650 ： loss 0.088\n",
      "epcho --> 655 ： loss 0.088\n",
      "epcho --> 660 ： loss 0.088\n",
      "epcho --> 665 ： loss 0.088\n",
      "epcho --> 670 ： loss 0.088\n",
      "epcho --> 675 ： loss 0.088\n",
      "epcho --> 680 ： loss 0.088\n",
      "epcho --> 685 ： loss 0.088\n",
      "epcho --> 690 ： loss 0.088\n",
      "epcho --> 695 ： loss 0.088\n",
      "epcho --> 700 ： loss 0.088\n",
      "epcho --> 705 ： loss 0.088\n",
      "epcho --> 710 ： loss 0.088\n",
      "epcho --> 715 ： loss 0.088\n",
      "epcho --> 720 ： loss 0.088\n",
      "epcho --> 725 ： loss 0.088\n",
      "epcho --> 730 ： loss 0.088\n",
      "epcho --> 735 ： loss 0.088\n",
      "epcho --> 740 ： loss 0.088\n",
      "epcho --> 745 ： loss 0.088\n",
      "epcho --> 750 ： loss 0.088\n",
      "epcho --> 755 ： loss 0.088\n",
      "epcho --> 760 ： loss 0.088\n",
      "epcho --> 765 ： loss 0.088\n",
      "epcho --> 770 ： loss 0.088\n",
      "epcho --> 775 ： loss 0.088\n",
      "epcho --> 780 ： loss 0.088\n",
      "epcho --> 785 ： loss 0.088\n",
      "epcho --> 790 ： loss 0.088\n",
      "epcho --> 795 ： loss 0.088\n",
      "epcho --> 800 ： loss 0.088\n",
      "epcho --> 805 ： loss 0.088\n",
      "epcho --> 810 ： loss 0.088\n",
      "epcho --> 815 ： loss 0.088\n",
      "epcho --> 820 ： loss 0.088\n",
      "epcho --> 825 ： loss 0.088\n",
      "epcho --> 830 ： loss 0.088\n",
      "epcho --> 835 ： loss 0.088\n",
      "epcho --> 840 ： loss 0.088\n",
      "epcho --> 845 ： loss 0.088\n",
      "epcho --> 850 ： loss 0.088\n",
      "epcho --> 855 ： loss 0.088\n",
      "epcho --> 860 ： loss 0.088\n",
      "epcho --> 865 ： loss 0.088\n",
      "epcho --> 870 ： loss 0.088\n",
      "epcho --> 875 ： loss 0.088\n",
      "epcho --> 880 ： loss 0.088\n",
      "epcho --> 885 ： loss 0.088\n",
      "epcho --> 890 ： loss 0.088\n",
      "epcho --> 895 ： loss 0.088\n",
      "epcho --> 900 ： loss 0.088\n",
      "epcho --> 905 ： loss 0.088\n",
      "epcho --> 910 ： loss 0.088\n",
      "epcho --> 915 ： loss 0.088\n",
      "epcho --> 920 ： loss 0.088\n",
      "epcho --> 925 ： loss 0.088\n",
      "epcho --> 930 ： loss 0.088\n",
      "epcho --> 935 ： loss 0.088\n",
      "epcho --> 940 ： loss 0.088\n",
      "epcho --> 945 ： loss 0.088\n",
      "epcho --> 950 ： loss 0.088\n",
      "epcho --> 955 ： loss 0.088\n",
      "epcho --> 960 ： loss 0.088\n",
      "epcho --> 965 ： loss 0.088\n",
      "epcho --> 970 ： loss 0.088\n",
      "epcho --> 975 ： loss 0.088\n",
      "epcho --> 980 ： loss 0.088\n",
      "epcho --> 985 ： loss 0.088\n",
      "epcho --> 990 ： loss 0.088\n",
      "epcho --> 995 ： loss 0.088\n",
      "epcho --> 1000 ： loss 0.088\n",
      "epcho --> 1005 ： loss 0.088\n",
      "epcho --> 1010 ： loss 0.088\n",
      "epcho --> 1015 ： loss 0.088\n",
      "epcho --> 1020 ： loss 0.088\n",
      "epcho --> 1025 ： loss 0.088\n",
      "epcho --> 1030 ： loss 0.088\n",
      "epcho --> 1035 ： loss 0.088\n",
      "epcho --> 1040 ： loss 0.088\n",
      "epcho --> 1045 ： loss 0.088\n",
      "epcho --> 1050 ： loss 0.088\n",
      "epcho --> 1055 ： loss 0.088\n",
      "epcho --> 1060 ： loss 0.088\n",
      "epcho --> 1065 ： loss 0.088\n",
      "epcho --> 1070 ： loss 0.088\n",
      "epcho --> 1075 ： loss 0.088\n",
      "epcho --> 1080 ： loss 0.088\n",
      "epcho --> 1085 ： loss 0.088\n",
      "epcho --> 1090 ： loss 0.088\n",
      "epcho --> 1095 ： loss 0.088\n",
      "epcho --> 1100 ： loss 0.088\n",
      "epcho --> 1105 ： loss 0.088\n",
      "epcho --> 1110 ： loss 0.088\n",
      "epcho --> 1115 ： loss 0.088\n",
      "epcho --> 1120 ： loss 0.088\n",
      "epcho --> 1125 ： loss 0.088\n",
      "epcho --> 1130 ： loss 0.088\n",
      "epcho --> 1135 ： loss 0.088\n",
      "epcho --> 1140 ： loss 0.088\n",
      "epcho --> 1145 ： loss 0.088\n",
      "epcho --> 1150 ： loss 0.088\n",
      "epcho --> 1155 ： loss 0.088\n",
      "epcho --> 1160 ： loss 0.088\n",
      "epcho --> 1165 ： loss 0.088\n",
      "epcho --> 1170 ： loss 0.088\n",
      "epcho --> 1175 ： loss 0.088\n",
      "epcho --> 1180 ： loss 0.088\n",
      "epcho --> 1185 ： loss 0.088\n",
      "epcho --> 1190 ： loss 0.088\n",
      "epcho --> 1195 ： loss 0.088\n",
      "epcho --> 1200 ： loss 0.088\n",
      "epcho --> 1205 ： loss 0.088\n",
      "epcho --> 1210 ： loss 0.088\n",
      "epcho --> 1215 ： loss 0.088\n",
      "epcho --> 1220 ： loss 0.088\n",
      "epcho --> 1225 ： loss 0.088\n",
      "epcho --> 1230 ： loss 0.088\n",
      "epcho --> 1235 ： loss 0.088\n",
      "epcho --> 1240 ： loss 0.088\n",
      "epcho --> 1245 ： loss 0.088\n",
      "epcho --> 1250 ： loss 0.088\n",
      "epcho --> 1255 ： loss 0.088\n",
      "epcho --> 1260 ： loss 0.088\n",
      "epcho --> 1265 ： loss 0.088\n",
      "epcho --> 1270 ： loss 0.088\n",
      "epcho --> 1275 ： loss 0.088\n",
      "epcho --> 1280 ： loss 0.088\n",
      "epcho --> 1285 ： loss 0.088\n",
      "epcho --> 1290 ： loss 0.088\n",
      "epcho --> 1295 ： loss 0.088\n",
      "epcho --> 1300 ： loss 0.088\n",
      "epcho --> 1305 ： loss 0.088\n",
      "epcho --> 1310 ： loss 0.088\n",
      "epcho --> 1315 ： loss 0.088\n",
      "epcho --> 1320 ： loss 0.088\n",
      "epcho --> 1325 ： loss 0.088\n",
      "epcho --> 1330 ： loss 0.088\n",
      "epcho --> 1335 ： loss 0.088\n",
      "epcho --> 1340 ： loss 0.088\n",
      "epcho --> 1345 ： loss 0.088\n",
      "epcho --> 1350 ： loss 0.088\n",
      "epcho --> 1355 ： loss 0.088\n",
      "epcho --> 1360 ： loss 0.088\n",
      "epcho --> 1365 ： loss 0.088\n",
      "epcho --> 1370 ： loss 0.088\n",
      "epcho --> 1375 ： loss 0.088\n",
      "epcho --> 1380 ： loss 0.088\n",
      "epcho --> 1385 ： loss 0.088\n",
      "epcho --> 1390 ： loss 0.088\n",
      "epcho --> 1395 ： loss 0.088\n",
      "epcho --> 1400 ： loss 0.088\n",
      "epcho --> 1405 ： loss 0.088\n",
      "epcho --> 1410 ： loss 0.088\n",
      "epcho --> 1415 ： loss 0.088\n",
      "epcho --> 1420 ： loss 0.088\n",
      "epcho --> 1425 ： loss 0.088\n",
      "epcho --> 1430 ： loss 0.088\n",
      "epcho --> 1435 ： loss 0.088\n",
      "epcho --> 1440 ： loss 0.088\n",
      "epcho --> 1445 ： loss 0.088\n",
      "epcho --> 1450 ： loss 0.088\n",
      "epcho --> 1455 ： loss 0.088\n",
      "epcho --> 1460 ： loss 0.088\n",
      "epcho --> 1465 ： loss 0.088\n",
      "epcho --> 1470 ： loss 0.088\n",
      "epcho --> 1475 ： loss 0.088\n",
      "epcho --> 1480 ： loss 0.088\n",
      "epcho --> 1485 ： loss 0.088\n",
      "epcho --> 1490 ： loss 0.088\n",
      "epcho --> 1495 ： loss 0.088\n",
      "epcho --> 1500 ： loss 0.089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcho --> 1505 ： loss 0.088\n",
      "epcho --> 1510 ： loss 0.088\n",
      "epcho --> 1515 ： loss 0.088\n",
      "epcho --> 1520 ： loss 0.088\n",
      "epcho --> 1525 ： loss 0.088\n",
      "epcho --> 1530 ： loss 0.088\n",
      "epcho --> 1535 ： loss 0.088\n",
      "epcho --> 1540 ： loss 0.088\n",
      "epcho --> 1545 ： loss 0.088\n",
      "epcho --> 1550 ： loss 0.088\n",
      "epcho --> 1555 ： loss 0.088\n",
      "epcho --> 1560 ： loss 0.088\n",
      "epcho --> 1565 ： loss 0.088\n",
      "epcho --> 1570 ： loss 0.088\n",
      "epcho --> 1575 ： loss 0.088\n",
      "epcho --> 1580 ： loss 0.088\n",
      "epcho --> 1585 ： loss 0.088\n",
      "epcho --> 1590 ： loss 0.088\n",
      "epcho --> 1595 ： loss 0.088\n",
      "epcho --> 1600 ： loss 0.088\n",
      "epcho --> 1605 ： loss 0.088\n",
      "epcho --> 1610 ： loss 0.088\n",
      "epcho --> 1615 ： loss 0.088\n",
      "epcho --> 1620 ： loss 0.088\n",
      "epcho --> 1625 ： loss 0.088\n",
      "epcho --> 1630 ： loss 0.088\n",
      "epcho --> 1635 ： loss 0.088\n",
      "epcho --> 1640 ： loss 0.088\n",
      "epcho --> 1645 ： loss 0.088\n",
      "epcho --> 1650 ： loss 0.088\n",
      "epcho --> 1655 ： loss 0.088\n",
      "epcho --> 1660 ： loss 0.088\n",
      "epcho --> 1665 ： loss 0.088\n",
      "epcho --> 1670 ： loss 0.088\n",
      "epcho --> 1675 ： loss 0.088\n",
      "epcho --> 1680 ： loss 0.088\n",
      "epcho --> 1685 ： loss 0.088\n",
      "epcho --> 1690 ： loss 0.088\n",
      "epcho --> 1695 ： loss 0.088\n",
      "epcho --> 1700 ： loss 0.088\n",
      "epcho --> 1705 ： loss 0.088\n",
      "epcho --> 1710 ： loss 0.088\n",
      "epcho --> 1715 ： loss 0.088\n",
      "epcho --> 1720 ： loss 0.088\n",
      "epcho --> 1725 ： loss 0.088\n",
      "epcho --> 1730 ： loss 0.088\n",
      "epcho --> 1735 ： loss 0.088\n",
      "epcho --> 1740 ： loss 0.088\n",
      "epcho --> 1745 ： loss 0.088\n",
      "epcho --> 1750 ： loss 0.088\n",
      "epcho --> 1755 ： loss 0.088\n",
      "epcho --> 1760 ： loss 0.088\n",
      "epcho --> 1765 ： loss 0.088\n",
      "epcho --> 1770 ： loss 0.088\n",
      "epcho --> 1775 ： loss 0.088\n",
      "epcho --> 1780 ： loss 0.088\n",
      "epcho --> 1785 ： loss 0.088\n",
      "epcho --> 1790 ： loss 0.088\n",
      "epcho --> 1795 ： loss 0.088\n",
      "epcho --> 1800 ： loss 0.088\n",
      "epcho --> 1805 ： loss 0.088\n",
      "epcho --> 1810 ： loss 0.088\n",
      "epcho --> 1815 ： loss 0.088\n",
      "epcho --> 1820 ： loss 0.088\n",
      "epcho --> 1825 ： loss 0.088\n",
      "epcho --> 1830 ： loss 0.088\n",
      "epcho --> 1835 ： loss 0.088\n",
      "epcho --> 1840 ： loss 0.088\n",
      "epcho --> 1845 ： loss 0.088\n",
      "epcho --> 1850 ： loss 0.088\n",
      "epcho --> 1855 ： loss 0.088\n",
      "epcho --> 1860 ： loss 0.088\n",
      "epcho --> 1865 ： loss 0.088\n",
      "epcho --> 1870 ： loss 0.088\n",
      "epcho --> 1875 ： loss 0.088\n",
      "epcho --> 1880 ： loss 0.088\n",
      "epcho --> 1885 ： loss 0.088\n",
      "epcho --> 1890 ： loss 0.088\n",
      "epcho --> 1895 ： loss 0.088\n",
      "epcho --> 1900 ： loss 0.088\n",
      "epcho --> 1905 ： loss 0.088\n",
      "epcho --> 1910 ： loss 0.088\n",
      "epcho --> 1915 ： loss 0.088\n",
      "epcho --> 1920 ： loss 0.088\n",
      "epcho --> 1925 ： loss 0.088\n",
      "epcho --> 1930 ： loss 0.088\n",
      "epcho --> 1935 ： loss 0.088\n",
      "epcho --> 1940 ： loss 0.088\n",
      "epcho --> 1945 ： loss 0.088\n",
      "epcho --> 1950 ： loss 0.088\n",
      "epcho --> 1955 ： loss 0.088\n",
      "epcho --> 1960 ： loss 0.088\n",
      "epcho --> 1965 ： loss 0.088\n",
      "epcho --> 1970 ： loss 0.088\n",
      "epcho --> 1975 ： loss 0.088\n",
      "epcho --> 1980 ： loss 0.088\n",
      "epcho --> 1985 ： loss 0.088\n",
      "epcho --> 1990 ： loss 0.088\n",
      "epcho --> 1995 ： loss 0.088\n",
      "epcho --> 2000 ： loss 0.088\n",
      "epcho --> 2005 ： loss 0.088\n",
      "epcho --> 2010 ： loss 0.088\n",
      "epcho --> 2015 ： loss 0.088\n",
      "epcho --> 2020 ： loss 0.088\n",
      "epcho --> 2025 ： loss 0.088\n",
      "epcho --> 2030 ： loss 0.088\n",
      "epcho --> 2035 ： loss 0.088\n",
      "epcho --> 2040 ： loss 0.088\n",
      "epcho --> 2045 ： loss 0.088\n",
      "epcho --> 2050 ： loss 0.088\n",
      "epcho --> 2055 ： loss 0.088\n",
      "epcho --> 2060 ： loss 0.088\n",
      "epcho --> 2065 ： loss 0.088\n",
      "epcho --> 2070 ： loss 0.088\n",
      "epcho --> 2075 ： loss 0.088\n",
      "epcho --> 2080 ： loss 0.088\n",
      "epcho --> 2085 ： loss 0.088\n",
      "epcho --> 2090 ： loss 0.088\n",
      "epcho --> 2095 ： loss 0.088\n",
      "epcho --> 2100 ： loss 0.088\n",
      "epcho --> 2105 ： loss 0.088\n",
      "epcho --> 2110 ： loss 0.088\n",
      "epcho --> 2115 ： loss 0.088\n",
      "epcho --> 2120 ： loss 0.088\n",
      "epcho --> 2125 ： loss 0.088\n",
      "epcho --> 2130 ： loss 0.088\n",
      "epcho --> 2135 ： loss 0.088\n",
      "epcho --> 2140 ： loss 0.088\n",
      "epcho --> 2145 ： loss 0.088\n",
      "epcho --> 2150 ： loss 0.088\n",
      "epcho --> 2155 ： loss 0.088\n",
      "epcho --> 2160 ： loss 0.088\n",
      "epcho --> 2165 ： loss 0.088\n",
      "epcho --> 2170 ： loss 0.088\n",
      "epcho --> 2175 ： loss 0.088\n",
      "epcho --> 2180 ： loss 0.088\n",
      "epcho --> 2185 ： loss 0.088\n",
      "epcho --> 2190 ： loss 0.088\n",
      "epcho --> 2195 ： loss 0.088\n",
      "epcho --> 2200 ： loss 0.088\n",
      "epcho --> 2205 ： loss 0.088\n",
      "epcho --> 2210 ： loss 0.088\n",
      "epcho --> 2215 ： loss 0.088\n",
      "epcho --> 2220 ： loss 0.088\n",
      "epcho --> 2225 ： loss 0.088\n",
      "epcho --> 2230 ： loss 0.088\n",
      "epcho --> 2235 ： loss 0.088\n",
      "epcho --> 2240 ： loss 0.088\n",
      "epcho --> 2245 ： loss 0.088\n",
      "epcho --> 2250 ： loss 0.088\n",
      "epcho --> 2255 ： loss 0.088\n",
      "epcho --> 2260 ： loss 0.088\n",
      "epcho --> 2265 ： loss 0.088\n",
      "epcho --> 2270 ： loss 0.088\n",
      "epcho --> 2275 ： loss 0.088\n",
      "epcho --> 2280 ： loss 0.088\n",
      "epcho --> 2285 ： loss 0.088\n",
      "epcho --> 2290 ： loss 0.088\n",
      "epcho --> 2295 ： loss 0.088\n",
      "epcho --> 2300 ： loss 0.088\n",
      "epcho --> 2305 ： loss 0.088\n",
      "epcho --> 2310 ： loss 0.088\n",
      "epcho --> 2315 ： loss 0.088\n",
      "epcho --> 2320 ： loss 0.088\n",
      "epcho --> 2325 ： loss 0.088\n",
      "epcho --> 2330 ： loss 0.088\n",
      "epcho --> 2335 ： loss 0.088\n",
      "epcho --> 2340 ： loss 0.088\n",
      "epcho --> 2345 ： loss 0.088\n",
      "epcho --> 2350 ： loss 0.088\n",
      "epcho --> 2355 ： loss 0.088\n",
      "epcho --> 2360 ： loss 0.088\n",
      "epcho --> 2365 ： loss 0.088\n",
      "epcho --> 2370 ： loss 0.088\n",
      "epcho --> 2375 ： loss 0.088\n",
      "epcho --> 2380 ： loss 0.088\n",
      "epcho --> 2385 ： loss 0.088\n",
      "epcho --> 2390 ： loss 0.088\n",
      "epcho --> 2395 ： loss 0.088\n",
      "epcho --> 2400 ： loss 0.088\n",
      "epcho --> 2405 ： loss 0.088\n",
      "epcho --> 2410 ： loss 0.088\n",
      "epcho --> 2415 ： loss 0.088\n",
      "epcho --> 2420 ： loss 0.088\n",
      "epcho --> 2425 ： loss 0.088\n",
      "epcho --> 2430 ： loss 0.088\n",
      "epcho --> 2435 ： loss 0.088\n",
      "epcho --> 2440 ： loss 0.088\n",
      "epcho --> 2445 ： loss 0.088\n",
      "epcho --> 2450 ： loss 0.088\n",
      "epcho --> 2455 ： loss 0.088\n",
      "epcho --> 2460 ： loss 0.088\n",
      "epcho --> 2465 ： loss 0.088\n",
      "epcho --> 2470 ： loss 0.088\n",
      "epcho --> 2475 ： loss 0.088\n",
      "epcho --> 2480 ： loss 0.088\n",
      "epcho --> 2485 ： loss 0.088\n",
      "epcho --> 2490 ： loss 0.088\n",
      "epcho --> 2495 ： loss 0.088\n",
      "epcho --> 2500 ： loss 0.088\n",
      "epcho --> 2505 ： loss 0.088\n",
      "epcho --> 2510 ： loss 0.088\n",
      "epcho --> 2515 ： loss 0.088\n",
      "epcho --> 2520 ： loss 0.088\n",
      "epcho --> 2525 ： loss 0.088\n",
      "epcho --> 2530 ： loss 0.088\n",
      "epcho --> 2535 ： loss 0.088\n",
      "epcho --> 2540 ： loss 0.088\n",
      "epcho --> 2545 ： loss 0.088\n",
      "epcho --> 2550 ： loss 0.088\n",
      "epcho --> 2555 ： loss 0.088\n",
      "epcho --> 2560 ： loss 0.088\n",
      "epcho --> 2565 ： loss 0.088\n",
      "epcho --> 2570 ： loss 0.088\n",
      "epcho --> 2575 ： loss 0.088\n",
      "epcho --> 2580 ： loss 0.088\n",
      "epcho --> 2585 ： loss 0.088\n",
      "epcho --> 2590 ： loss 0.088\n",
      "epcho --> 2595 ： loss 0.088\n",
      "epcho --> 2600 ： loss 0.088\n",
      "epcho --> 2605 ： loss 0.088\n",
      "epcho --> 2610 ： loss 0.088\n",
      "epcho --> 2615 ： loss 0.088\n",
      "epcho --> 2620 ： loss 0.088\n",
      "epcho --> 2625 ： loss 0.088\n",
      "epcho --> 2630 ： loss 0.088\n",
      "epcho --> 2635 ： loss 0.088\n",
      "epcho --> 2640 ： loss 0.088\n",
      "epcho --> 2645 ： loss 0.088\n",
      "epcho --> 2650 ： loss 0.088\n",
      "epcho --> 2655 ： loss 0.088\n",
      "epcho --> 2660 ： loss 0.088\n",
      "epcho --> 2665 ： loss 0.088\n",
      "epcho --> 2670 ： loss 0.088\n",
      "epcho --> 2675 ： loss 0.088\n",
      "epcho --> 2680 ： loss 0.088\n",
      "epcho --> 2685 ： loss 0.088\n",
      "epcho --> 2690 ： loss 0.088\n",
      "epcho --> 2695 ： loss 0.088\n",
      "epcho --> 2700 ： loss 0.088\n",
      "epcho --> 2705 ： loss 0.088\n",
      "epcho --> 2710 ： loss 0.088\n",
      "epcho --> 2715 ： loss 0.088\n",
      "epcho --> 2720 ： loss 0.088\n",
      "epcho --> 2725 ： loss 0.088\n",
      "epcho --> 2730 ： loss 0.088\n",
      "epcho --> 2735 ： loss 0.088\n",
      "epcho --> 2740 ： loss 0.088\n",
      "epcho --> 2745 ： loss 0.088\n",
      "epcho --> 2750 ： loss 0.088\n",
      "epcho --> 2755 ： loss 0.088\n",
      "epcho --> 2760 ： loss 0.088\n",
      "epcho --> 2765 ： loss 0.088\n",
      "epcho --> 2770 ： loss 0.088\n",
      "epcho --> 2775 ： loss 0.088\n",
      "epcho --> 2780 ： loss 0.088\n",
      "epcho --> 2785 ： loss 0.088\n",
      "epcho --> 2790 ： loss 0.088\n",
      "epcho --> 2795 ： loss 0.088\n",
      "epcho --> 2800 ： loss 0.088\n",
      "epcho --> 2805 ： loss 0.088\n",
      "epcho --> 2810 ： loss 0.088\n",
      "epcho --> 2815 ： loss 0.088\n",
      "epcho --> 2820 ： loss 0.088\n",
      "epcho --> 2825 ： loss 0.088\n",
      "epcho --> 2830 ： loss 0.088\n",
      "epcho --> 2835 ： loss 0.088\n",
      "epcho --> 2840 ： loss 0.088\n",
      "epcho --> 2845 ： loss 0.088\n",
      "epcho --> 2850 ： loss 0.088\n",
      "epcho --> 2855 ： loss 0.088\n",
      "epcho --> 2860 ： loss 0.088\n",
      "epcho --> 2865 ： loss 0.088\n",
      "epcho --> 2870 ： loss 0.088\n",
      "epcho --> 2875 ： loss 0.087\n",
      "epcho --> 2880 ： loss 0.088\n",
      "epcho --> 2885 ： loss 0.088\n",
      "epcho --> 2890 ： loss 0.088\n",
      "epcho --> 2895 ： loss 0.088\n",
      "epcho --> 2900 ： loss 0.088\n",
      "epcho --> 2905 ： loss 0.088\n",
      "epcho --> 2910 ： loss 0.088\n",
      "epcho --> 2915 ： loss 0.088\n",
      "epcho --> 2920 ： loss 0.088\n",
      "epcho --> 2925 ： loss 0.087\n",
      "epcho --> 2930 ： loss 0.088\n",
      "epcho --> 2935 ： loss 0.088\n",
      "epcho --> 2940 ： loss 0.087\n",
      "epcho --> 2945 ： loss 0.088\n",
      "epcho --> 2950 ： loss 0.087\n",
      "epcho --> 2955 ： loss 0.088\n",
      "epcho --> 2960 ： loss 0.088\n",
      "epcho --> 2965 ： loss 0.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcho --> 2970 ： loss 0.088\n",
      "epcho --> 2975 ： loss 0.088\n",
      "epcho --> 2980 ： loss 0.088\n",
      "epcho --> 2985 ： loss 0.088\n",
      "epcho --> 2990 ： loss 0.088\n",
      "epcho --> 2995 ： loss 0.088\n",
      "epcho --> 3000 ： loss 0.088\n",
      "epcho --> 3005 ： loss 0.088\n",
      "epcho --> 3010 ： loss 0.088\n",
      "epcho --> 3015 ： loss 0.088\n",
      "epcho --> 3020 ： loss 0.088\n",
      "epcho --> 3025 ： loss 0.088\n",
      "epcho --> 3030 ： loss 0.088\n",
      "epcho --> 3035 ： loss 0.088\n",
      "epcho --> 3040 ： loss 0.088\n",
      "epcho --> 3045 ： loss 0.088\n",
      "epcho --> 3050 ： loss 0.088\n",
      "epcho --> 3055 ： loss 0.088\n",
      "epcho --> 3060 ： loss 0.088\n",
      "epcho --> 3065 ： loss 0.088\n",
      "epcho --> 3070 ： loss 0.087\n",
      "epcho --> 3075 ： loss 0.088\n",
      "epcho --> 3080 ： loss 0.088\n",
      "epcho --> 3085 ： loss 0.088\n",
      "epcho --> 3090 ： loss 0.088\n",
      "epcho --> 3095 ： loss 0.088\n",
      "epcho --> 3100 ： loss 0.088\n",
      "epcho --> 3105 ： loss 0.088\n",
      "epcho --> 3110 ： loss 0.088\n",
      "epcho --> 3115 ： loss 0.088\n",
      "epcho --> 3120 ： loss 0.087\n",
      "epcho --> 3125 ： loss 0.087\n",
      "epcho --> 3130 ： loss 0.088\n",
      "epcho --> 3135 ： loss 0.088\n",
      "epcho --> 3140 ： loss 0.088\n",
      "epcho --> 3145 ： loss 0.088\n",
      "epcho --> 3150 ： loss 0.087\n",
      "epcho --> 3155 ： loss 0.087\n",
      "epcho --> 3160 ： loss 0.088\n",
      "epcho --> 3165 ： loss 0.088\n",
      "epcho --> 3170 ： loss 0.088\n",
      "epcho --> 3175 ： loss 0.088\n",
      "epcho --> 3180 ： loss 0.087\n",
      "epcho --> 3185 ： loss 0.088\n",
      "epcho --> 3190 ： loss 0.088\n",
      "epcho --> 3195 ： loss 0.088\n",
      "epcho --> 3200 ： loss 0.088\n",
      "epcho --> 3205 ： loss 0.088\n",
      "epcho --> 3210 ： loss 0.088\n",
      "epcho --> 3215 ： loss 0.088\n",
      "epcho --> 3220 ： loss 0.087\n",
      "epcho --> 3225 ： loss 0.087\n",
      "epcho --> 3230 ： loss 0.088\n",
      "epcho --> 3235 ： loss 0.088\n",
      "epcho --> 3240 ： loss 0.088\n",
      "epcho --> 3245 ： loss 0.087\n",
      "epcho --> 3250 ： loss 0.088\n",
      "epcho --> 3255 ： loss 0.087\n",
      "epcho --> 3260 ： loss 0.087\n",
      "epcho --> 3265 ： loss 0.088\n",
      "epcho --> 3270 ： loss 0.088\n",
      "epcho --> 3275 ： loss 0.087\n",
      "epcho --> 3280 ： loss 0.088\n",
      "epcho --> 3285 ： loss 0.088\n",
      "epcho --> 3290 ： loss 0.088\n",
      "epcho --> 3295 ： loss 0.087\n",
      "epcho --> 3300 ： loss 0.088\n",
      "epcho --> 3305 ： loss 0.087\n",
      "epcho --> 3310 ： loss 0.087\n",
      "epcho --> 3315 ： loss 0.088\n",
      "epcho --> 3320 ： loss 0.087\n",
      "epcho --> 3325 ： loss 0.088\n",
      "epcho --> 3330 ： loss 0.088\n",
      "epcho --> 3335 ： loss 0.087\n",
      "epcho --> 3340 ： loss 0.088\n",
      "epcho --> 3345 ： loss 0.087\n",
      "epcho --> 3350 ： loss 0.087\n",
      "epcho --> 3355 ： loss 0.087\n",
      "epcho --> 3360 ： loss 0.088\n",
      "epcho --> 3365 ： loss 0.087\n",
      "epcho --> 3370 ： loss 0.087\n",
      "epcho --> 3375 ： loss 0.088\n",
      "epcho --> 3380 ： loss 0.087\n",
      "epcho --> 3385 ： loss 0.087\n",
      "epcho --> 3390 ： loss 0.087\n",
      "epcho --> 3395 ： loss 0.088\n",
      "epcho --> 3400 ： loss 0.087\n",
      "epcho --> 3405 ： loss 0.088\n",
      "epcho --> 3410 ： loss 0.087\n",
      "epcho --> 3415 ： loss 0.087\n",
      "epcho --> 3420 ： loss 0.088\n",
      "epcho --> 3425 ： loss 0.087\n",
      "epcho --> 3430 ： loss 0.087\n",
      "epcho --> 3435 ： loss 0.088\n",
      "epcho --> 3440 ： loss 0.088\n",
      "epcho --> 3445 ： loss 0.087\n",
      "epcho --> 3450 ： loss 0.087\n",
      "epcho --> 3455 ： loss 0.087\n",
      "epcho --> 3460 ： loss 0.087\n",
      "epcho --> 3465 ： loss 0.087\n",
      "epcho --> 3470 ： loss 0.088\n",
      "epcho --> 3475 ： loss 0.087\n",
      "epcho --> 3480 ： loss 0.087\n",
      "epcho --> 3485 ： loss 0.088\n",
      "epcho --> 3490 ： loss 0.088\n",
      "epcho --> 3495 ： loss 0.087\n",
      "epcho --> 3500 ： loss 0.088\n",
      "epcho --> 3505 ： loss 0.087\n",
      "epcho --> 3510 ： loss 0.087\n",
      "epcho --> 3515 ： loss 0.087\n",
      "epcho --> 3520 ： loss 0.087\n",
      "epcho --> 3525 ： loss 0.087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8027d59207ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrun_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# clear gradients for this training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "LR = 0.0001\n",
    "autoencoder = AutoEncoder(n_feature = n_feature)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "epcho = 50000\n",
    "\n",
    "if os.path.exists('/home/hecong/dplearn/model/autoencode_model.pkl'):\n",
    "    print('加载上次训练模型...')\n",
    "    autoencoder.load_state_dict(torch.load('/home/hecong/dplearn/model/autoencode_model.pkl'))\n",
    "    \n",
    "for epcho in range(epcho):\n",
    "    run_loss = 0\n",
    "    for step, x in enumerate(train_loader):\n",
    "#         print(x[0])\n",
    "#         print(x[0].shape)\n",
    "        in_x = x[0]\n",
    "        target  = x[0]\n",
    "        encode,decode = autoencoder(in_x)\n",
    "        loss = loss_func(decode, target)\n",
    "        run_loss = run_loss + loss\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients        \n",
    "        \n",
    "    if epcho % 5 == 0:\n",
    "        print('epcho --> {} ： loss {:.3f}'.format(epcho, run_loss/step))\n",
    "        _wts = copy.deepcopy(autoencoder.state_dict())  \n",
    "        torch.save(_wts,'/home/hecong/dplearn/model/autoencode_model.pkl')            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2963,  0.2585,  0.2607,  ..., -0.5424,  0.3590,  0.3663],\n",
      "        [ 0.2741,  0.2462,  0.2562,  ..., -0.5206,  0.3639,  0.3743],\n",
      "        [ 0.1847,  0.1893,  0.2196,  ..., -0.2821,  0.3042,  0.3137],\n",
      "        ...,\n",
      "        [-0.0709, -0.0973, -0.1053,  ..., -0.2487,  0.1034,  0.1332],\n",
      "        [-0.0457, -0.0441, -0.0236,  ..., -0.1772,  0.1133,  0.1354],\n",
      "        [-0.0962, -0.0890, -0.0652,  ..., -0.1568,  0.0548,  0.0758]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[ 0.1551,  0.1076,  0.1283,  ..., -1.1425,  0.2122,  0.2478],\n",
      "        [ 0.1354,  0.0999,  0.1165,  ..., -1.1969,  0.2066,  0.2351],\n",
      "        [ 0.0841,  0.1038,  0.1283,  ..., -1.1982,  0.2022,  0.2220],\n",
      "        ...,\n",
      "        [-0.1642, -0.1878, -0.2069,  ..., -0.5384, -0.0984, -0.1132],\n",
      "        [-0.1878, -0.2109, -0.1871,  ..., -0.4280, -0.1039, -0.1156],\n",
      "        [-0.1760, -0.1993, -0.1674,  ..., -0.3667, -0.1078, -0.1183]])\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoder(n_feature = n_feature)\n",
    "autoencoder.load_state_dict(torch.load('/home/hecong/dplearn/model/autoencode_model.pkl'))\n",
    "encode, decode = autoencoder(test_data)\n",
    "print(decode)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "autoencoder = AutoEncoder(n_feature = n_feature)\n",
    "autoencoder.load_state_dict(torch.load('/home/hecong/dplearn/model/autoencode_model.pkl'))\n",
    "data_encode, _ = autoencoder(train_data)\n",
    "data_encode = data_encode.detach().numpy()\n",
    "test_encode, _ = autoencoder(test_data)\n",
    "test_encode = test_encode.detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.479646017699115\n",
      "0.479646017699115\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier() \n",
    "fit = knn.fit(data_encode,train_target)\n",
    "print(knn.score(test_encode,test_target))\n",
    "result = knn.predict(test_encode)\n",
    "print(len(result[result==test_target])/len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx, train_idx = np.where(test_target==1), np.where(train_target == 1)\n",
    "test_target[test_idx]=0\n",
    "train_target[train_idx]=0\n",
    "\n",
    "test_idx, train_idx = np.where(test_target==2),  np.where(train_target==2)\n",
    "test_target[test_idx] = 1\n",
    "train_target[train_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 0.01\n",
      "cache_size 200\n",
      "class_weight None\n",
      "coef0 0.0\n",
      "decision_function_shape ovr\n",
      "degree 3\n",
      "gamma auto\n",
      "kernel rbf\n",
      "max_iter -1\n",
      "probability False\n",
      "random_state None\n",
      "shrinking True\n",
      "tol 0.001\n",
      "verbose False\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.6442477876106195\n",
      "0.35575221238938054\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV   # sklearn 调参 采用  grid search 方式 \n",
    "clf = SVC(gamma='auto')\n",
    "param_grid = {'C': [0.01,0.1,0.5, 1, 10], 'gamma': ['auto'],'kernel':['rbf','poly','sigmoid','linear']}\n",
    "grid_search = GridSearchCV(clf, param_grid, n_jobs = 1, verbose=1)  \n",
    "grid_search.fit(data_encode, train_target)  \n",
    "best_parameters = grid_search.best_estimator_.get_params()  \n",
    "for para, val in best_parameters.items():  \n",
    "    print(para, val)  \n",
    "    \n",
    "model = SVC(C=best_parameters['C'], gamma=best_parameters['gamma'], probability=False,kernel=best_parameters['kernel'])      \n",
    "model.fit(data_encode,train_target)\n",
    "clf.fit(data_encode,train_target)  \n",
    "print(clf)\n",
    "print(model)\n",
    "print(clf.score(test_encode,test_target))\n",
    "print(model.score(test_encode,test_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

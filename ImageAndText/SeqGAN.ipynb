{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本说明 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://cloud.tencent.com/developer/article/1162834\n",
    "    https://www.zhihu.com/question/52602529/answer/155743699\n",
    "    https://github.com/ZiJianZhao/SeqGAN-PyTorch\n",
    "    https://github.com/ChenChengKuan/SeqGAN_tensorflow\n",
    "    https://github.com/suragnair/seqGAN\n",
    "    \n",
    "    https://arxiv.org/pdf/1609.05473.pdf\n",
    "    \n",
    "    https://baike.baidu.com/item/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/3350286?fr=aladdin  MLE 极大似然估计\n",
    "    https://blog.csdn.net/lanchunhui/article/details/51248184 softmax 及 logsoftmax\n",
    "    \n",
    "    https://blog.csdn.net/bobobe/article/details/81297064  scheduled sampling （计划采样）\n",
    "    http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf  计划采样\n",
    "    https://blog.csdn.net/dukuku5038/article/details/84060969  Scheduled Sampling\n",
    "    https://stackoverflow.com/questions/43795423/scheduled-sampling-in-tensorflow \n",
    "    https://github.com/TobiasLee/SeqGAN_Poem 诗歌\n",
    "    \n",
    "    SeqGAN significantly outperforms the maximum likelihood methods, scheduled sampling and PG-BLEU\n",
    "    https://www.jianshu.com/p/15c22fadcba5 机器翻译质量评测算法-BLEU\n",
    "    https://blog.csdn.net/dlphay/article/details/78200396 Policy Gradient简述\n",
    "    https://blog.csdn.net/suai9292/article/details/79910525 Policy Gradient理解\n",
    "    \n",
    "    https://www.leiphone.com/news/201810/cTCGyCN8w6pfRm0C.html 通过多对抗训练，从图像生成诗歌 \n",
    "    https://www.jianshu.com/p/e1b87286bfae  SeqGAN解读\n",
    "    \n",
    "    \n",
    "    \n",
    "    http://www.eeworld.com.cn/mp/QbitAI/a53664.jspx 韩国小哥哥用Pytorch实现谷歌最强NLP预训练模型BERT | 代码\n",
    "    \n",
    "    https://blog.csdn.net/zhl493722771/article/details/82781914 令人拍案叫绝的WGAN\n",
    "    \n",
    "    https://www.colabug.com/2639033.html 对抗思想与强化学习的碰撞-SeqGAN模型原理和代码解析\n",
    "    \n",
    "    https://blog.csdn.net/Irving_zhang/article/details/79088143  实现基于seq2seq的聊天机器人\n",
    "    \n",
    "    https://www.leiphone.com/news/201709/QRJPQr3jCOtY7ncQ.html 如何让对抗网络GAN生成更高质量的文本？\n",
    "    \n",
    "    https://blog.csdn.net/Young_Gy/article/details/76474939  构建聊天机器人：检索、seq2seq、RL、SeqGAN\n",
    "    \n",
    "    https://blog.csdn.net/yinruiyang94/article/details/77675586 SeqGAN——对抗思想与增强学习的碰撞\n",
    "    \n",
    "    https://blog.csdn.net/qunnie_yi/article/details/80129851 只知道GAN你就OUT了——VAE背后的哲学思想及数学原理\n",
    "    \n",
    "    https://blog.csdn.net/GitChat/article/details/79081190 手把手教你写一个中文聊天机器人\n",
    "    \n",
    "    https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/79184714 一文读懂智能对话系统\n",
    "    \n",
    "    https://blog.csdn.net/taoyafan/article/details/81229466#1%20%E4%BB%80%E4%B9%88%E6%98%AF%20Condition%20GAN  Condition GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " https://www.cnblogs.com/steven-yang/p/6624253.html 强化学习读书笔记 - 13 - 策略梯度方法(Policy Gradient Methods)\n",
    " https://blog.csdn.net/aliceyangxi1987/article/details/73327378 一文了解强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     32,
     52,
     82,
     100
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data ...\n",
      "Pretrain with MLE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project_tw\\anly\\venv\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Model Loss: 4179.830802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project_tw\\anly\\venv\\lib\\site-packages\\ipykernel_launcher.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] True Loss: 39762.068270\n",
      "Epoch [1] Model Loss: 2214.741282\n",
      "Epoch [1] True Loss: 30504.288479\n",
      "Epoch [2] Model Loss: 2011.072374\n",
      "Epoch [2] True Loss: 29778.084337\n",
      "Epoch [3] Model Loss: 1971.645335\n",
      "Epoch [3] True Loss: 28768.750388\n",
      "Epoch [4] Model Loss: 1937.013228\n",
      "Epoch [4] True Loss: 28778.107592\n",
      "Epoch [5] Model Loss: 1892.705449\n",
      "Epoch [5] True Loss: 29491.482321\n",
      "Epoch [6] Model Loss: 1845.944281\n",
      "Epoch [6] True Loss: 29547.617465\n",
      "Epoch [7] Model Loss: 1802.783254\n",
      "Epoch [7] True Loss: 29530.996647\n",
      "Epoch [8] Model Loss: 1765.819795\n",
      "Epoch [8] True Loss: 29424.984891\n",
      "Epoch [9] Model Loss: 1736.859886\n",
      "Epoch [9] True Loss: 30260.408347\n",
      "Pretrain Dsicriminator ...\n",
      "Epoch [0], loss: 1.038631\n",
      "Epoch [0], loss: 1.050110\n",
      "Epoch [0], loss: 1.031522\n",
      "Epoch [1], loss: 1.016951\n",
      "Epoch [1], loss: 1.036008\n",
      "Epoch [1], loss: 1.002655\n",
      "Epoch [2], loss: 1.001831\n",
      "Epoch [2], loss: 1.021458\n",
      "Epoch [2], loss: 1.000394\n",
      "Epoch [3], loss: 1.001053\n",
      "Epoch [3], loss: 1.005552\n",
      "Epoch [3], loss: 1.000132\n",
      "Epoch [4], loss: 1.000787\n",
      "Epoch [4], loss: 1.002103\n",
      "Epoch [4], loss: 1.000200\n",
      "#####################################################\n",
      "Start Adeversatial Training...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (320) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f747d1fb0132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_gan_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[0mgen_gan_optm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f747d1fb0132>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, prob, target, reward)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mone_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (320) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from lib.seqgan.generator import Generator\n",
    "from lib.seqgan.discriminator import Discriminator\n",
    "from lib.seqgan.target_lstm import TargetLSTM\n",
    "from lib.seqgan.rollout import Rollout\n",
    "from lib.seqgan.data_iter import GenDataIter, DisDataIter\n",
    "opt = edict()\n",
    "opt.cuda = None\n",
    "# Basic Training Paramters\n",
    "SEED = 88\n",
    "BATCH_SIZE = 16\n",
    "TOTAL_BATCH = 200\n",
    "GENERATED_NUM = 1000\n",
    "POSITIVE_FILE = 'real.data'\n",
    "NEGATIVE_FILE = 'gene.data'\n",
    "EVAL_FILE = 'eval.data'\n",
    "VOCAB_SIZE = 5000\n",
    "PRE_EPOCH_NUM = 10 #120\n",
    "\n",
    "if opt.cuda is not None and opt.cuda >= 0:\n",
    "    torch.cuda.set_device(opt.cuda)\n",
    "    opt.cuda = True\n",
    "\n",
    "# Genrator Parameters\n",
    "g_emb_dim = 32\n",
    "g_hidden_dim = 32\n",
    "g_sequence_len = 20\n",
    "\n",
    "# Discriminator Parameters\n",
    "d_emb_dim = 64\n",
    "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "\n",
    "d_dropout = 0.75\n",
    "d_num_class = 2\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def generate_samples(model, batch_size, generated_num, output_file):\n",
    "    samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        sample = model.sample(batch_size, g_sequence_len).cpu().data.numpy().tolist()\n",
    "        samples.extend(sample)\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in samples:\n",
    "            string = ' '.join([str(s) for s in sample])\n",
    "            fout.write('%s\\n' % string)\n",
    "            \n",
    "def train_epoch(model, data_iter, criterion, optimizer):\n",
    "    total_loss = 0.\n",
    "    total_words = 0.\n",
    "    for (data, target) in data_iter:\n",
    "        #tqdm(data_iter, mininterval=2, desc=' - Training', leave=False):\n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        if opt.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        target = target.contiguous().view(-1)\n",
    "        pred = model.forward(data)\n",
    "        loss = criterion(pred, target)\n",
    "        total_loss += loss.data[0]\n",
    "        total_words += data.size(0) * data.size(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    data_iter.reset()\n",
    "    return math.exp(total_loss / total_words)\n",
    "\n",
    "def eval_epoch(model, data_iter, criterion):\n",
    "    total_loss = 0.\n",
    "    total_words = 0.\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in data_iter:#tqdm(\n",
    "            #data_iter, mininterval=2, desc=' - Training', leave=False):\n",
    "            data = Variable(data)\n",
    "            target = Variable(target)\n",
    "            if opt.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            target = target.contiguous().view(-1)\n",
    "            pred = model.forward(data)\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.data[0]\n",
    "            total_words += data.size(0) * data.size(1)\n",
    "        data_iter.reset()\n",
    "    return math.exp(total_loss / total_words)\n",
    "            \n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Reward-Refined NLLLoss Function for adversial training of Gnerator\"\"\"\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prob, target, reward):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prob: (N, C), torch Variable \n",
    "            target : (N, ), torch Variable\n",
    "            reward : (N, ), torch Variable\n",
    "        \"\"\"\n",
    "        N = target.size(0)\n",
    "        C = prob.size(1)\n",
    "        one_hot = torch.zeros((N, C))\n",
    "        if prob.is_cuda:\n",
    "            one_hot = one_hot.cuda()\n",
    "        one_hot.scatter_(1, target.data.view((-1,1)), 1)\n",
    "        one_hot = one_hot.type(torch.ByteTensor)\n",
    "        one_hot = Variable(one_hot)\n",
    "        if prob.is_cuda:\n",
    "            one_hot = one_hot.cuda()\n",
    "        loss = torch.masked_select(prob, one_hot)\n",
    "        loss = loss * reward\n",
    "        loss =  -torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "# Define Networks\n",
    "# VOCAB_SIZE = 5000, g_emb_dim = 32, g_hidden_dim = 32\n",
    "generator = Generator(VOCAB_SIZE, g_emb_dim, g_hidden_dim, opt.cuda)\n",
    "# d_emb_dim = 64\n",
    "# d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "# d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "discriminator = Discriminator(d_num_class, VOCAB_SIZE, d_emb_dim, d_filter_sizes, d_num_filters, d_dropout)\n",
    "target_lstm = TargetLSTM(VOCAB_SIZE, g_emb_dim, g_hidden_dim, opt.cuda)\n",
    "\n",
    "print('Generating data ...')\n",
    "generate_samples(target_lstm, BATCH_SIZE, GENERATED_NUM, POSITIVE_FILE)\n",
    "\n",
    "# Load data from file\n",
    "gen_data_iter = GenDataIter(POSITIVE_FILE, BATCH_SIZE)\n",
    "\n",
    "# Pretrain Generator using MLE\n",
    "gen_criterion = nn.NLLLoss(size_average=False)\n",
    "gen_optimizer = optim.Adam(generator.parameters())\n",
    "\n",
    "print('Pretrain with MLE ...')  # ？？？？\n",
    "for epoch in range(PRE_EPOCH_NUM):\n",
    "    loss = train_epoch(generator, gen_data_iter, gen_criterion, gen_optimizer)\n",
    "    print('Epoch [%d] Model Loss: %f'% (epoch, loss))\n",
    "    generate_samples(generator, BATCH_SIZE, GENERATED_NUM, EVAL_FILE)\n",
    "    eval_iter = GenDataIter(EVAL_FILE, BATCH_SIZE)\n",
    "    loss = eval_epoch(target_lstm, eval_iter, gen_criterion)\n",
    "    print('Epoch [%d] True Loss: %f' % (epoch, loss))\n",
    "\n",
    "# Pretrain Discriminator\n",
    "dis_criterion = nn.NLLLoss(size_average=False)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "if opt.cuda:\n",
    "    dis_criterion = dis_criterion.cuda()\n",
    "    \n",
    "print('Pretrain Dsicriminator ...')\n",
    "for epoch in range(5):\n",
    "    generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "    dis_data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
    "    for _ in range(3):\n",
    "        loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)\n",
    "        print('Epoch [%d], loss: %f' % (epoch, loss))\n",
    "\n",
    "# Adversarial Training 对抗训练\n",
    "rollout = Rollout(generator, 0.8)    \n",
    "print('#####################################################')\n",
    "print('Start Adeversatial Training...\\n')\n",
    "gen_gan_loss = GANLoss()\n",
    "gen_gan_optm = optim.Adam(generator.parameters())\n",
    "if opt.cuda:\n",
    "    gen_gan_loss = gen_gan_loss.cuda()\n",
    "gen_criterion = nn.NLLLoss(size_average=False)\n",
    "if opt.cuda:\n",
    "    gen_criterion = gen_criterion.cuda()\n",
    "dis_criterion = nn.NLLLoss(size_average=False)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "if opt.cuda:\n",
    "    dis_criterion = dis_criterion.cuda()\n",
    "\n",
    "\n",
    "if opt.cuda:\n",
    "    dis_criterion = dis_criterion.cuda()\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    ## Train the generator for one step\n",
    "    for it in range(1):\n",
    "        samples = generator.sample(BATCH_SIZE, g_sequence_len)\n",
    "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
    "        zeros = torch.zeros((BATCH_SIZE, 1)).type(torch.LongTensor)\n",
    "        if samples.is_cuda:\n",
    "            zeros = zeros.cuda()\n",
    "        inputs = Variable(torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous())\n",
    "        targets = Variable(samples.data).contiguous().view((-1,))\n",
    "        # calculate the reward, 16是作蒙特卡罗搜索次数 ， 确认一下\n",
    "        rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "        rewards = Variable(torch.Tensor(rewards))\n",
    "        if opt.cuda:\n",
    "            rewards = torch.exp(rewards.cuda()).contiguous().view((-1,))\n",
    "        prob = generator.forward(inputs)\n",
    "        loss = gen_gan_loss(prob, targets, rewards)\n",
    "        gen_gan_optm.zero_grad()\n",
    "        loss.backward()\n",
    "        gen_gan_optm.step()\n",
    "        \n",
    "    if total_batch % 1 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "        generate_samples(generator, BATCH_SIZE, GENERATED_NUM, EVAL_FILE)\n",
    "        eval_iter = GenDataIter(EVAL_FILE, BATCH_SIZE)\n",
    "        loss = eval_epoch(target_lstm, eval_iter, gen_criterion)\n",
    "        print('Batch [%d] True Loss: %f' % (total_batch, loss))\n",
    "    rollout.update_params()\n",
    "\n",
    "    for _ in range(4):\n",
    "        generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "        dis_data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
    "        for _ in range(2):\n",
    "            loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "gen_data_iter = GenDataIter(POSITIVE_FILE, BATCH_SIZE)\n",
    "data,target = next(gen_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 21])\n",
      "torch.Size([16, 21])\n",
      "tensor([    0,  3978,  4961,   847,  2077,  4080,  4628,  2597,    80,\n",
      "         3845,   393,   233,   754,  3659,  2915,  1876,  2673,  3482,\n",
      "         2574,  2748,  1049])\n",
      "----\n",
      "tensor([ 3978,  4961,   847,  2077,  4080,  4628,  2597,    80,  3845,\n",
      "          393,   233,   754,  3659,  2915,  1876,  2673,  3482,  2574,\n",
      "         2748,  1049,     0])\n"
     ]
    }
   ],
   "source": [
    "print(data.size())\n",
    "print(target.size())\n",
    "print(data[1])\n",
    "print('----')\n",
    "print(target[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

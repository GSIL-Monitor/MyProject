{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "\n",
    "# https://github.com/BelBES/crnn-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\n",
      "Written 1000 / 1438\n",
      "Created dataset with 1438 samples\n"
     ]
    }
   ],
   "source": [
    "import lib.data.gen_data as gd\n",
    "import lib.data.dataset as ds\n",
    "import lib.data.char as char\n",
    "import importlib\n",
    "importlib.reload(gd)\n",
    "importlib.reload(char)\n",
    "\n",
    "# 生成文字图片\n",
    "gd.create_data(1000)\n",
    "\n",
    "# 根据生成文字图片，保存到lmdb\n",
    "ds.main_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import lib.data.lmdb_dataset as lds\n",
    "import torch\n",
    "train_path = '/home/hecong/temp/data/ocr/lmdb'\n",
    "batchSize = 10\n",
    "sampler = None\n",
    "workers = 1\n",
    "imgH = 32\n",
    "imgW = 256\n",
    "\n",
    "train_dataset = lds.lmdbDataset(root=train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "    sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=lds.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('洁洪洒浇浊洞测洗活派', '狡狱狠贸怨急饶蚀饺饼', '羽观欢买红纤级约纪驰', '夕丸么广亡门义之尸弓', '勉狭狮独狡狱狠贸怨急')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "_,(image,label) = next(enumerate(train_loader))\n",
    "image_0 = image[0][0]\n",
    "image_0 = image_0.numpy()\n",
    "print(label)\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def data_parallel(model, input, ngpu):\n",
    "    if ngpu > 1 and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "        output = nn.parallel.data_parallel(model, input, range(ngpu))\n",
    "    else:\n",
    "        output = model(input)\n",
    "    return output\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut, ngpu):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = data_parallel(self.rnn, input,\n",
    "                                           self.ngpu)  # [T, b, h * 2]\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = data_parallel(self.embedding, \n",
    "                               t_rec,self.ngpu)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, ngpu, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "                                                                    \n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        # pool = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
    "        # 注意MaxPool2d 当stride = (2,1),表示只会根据H轴方向进行Pool，因为W方向是1。\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh, ngpu),\n",
    "            BidirectionalLSTM(nh, nh, nclass, ngpu))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "#         print('input size --> {}'.format(input.size()))\n",
    "        conv = data_parallel(self.cnn, input, self.ngpu)\n",
    "        b, c, h, w = conv.size()\n",
    "#         print('conv out size --> {}'.format(conv.size()))\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        \n",
    "        # rnn features\n",
    "        output = data_parallel(self.rnn, conv, self.ngpu)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 loss --> tensor([2.0093], grad_fn=<DivBackward0>)\n",
      "0:10 loss --> tensor([50.4357], grad_fn=<DivBackward0>)\n",
      "0:20 loss --> tensor([32.7192], grad_fn=<DivBackward0>)\n",
      "0:30 loss --> tensor([29.6999], grad_fn=<DivBackward0>)\n",
      "0:40 loss --> tensor([30.8185], grad_fn=<DivBackward0>)\n",
      "0:50 loss --> tensor([17.9923], grad_fn=<DivBackward0>)\n",
      "0:60 loss --> tensor([21.9073], grad_fn=<DivBackward0>)\n",
      "0:70 loss --> tensor([19.4085], grad_fn=<DivBackward0>)\n",
      "0:80 loss --> tensor([14.9581], grad_fn=<DivBackward0>)\n",
      "0:90 loss --> tensor([10.8917], grad_fn=<DivBackward0>)\n",
      "0:100 loss --> tensor([17.9379], grad_fn=<DivBackward0>)\n",
      "0:110 loss --> tensor([29.3240], grad_fn=<DivBackward0>)\n",
      "0:120 loss --> tensor([28.2609], grad_fn=<DivBackward0>)\n",
      "0:130 loss --> tensor([18.1639], grad_fn=<DivBackward0>)\n",
      "0:140 loss --> tensor([18.5935], grad_fn=<DivBackward0>)\n",
      "1:0 loss --> tensor([10.9817], grad_fn=<DivBackward0>)\n",
      "1:10 loss --> tensor([28.2779], grad_fn=<DivBackward0>)\n",
      "1:20 loss --> tensor([12.5013], grad_fn=<DivBackward0>)\n",
      "1:30 loss --> tensor([22.7310], grad_fn=<DivBackward0>)\n",
      "1:40 loss --> tensor([11.0990], grad_fn=<DivBackward0>)\n",
      "1:50 loss --> tensor([9.0520], grad_fn=<DivBackward0>)\n",
      "1:60 loss --> tensor([15.1589], grad_fn=<DivBackward0>)\n",
      "1:70 loss --> tensor([16.1022], grad_fn=<DivBackward0>)\n",
      "1:80 loss --> tensor([21.4937], grad_fn=<DivBackward0>)\n",
      "1:90 loss --> tensor([24.7296], grad_fn=<DivBackward0>)\n",
      "1:100 loss --> tensor([9.4360], grad_fn=<DivBackward0>)\n",
      "1:110 loss --> tensor([10.3729], grad_fn=<DivBackward0>)\n",
      "1:120 loss --> tensor([9.8497], grad_fn=<DivBackward0>)\n",
      "1:130 loss --> tensor([8.0697], grad_fn=<DivBackward0>)\n",
      "1:140 loss --> tensor([9.7845], grad_fn=<DivBackward0>)\n",
      "2:0 loss --> tensor([7.6384], grad_fn=<DivBackward0>)\n",
      "2:10 loss --> tensor([8.4288], grad_fn=<DivBackward0>)\n",
      "2:20 loss --> tensor([7.4787], grad_fn=<DivBackward0>)\n",
      "2:30 loss --> tensor([5.1242], grad_fn=<DivBackward0>)\n",
      "2:40 loss --> tensor([5.4999], grad_fn=<DivBackward0>)\n",
      "2:50 loss --> tensor([5.6282], grad_fn=<DivBackward0>)\n",
      "2:60 loss --> tensor([4.4453], grad_fn=<DivBackward0>)\n",
      "2:70 loss --> tensor([9.8130], grad_fn=<DivBackward0>)\n",
      "2:80 loss --> tensor([5.6961], grad_fn=<DivBackward0>)\n",
      "2:90 loss --> tensor([10.1451], grad_fn=<DivBackward0>)\n",
      "2:100 loss --> tensor([4.4921], grad_fn=<DivBackward0>)\n",
      "2:110 loss --> tensor([3.9594], grad_fn=<DivBackward0>)\n",
      "2:120 loss --> tensor([19.1258], grad_fn=<DivBackward0>)\n",
      "2:130 loss --> tensor([3.0576], grad_fn=<DivBackward0>)\n",
      "2:140 loss --> tensor([6.7883], grad_fn=<DivBackward0>)\n",
      "3:0 loss --> tensor([6.2436], grad_fn=<DivBackward0>)\n",
      "3:10 loss --> tensor([5.2135], grad_fn=<DivBackward0>)\n",
      "3:20 loss --> tensor([5.4170], grad_fn=<DivBackward0>)\n",
      "3:30 loss --> tensor([11.4397], grad_fn=<DivBackward0>)\n",
      "3:40 loss --> tensor([5.0069], grad_fn=<DivBackward0>)\n",
      "3:50 loss --> tensor([9.2299], grad_fn=<DivBackward0>)\n",
      "3:60 loss --> tensor([2.7963], grad_fn=<DivBackward0>)\n",
      "3:70 loss --> tensor([2.7076], grad_fn=<DivBackward0>)\n",
      "3:80 loss --> tensor([12.6714], grad_fn=<DivBackward0>)\n",
      "3:90 loss --> tensor([1.5503], grad_fn=<DivBackward0>)\n",
      "3:100 loss --> tensor([12.2525], grad_fn=<DivBackward0>)\n",
      "3:110 loss --> tensor([4.6674], grad_fn=<DivBackward0>)\n",
      "3:120 loss --> tensor([2.8498], grad_fn=<DivBackward0>)\n",
      "3:130 loss --> tensor([2.8989], grad_fn=<DivBackward0>)\n",
      "3:140 loss --> tensor([6.5617], grad_fn=<DivBackward0>)\n",
      "4:0 loss --> tensor([8.6493], grad_fn=<DivBackward0>)\n",
      "4:10 loss --> tensor([2.2715], grad_fn=<DivBackward0>)\n",
      "4:20 loss --> tensor([4.1219], grad_fn=<DivBackward0>)\n",
      "4:30 loss --> tensor([5.0218], grad_fn=<DivBackward0>)\n",
      "4:40 loss --> tensor([3.0168], grad_fn=<DivBackward0>)\n",
      "4:50 loss --> tensor([1.8082], grad_fn=<DivBackward0>)\n",
      "4:60 loss --> tensor([2.5561], grad_fn=<DivBackward0>)\n",
      "4:70 loss --> tensor([2.4951], grad_fn=<DivBackward0>)\n",
      "4:80 loss --> tensor([8.2852], grad_fn=<DivBackward0>)\n",
      "4:90 loss --> tensor([4.6325], grad_fn=<DivBackward0>)\n",
      "4:100 loss --> tensor([3.0762], grad_fn=<DivBackward0>)\n",
      "4:110 loss --> tensor([5.6552], grad_fn=<DivBackward0>)\n",
      "4:120 loss --> tensor([1.9165], grad_fn=<DivBackward0>)\n",
      "4:130 loss --> tensor([5.8676], grad_fn=<DivBackward0>)\n",
      "4:140 loss --> tensor([3.0931], grad_fn=<DivBackward0>)\n",
      "5:0 loss --> tensor([1.3279], grad_fn=<DivBackward0>)\n",
      "5:10 loss --> tensor([4.6664], grad_fn=<DivBackward0>)\n",
      "5:20 loss --> tensor([3.6021], grad_fn=<DivBackward0>)\n",
      "5:30 loss --> tensor([10.9734], grad_fn=<DivBackward0>)\n",
      "5:40 loss --> tensor([3.4750], grad_fn=<DivBackward0>)\n",
      "5:50 loss --> tensor([2.8559], grad_fn=<DivBackward0>)\n",
      "5:60 loss --> tensor([3.5577], grad_fn=<DivBackward0>)\n",
      "5:70 loss --> tensor([4.4905], grad_fn=<DivBackward0>)\n",
      "5:80 loss --> tensor([2.9750], grad_fn=<DivBackward0>)\n",
      "5:90 loss --> tensor([2.9978], grad_fn=<DivBackward0>)\n",
      "5:100 loss --> tensor([5.9025], grad_fn=<DivBackward0>)\n",
      "5:110 loss --> tensor([2.3720], grad_fn=<DivBackward0>)\n",
      "5:120 loss --> tensor([2.4343], grad_fn=<DivBackward0>)\n",
      "5:130 loss --> tensor([28.6452], grad_fn=<DivBackward0>)\n",
      "5:140 loss --> tensor([6.8061], grad_fn=<DivBackward0>)\n",
      "6:0 loss --> tensor([14.3352], grad_fn=<DivBackward0>)\n",
      "6:10 loss --> tensor([9.9875], grad_fn=<DivBackward0>)\n",
      "6:20 loss --> tensor([3.7326], grad_fn=<DivBackward0>)\n",
      "6:30 loss --> tensor([6.0018], grad_fn=<DivBackward0>)\n",
      "6:40 loss --> tensor([5.0011], grad_fn=<DivBackward0>)\n",
      "6:50 loss --> tensor([4.4327], grad_fn=<DivBackward0>)\n",
      "6:60 loss --> tensor([5.4890], grad_fn=<DivBackward0>)\n",
      "6:70 loss --> tensor([4.0963], grad_fn=<DivBackward0>)\n",
      "6:80 loss --> tensor([3.7195], grad_fn=<DivBackward0>)\n",
      "6:90 loss --> tensor([5.6513], grad_fn=<DivBackward0>)\n",
      "6:100 loss --> tensor([1.8962], grad_fn=<DivBackward0>)\n",
      "6:110 loss --> tensor([4.0270], grad_fn=<DivBackward0>)\n",
      "6:120 loss --> tensor([1.8950], grad_fn=<DivBackward0>)\n",
      "6:130 loss --> tensor([3.9196], grad_fn=<DivBackward0>)\n",
      "6:140 loss --> tensor([3.2349], grad_fn=<DivBackward0>)\n",
      "7:0 loss --> tensor([1.7780], grad_fn=<DivBackward0>)\n",
      "7:10 loss --> tensor([1.7843], grad_fn=<DivBackward0>)\n",
      "7:20 loss --> tensor([3.7219], grad_fn=<DivBackward0>)\n",
      "7:30 loss --> tensor([3.1673], grad_fn=<DivBackward0>)\n",
      "7:40 loss --> tensor([2.0574], grad_fn=<DivBackward0>)\n",
      "7:50 loss --> tensor([1.9338], grad_fn=<DivBackward0>)\n",
      "7:60 loss --> tensor([1.6865], grad_fn=<DivBackward0>)\n",
      "7:70 loss --> tensor([48.5906], grad_fn=<DivBackward0>)\n",
      "7:80 loss --> tensor([4.2701], grad_fn=<DivBackward0>)\n",
      "7:90 loss --> tensor([5.7861], grad_fn=<DivBackward0>)\n",
      "7:100 loss --> tensor([4.8526], grad_fn=<DivBackward0>)\n",
      "7:110 loss --> tensor([3.1499], grad_fn=<DivBackward0>)\n",
      "7:120 loss --> tensor([1.8716], grad_fn=<DivBackward0>)\n",
      "7:130 loss --> tensor([6.7886], grad_fn=<DivBackward0>)\n",
      "7:140 loss --> tensor([3.9944], grad_fn=<DivBackward0>)\n",
      "8:0 loss --> tensor([2.5778], grad_fn=<DivBackward0>)\n",
      "8:10 loss --> tensor([1.6472], grad_fn=<DivBackward0>)\n",
      "8:20 loss --> tensor([1.8341], grad_fn=<DivBackward0>)\n",
      "8:30 loss --> tensor([0.8777], grad_fn=<DivBackward0>)\n",
      "8:40 loss --> tensor([2.0932], grad_fn=<DivBackward0>)\n",
      "8:50 loss --> tensor([2.4232], grad_fn=<DivBackward0>)\n",
      "8:60 loss --> tensor([1.4747], grad_fn=<DivBackward0>)\n",
      "8:70 loss --> tensor([2.6397], grad_fn=<DivBackward0>)\n",
      "8:80 loss --> tensor([1.4562], grad_fn=<DivBackward0>)\n",
      "8:90 loss --> tensor([5.6277], grad_fn=<DivBackward0>)\n",
      "8:100 loss --> tensor([2.2702], grad_fn=<DivBackward0>)\n",
      "8:110 loss --> tensor([4.9212], grad_fn=<DivBackward0>)\n",
      "8:120 loss --> tensor([3.8063], grad_fn=<DivBackward0>)\n",
      "8:130 loss --> tensor([4.5863], grad_fn=<DivBackward0>)\n",
      "8:140 loss --> tensor([7.3029], grad_fn=<DivBackward0>)\n",
      "9:0 loss --> tensor([1.6593], grad_fn=<DivBackward0>)\n",
      "9:10 loss --> tensor([4.7982], grad_fn=<DivBackward0>)\n",
      "9:20 loss --> tensor([6.8615], grad_fn=<DivBackward0>)\n",
      "9:30 loss --> tensor([6.2552], grad_fn=<DivBackward0>)\n",
      "9:40 loss --> tensor([0.7976], grad_fn=<DivBackward0>)\n",
      "9:50 loss --> tensor([1.9928], grad_fn=<DivBackward0>)\n",
      "9:60 loss --> tensor([2.1863], grad_fn=<DivBackward0>)\n",
      "9:70 loss --> tensor([2.1109], grad_fn=<DivBackward0>)\n",
      "9:80 loss --> tensor([2.0646], grad_fn=<DivBackward0>)\n",
      "9:90 loss --> tensor([1.9391], grad_fn=<DivBackward0>)\n",
      "9:100 loss --> tensor([1.0768], grad_fn=<DivBackward0>)\n",
      "9:110 loss --> tensor([1.5237], grad_fn=<DivBackward0>)\n",
      "9:120 loss --> tensor([1.2981], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:130 loss --> tensor([1.2131], grad_fn=<DivBackward0>)\n",
      "9:140 loss --> tensor([2.1582], grad_fn=<DivBackward0>)\n",
      "10:0 loss --> tensor([0.7925], grad_fn=<DivBackward0>)\n",
      "10:10 loss --> tensor([1.0252], grad_fn=<DivBackward0>)\n",
      "10:20 loss --> tensor([0.9202], grad_fn=<DivBackward0>)\n",
      "10:30 loss --> tensor([2.2983], grad_fn=<DivBackward0>)\n",
      "10:40 loss --> tensor([1.7586], grad_fn=<DivBackward0>)\n",
      "10:50 loss --> tensor([1.0025], grad_fn=<DivBackward0>)\n",
      "10:60 loss --> tensor([1.1002], grad_fn=<DivBackward0>)\n",
      "10:70 loss --> tensor([1.6339], grad_fn=<DivBackward0>)\n",
      "10:80 loss --> tensor([1.1685], grad_fn=<DivBackward0>)\n",
      "10:90 loss --> tensor([1.7231], grad_fn=<DivBackward0>)\n",
      "10:100 loss --> tensor([1.5969], grad_fn=<DivBackward0>)\n",
      "10:110 loss --> tensor([1.5072], grad_fn=<DivBackward0>)\n",
      "10:120 loss --> tensor([1.4141], grad_fn=<DivBackward0>)\n",
      "10:130 loss --> tensor([0.9110], grad_fn=<DivBackward0>)\n",
      "10:140 loss --> tensor([0.9626], grad_fn=<DivBackward0>)\n",
      "11:0 loss --> tensor([1.0419], grad_fn=<DivBackward0>)\n",
      "11:10 loss --> tensor([0.6547], grad_fn=<DivBackward0>)\n",
      "11:20 loss --> tensor([0.7174], grad_fn=<DivBackward0>)\n",
      "11:30 loss --> tensor([0.4620], grad_fn=<DivBackward0>)\n",
      "11:40 loss --> tensor([1.2781], grad_fn=<DivBackward0>)\n",
      "11:50 loss --> tensor([6.5450], grad_fn=<DivBackward0>)\n",
      "11:60 loss --> tensor([7.3881], grad_fn=<DivBackward0>)\n",
      "11:70 loss --> tensor([2.4166], grad_fn=<DivBackward0>)\n",
      "11:80 loss --> tensor([2.4075], grad_fn=<DivBackward0>)\n",
      "11:90 loss --> tensor([1.7167], grad_fn=<DivBackward0>)\n",
      "11:100 loss --> tensor([0.8139], grad_fn=<DivBackward0>)\n",
      "11:110 loss --> tensor([1.8643], grad_fn=<DivBackward0>)\n",
      "11:120 loss --> tensor([1.5442], grad_fn=<DivBackward0>)\n",
      "11:130 loss --> tensor([2.2063], grad_fn=<DivBackward0>)\n",
      "11:140 loss --> tensor([1.2729], grad_fn=<DivBackward0>)\n",
      "12:0 loss --> tensor([0.7332], grad_fn=<DivBackward0>)\n",
      "12:10 loss --> tensor([1.1601], grad_fn=<DivBackward0>)\n",
      "12:20 loss --> tensor([0.6064], grad_fn=<DivBackward0>)\n",
      "12:30 loss --> tensor([1.3589], grad_fn=<DivBackward0>)\n",
      "12:40 loss --> tensor([2.3803], grad_fn=<DivBackward0>)\n",
      "12:50 loss --> tensor([1.9258], grad_fn=<DivBackward0>)\n",
      "12:60 loss --> tensor([1.3479], grad_fn=<DivBackward0>)\n",
      "12:70 loss --> tensor([0.5424], grad_fn=<DivBackward0>)\n",
      "12:80 loss --> tensor([1.5136], grad_fn=<DivBackward0>)\n",
      "12:90 loss --> tensor([0.8215], grad_fn=<DivBackward0>)\n",
      "12:100 loss --> tensor([1.0844], grad_fn=<DivBackward0>)\n",
      "12:110 loss --> tensor([0.7729], grad_fn=<DivBackward0>)\n",
      "12:120 loss --> tensor([0.7835], grad_fn=<DivBackward0>)\n",
      "12:130 loss --> tensor([1.4401], grad_fn=<DivBackward0>)\n",
      "12:140 loss --> tensor([0.6424], grad_fn=<DivBackward0>)\n",
      "13:0 loss --> tensor([0.5060], grad_fn=<DivBackward0>)\n",
      "13:10 loss --> tensor([0.8054], grad_fn=<DivBackward0>)\n",
      "13:20 loss --> tensor([1.5803], grad_fn=<DivBackward0>)\n",
      "13:30 loss --> tensor([0.8691], grad_fn=<DivBackward0>)\n",
      "13:40 loss --> tensor([0.7022], grad_fn=<DivBackward0>)\n",
      "13:50 loss --> tensor([2.0343], grad_fn=<DivBackward0>)\n",
      "13:60 loss --> tensor([0.5828], grad_fn=<DivBackward0>)\n",
      "13:70 loss --> tensor([1.5815], grad_fn=<DivBackward0>)\n",
      "13:80 loss --> tensor([0.4141], grad_fn=<DivBackward0>)\n",
      "13:90 loss --> tensor([0.7840], grad_fn=<DivBackward0>)\n",
      "13:100 loss --> tensor([0.8207], grad_fn=<DivBackward0>)\n",
      "13:110 loss --> tensor([0.4739], grad_fn=<DivBackward0>)\n",
      "13:120 loss --> tensor([0.7440], grad_fn=<DivBackward0>)\n",
      "13:130 loss --> tensor([0.4984], grad_fn=<DivBackward0>)\n",
      "13:140 loss --> tensor([3.0751], grad_fn=<DivBackward0>)\n",
      "14:0 loss --> tensor([1.7687], grad_fn=<DivBackward0>)\n",
      "14:10 loss --> tensor([0.6994], grad_fn=<DivBackward0>)\n",
      "14:20 loss --> tensor([0.5382], grad_fn=<DivBackward0>)\n",
      "14:30 loss --> tensor([0.5635], grad_fn=<DivBackward0>)\n",
      "14:40 loss --> tensor([1.2344], grad_fn=<DivBackward0>)\n",
      "14:50 loss --> tensor([0.4917], grad_fn=<DivBackward0>)\n",
      "14:60 loss --> tensor([0.5344], grad_fn=<DivBackward0>)\n",
      "14:70 loss --> tensor([0.7996], grad_fn=<DivBackward0>)\n",
      "14:80 loss --> tensor([1.0381], grad_fn=<DivBackward0>)\n",
      "14:90 loss --> tensor([0.5895], grad_fn=<DivBackward0>)\n",
      "14:100 loss --> tensor([0.2939], grad_fn=<DivBackward0>)\n",
      "14:110 loss --> tensor([0.6370], grad_fn=<DivBackward0>)\n",
      "14:120 loss --> tensor([0.3712], grad_fn=<DivBackward0>)\n",
      "14:130 loss --> tensor([0.9777], grad_fn=<DivBackward0>)\n",
      "14:140 loss --> tensor([0.7015], grad_fn=<DivBackward0>)\n",
      "15:0 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "15:10 loss --> tensor([0.5328], grad_fn=<DivBackward0>)\n",
      "15:20 loss --> tensor([0.3816], grad_fn=<DivBackward0>)\n",
      "15:30 loss --> tensor([0.4990], grad_fn=<DivBackward0>)\n",
      "15:40 loss --> tensor([0.3379], grad_fn=<DivBackward0>)\n",
      "15:50 loss --> tensor([0.3161], grad_fn=<DivBackward0>)\n",
      "15:60 loss --> tensor([0.8670], grad_fn=<DivBackward0>)\n",
      "15:70 loss --> tensor([1.2586], grad_fn=<DivBackward0>)\n",
      "15:80 loss --> tensor([0.3472], grad_fn=<DivBackward0>)\n",
      "15:90 loss --> tensor([0.3999], grad_fn=<DivBackward0>)\n",
      "15:100 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "15:110 loss --> tensor([0.2552], grad_fn=<DivBackward0>)\n",
      "15:120 loss --> tensor([0.5391], grad_fn=<DivBackward0>)\n",
      "15:130 loss --> tensor([0.5850], grad_fn=<DivBackward0>)\n",
      "15:140 loss --> tensor([0.3354], grad_fn=<DivBackward0>)\n",
      "16:0 loss --> tensor([0.8459], grad_fn=<DivBackward0>)\n",
      "16:10 loss --> tensor([0.2451], grad_fn=<DivBackward0>)\n",
      "16:20 loss --> tensor([0.6537], grad_fn=<DivBackward0>)\n",
      "16:30 loss --> tensor([0.8570], grad_fn=<DivBackward0>)\n",
      "16:40 loss --> tensor([0.2404], grad_fn=<DivBackward0>)\n",
      "16:50 loss --> tensor([0.5455], grad_fn=<DivBackward0>)\n",
      "16:60 loss --> tensor([0.3966], grad_fn=<DivBackward0>)\n",
      "16:70 loss --> tensor([0.5658], grad_fn=<DivBackward0>)\n",
      "16:80 loss --> tensor([0.3174], grad_fn=<DivBackward0>)\n",
      "16:90 loss --> tensor([0.3947], grad_fn=<DivBackward0>)\n",
      "16:100 loss --> tensor([0.5657], grad_fn=<DivBackward0>)\n",
      "16:110 loss --> tensor([0.3207], grad_fn=<DivBackward0>)\n",
      "16:120 loss --> tensor([0.3135], grad_fn=<DivBackward0>)\n",
      "16:130 loss --> tensor([0.4019], grad_fn=<DivBackward0>)\n",
      "16:140 loss --> tensor([0.2041], grad_fn=<DivBackward0>)\n",
      "17:0 loss --> tensor([0.7636], grad_fn=<DivBackward0>)\n",
      "17:10 loss --> tensor([0.5931], grad_fn=<DivBackward0>)\n",
      "17:20 loss --> tensor([0.2087], grad_fn=<DivBackward0>)\n",
      "17:30 loss --> tensor([2.0180], grad_fn=<DivBackward0>)\n",
      "17:40 loss --> tensor([0.4556], grad_fn=<DivBackward0>)\n",
      "17:50 loss --> tensor([0.4006], grad_fn=<DivBackward0>)\n",
      "17:60 loss --> tensor([0.2948], grad_fn=<DivBackward0>)\n",
      "17:70 loss --> tensor([0.2974], grad_fn=<DivBackward0>)\n",
      "17:80 loss --> tensor([0.2688], grad_fn=<DivBackward0>)\n",
      "17:90 loss --> tensor([0.2111], grad_fn=<DivBackward0>)\n",
      "17:100 loss --> tensor([0.3759], grad_fn=<DivBackward0>)\n",
      "17:110 loss --> tensor([0.2699], grad_fn=<DivBackward0>)\n",
      "17:120 loss --> tensor([0.4306], grad_fn=<DivBackward0>)\n",
      "17:130 loss --> tensor([0.2390], grad_fn=<DivBackward0>)\n",
      "17:140 loss --> tensor([0.3765], grad_fn=<DivBackward0>)\n",
      "18:0 loss --> tensor([0.3233], grad_fn=<DivBackward0>)\n",
      "18:10 loss --> tensor([0.3022], grad_fn=<DivBackward0>)\n",
      "18:20 loss --> tensor([0.2504], grad_fn=<DivBackward0>)\n",
      "18:30 loss --> tensor([0.2636], grad_fn=<DivBackward0>)\n",
      "18:40 loss --> tensor([0.3139], grad_fn=<DivBackward0>)\n",
      "18:50 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "18:60 loss --> tensor([0.1966], grad_fn=<DivBackward0>)\n",
      "18:70 loss --> tensor([0.1337], grad_fn=<DivBackward0>)\n",
      "18:80 loss --> tensor([0.2214], grad_fn=<DivBackward0>)\n",
      "18:90 loss --> tensor([0.1435], grad_fn=<DivBackward0>)\n",
      "18:100 loss --> tensor([0.2211], grad_fn=<DivBackward0>)\n",
      "18:110 loss --> tensor([0.5994], grad_fn=<DivBackward0>)\n",
      "18:120 loss --> tensor([0.2253], grad_fn=<DivBackward0>)\n",
      "18:130 loss --> tensor([0.1877], grad_fn=<DivBackward0>)\n",
      "18:140 loss --> tensor([0.2907], grad_fn=<DivBackward0>)\n",
      "19:0 loss --> tensor([0.3078], grad_fn=<DivBackward0>)\n",
      "19:10 loss --> tensor([0.6215], grad_fn=<DivBackward0>)\n",
      "19:20 loss --> tensor([0.1777], grad_fn=<DivBackward0>)\n",
      "19:30 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "19:40 loss --> tensor([0.1926], grad_fn=<DivBackward0>)\n",
      "19:50 loss --> tensor([0.2600], grad_fn=<DivBackward0>)\n",
      "19:60 loss --> tensor([0.2780], grad_fn=<DivBackward0>)\n",
      "19:70 loss --> tensor([0.3205], grad_fn=<DivBackward0>)\n",
      "19:80 loss --> tensor([0.3586], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:90 loss --> tensor([0.2520], grad_fn=<DivBackward0>)\n",
      "19:100 loss --> tensor([0.4761], grad_fn=<DivBackward0>)\n",
      "19:110 loss --> tensor([0.4002], grad_fn=<DivBackward0>)\n",
      "19:120 loss --> tensor([0.2502], grad_fn=<DivBackward0>)\n",
      "19:130 loss --> tensor([0.3875], grad_fn=<DivBackward0>)\n",
      "19:140 loss --> tensor([0.4349], grad_fn=<DivBackward0>)\n",
      "20:0 loss --> tensor([0.2993], grad_fn=<DivBackward0>)\n",
      "20:10 loss --> tensor([0.2748], grad_fn=<DivBackward0>)\n",
      "20:20 loss --> tensor([0.3232], grad_fn=<DivBackward0>)\n",
      "20:30 loss --> tensor([0.2080], grad_fn=<DivBackward0>)\n",
      "20:40 loss --> tensor([0.6131], grad_fn=<DivBackward0>)\n",
      "20:50 loss --> tensor([0.2125], grad_fn=<DivBackward0>)\n",
      "20:60 loss --> tensor([0.3636], grad_fn=<DivBackward0>)\n",
      "20:70 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "20:80 loss --> tensor([0.1873], grad_fn=<DivBackward0>)\n",
      "20:90 loss --> tensor([0.2647], grad_fn=<DivBackward0>)\n",
      "20:100 loss --> tensor([0.2375], grad_fn=<DivBackward0>)\n",
      "20:110 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "20:120 loss --> tensor([0.3103], grad_fn=<DivBackward0>)\n",
      "20:130 loss --> tensor([0.2169], grad_fn=<DivBackward0>)\n",
      "20:140 loss --> tensor([0.2205], grad_fn=<DivBackward0>)\n",
      "21:0 loss --> tensor([0.1945], grad_fn=<DivBackward0>)\n",
      "21:10 loss --> tensor([0.1795], grad_fn=<DivBackward0>)\n",
      "21:20 loss --> tensor([0.2733], grad_fn=<DivBackward0>)\n",
      "21:30 loss --> tensor([0.5972], grad_fn=<DivBackward0>)\n",
      "21:40 loss --> tensor([0.2206], grad_fn=<DivBackward0>)\n",
      "21:50 loss --> tensor([0.5153], grad_fn=<DivBackward0>)\n",
      "21:60 loss --> tensor([0.2282], grad_fn=<DivBackward0>)\n",
      "21:70 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "21:80 loss --> tensor([0.3529], grad_fn=<DivBackward0>)\n",
      "21:90 loss --> tensor([0.1691], grad_fn=<DivBackward0>)\n",
      "21:100 loss --> tensor([0.8482], grad_fn=<DivBackward0>)\n",
      "21:110 loss --> tensor([0.5682], grad_fn=<DivBackward0>)\n",
      "21:120 loss --> tensor([0.2313], grad_fn=<DivBackward0>)\n",
      "21:130 loss --> tensor([0.3063], grad_fn=<DivBackward0>)\n",
      "21:140 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "22:0 loss --> tensor([0.4840], grad_fn=<DivBackward0>)\n",
      "22:10 loss --> tensor([0.1278], grad_fn=<DivBackward0>)\n",
      "22:20 loss --> tensor([0.5040], grad_fn=<DivBackward0>)\n",
      "22:30 loss --> tensor([0.2317], grad_fn=<DivBackward0>)\n",
      "22:40 loss --> tensor([0.1825], grad_fn=<DivBackward0>)\n",
      "22:50 loss --> tensor([0.1236], grad_fn=<DivBackward0>)\n",
      "22:60 loss --> tensor([0.1399], grad_fn=<DivBackward0>)\n",
      "22:70 loss --> tensor([0.6543], grad_fn=<DivBackward0>)\n",
      "22:80 loss --> tensor([0.2270], grad_fn=<DivBackward0>)\n",
      "22:90 loss --> tensor([0.1622], grad_fn=<DivBackward0>)\n",
      "22:100 loss --> tensor([0.3706], grad_fn=<DivBackward0>)\n",
      "22:110 loss --> tensor([0.2215], grad_fn=<DivBackward0>)\n",
      "22:120 loss --> tensor([0.1461], grad_fn=<DivBackward0>)\n",
      "22:130 loss --> tensor([0.3308], grad_fn=<DivBackward0>)\n",
      "22:140 loss --> tensor([0.6865], grad_fn=<DivBackward0>)\n",
      "23:0 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "23:10 loss --> tensor([0.1716], grad_fn=<DivBackward0>)\n",
      "23:20 loss --> tensor([0.1999], grad_fn=<DivBackward0>)\n",
      "23:30 loss --> tensor([0.2259], grad_fn=<DivBackward0>)\n",
      "23:40 loss --> tensor([0.6879], grad_fn=<DivBackward0>)\n",
      "23:50 loss --> tensor([0.6804], grad_fn=<DivBackward0>)\n",
      "23:60 loss --> tensor([0.6476], grad_fn=<DivBackward0>)\n",
      "23:70 loss --> tensor([0.2104], grad_fn=<DivBackward0>)\n",
      "23:80 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "23:90 loss --> tensor([0.1758], grad_fn=<DivBackward0>)\n",
      "23:100 loss --> tensor([0.2035], grad_fn=<DivBackward0>)\n",
      "23:110 loss --> tensor([0.3017], grad_fn=<DivBackward0>)\n",
      "23:120 loss --> tensor([0.1398], grad_fn=<DivBackward0>)\n",
      "23:130 loss --> tensor([0.1866], grad_fn=<DivBackward0>)\n",
      "23:140 loss --> tensor([0.2867], grad_fn=<DivBackward0>)\n",
      "24:0 loss --> tensor([0.2228], grad_fn=<DivBackward0>)\n",
      "24:10 loss --> tensor([0.2641], grad_fn=<DivBackward0>)\n",
      "24:20 loss --> tensor([0.4042], grad_fn=<DivBackward0>)\n",
      "24:30 loss --> tensor([0.1942], grad_fn=<DivBackward0>)\n",
      "24:40 loss --> tensor([0.2116], grad_fn=<DivBackward0>)\n",
      "24:50 loss --> tensor([0.1308], grad_fn=<DivBackward0>)\n",
      "24:60 loss --> tensor([0.2126], grad_fn=<DivBackward0>)\n",
      "24:70 loss --> tensor([0.3621], grad_fn=<DivBackward0>)\n",
      "24:80 loss --> tensor([0.1701], grad_fn=<DivBackward0>)\n",
      "24:90 loss --> tensor([0.6316], grad_fn=<DivBackward0>)\n",
      "24:100 loss --> tensor([0.1923], grad_fn=<DivBackward0>)\n",
      "24:110 loss --> tensor([0.2499], grad_fn=<DivBackward0>)\n",
      "24:120 loss --> tensor([0.1392], grad_fn=<DivBackward0>)\n",
      "24:130 loss --> tensor([0.1432], grad_fn=<DivBackward0>)\n",
      "24:140 loss --> tensor([0.3798], grad_fn=<DivBackward0>)\n",
      "25:0 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "25:10 loss --> tensor([0.1204], grad_fn=<DivBackward0>)\n",
      "25:20 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "25:30 loss --> tensor([0.2967], grad_fn=<DivBackward0>)\n",
      "25:40 loss --> tensor([0.1840], grad_fn=<DivBackward0>)\n",
      "25:50 loss --> tensor([0.5188], grad_fn=<DivBackward0>)\n",
      "25:60 loss --> tensor([0.1903], grad_fn=<DivBackward0>)\n",
      "25:70 loss --> tensor([0.3976], grad_fn=<DivBackward0>)\n",
      "25:80 loss --> tensor([0.5232], grad_fn=<DivBackward0>)\n",
      "25:90 loss --> tensor([0.1803], grad_fn=<DivBackward0>)\n",
      "25:100 loss --> tensor([0.1965], grad_fn=<DivBackward0>)\n",
      "25:110 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "25:120 loss --> tensor([0.2039], grad_fn=<DivBackward0>)\n",
      "25:130 loss --> tensor([0.4120], grad_fn=<DivBackward0>)\n",
      "25:140 loss --> tensor([0.1892], grad_fn=<DivBackward0>)\n",
      "26:0 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "26:10 loss --> tensor([0.1450], grad_fn=<DivBackward0>)\n",
      "26:20 loss --> tensor([0.1686], grad_fn=<DivBackward0>)\n",
      "26:30 loss --> tensor([0.1820], grad_fn=<DivBackward0>)\n",
      "26:40 loss --> tensor([0.0995], grad_fn=<DivBackward0>)\n",
      "26:50 loss --> tensor([0.1749], grad_fn=<DivBackward0>)\n",
      "26:60 loss --> tensor([0.1148], grad_fn=<DivBackward0>)\n",
      "26:70 loss --> tensor([0.3823], grad_fn=<DivBackward0>)\n",
      "26:80 loss --> tensor([0.1242], grad_fn=<DivBackward0>)\n",
      "26:90 loss --> tensor([0.1387], grad_fn=<DivBackward0>)\n",
      "26:100 loss --> tensor([0.1987], grad_fn=<DivBackward0>)\n",
      "26:110 loss --> tensor([0.2785], grad_fn=<DivBackward0>)\n",
      "26:120 loss --> tensor([0.7383], grad_fn=<DivBackward0>)\n",
      "26:130 loss --> tensor([0.1110], grad_fn=<DivBackward0>)\n",
      "26:140 loss --> tensor([0.2265], grad_fn=<DivBackward0>)\n",
      "27:0 loss --> tensor([0.2480], grad_fn=<DivBackward0>)\n",
      "27:10 loss --> tensor([0.1217], grad_fn=<DivBackward0>)\n",
      "27:20 loss --> tensor([0.5772], grad_fn=<DivBackward0>)\n",
      "27:30 loss --> tensor([0.1190], grad_fn=<DivBackward0>)\n",
      "27:40 loss --> tensor([0.1580], grad_fn=<DivBackward0>)\n",
      "27:50 loss --> tensor([0.3171], grad_fn=<DivBackward0>)\n",
      "27:60 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "27:70 loss --> tensor([0.1429], grad_fn=<DivBackward0>)\n",
      "27:80 loss --> tensor([0.1151], grad_fn=<DivBackward0>)\n",
      "27:90 loss --> tensor([0.2224], grad_fn=<DivBackward0>)\n",
      "27:100 loss --> tensor([0.2588], grad_fn=<DivBackward0>)\n",
      "27:110 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "27:120 loss --> tensor([0.1347], grad_fn=<DivBackward0>)\n",
      "27:130 loss --> tensor([0.1507], grad_fn=<DivBackward0>)\n",
      "27:140 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "28:0 loss --> tensor([0.1605], grad_fn=<DivBackward0>)\n",
      "28:10 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "28:20 loss --> tensor([0.1793], grad_fn=<DivBackward0>)\n",
      "28:30 loss --> tensor([0.1291], grad_fn=<DivBackward0>)\n",
      "28:40 loss --> tensor([0.1934], grad_fn=<DivBackward0>)\n",
      "28:50 loss --> tensor([0.1760], grad_fn=<DivBackward0>)\n",
      "28:60 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "28:70 loss --> tensor([0.1467], grad_fn=<DivBackward0>)\n",
      "28:80 loss --> tensor([0.2403], grad_fn=<DivBackward0>)\n",
      "28:90 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "28:100 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "28:110 loss --> tensor([0.1700], grad_fn=<DivBackward0>)\n",
      "28:120 loss --> tensor([0.3202], grad_fn=<DivBackward0>)\n",
      "28:130 loss --> tensor([0.1458], grad_fn=<DivBackward0>)\n",
      "28:140 loss --> tensor([0.2102], grad_fn=<DivBackward0>)\n",
      "29:0 loss --> tensor([0.1132], grad_fn=<DivBackward0>)\n",
      "29:10 loss --> tensor([0.1841], grad_fn=<DivBackward0>)\n",
      "29:20 loss --> tensor([0.1211], grad_fn=<DivBackward0>)\n",
      "29:30 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "29:40 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29:50 loss --> tensor([0.1220], grad_fn=<DivBackward0>)\n",
      "29:60 loss --> tensor([0.1549], grad_fn=<DivBackward0>)\n",
      "29:70 loss --> tensor([0.1106], grad_fn=<DivBackward0>)\n",
      "29:80 loss --> tensor([0.1326], grad_fn=<DivBackward0>)\n",
      "29:90 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "29:100 loss --> tensor([0.2556], grad_fn=<DivBackward0>)\n",
      "29:110 loss --> tensor([0.0985], grad_fn=<DivBackward0>)\n",
      "29:120 loss --> tensor([0.1253], grad_fn=<DivBackward0>)\n",
      "29:130 loss --> tensor([0.1685], grad_fn=<DivBackward0>)\n",
      "29:140 loss --> tensor([0.2129], grad_fn=<DivBackward0>)\n",
      "30:0 loss --> tensor([0.3538], grad_fn=<DivBackward0>)\n",
      "30:10 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "30:20 loss --> tensor([0.1093], grad_fn=<DivBackward0>)\n",
      "30:30 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "30:40 loss --> tensor([0.1302], grad_fn=<DivBackward0>)\n",
      "30:50 loss --> tensor([0.2209], grad_fn=<DivBackward0>)\n",
      "30:60 loss --> tensor([0.2022], grad_fn=<DivBackward0>)\n",
      "30:70 loss --> tensor([0.1259], grad_fn=<DivBackward0>)\n",
      "30:80 loss --> tensor([0.1509], grad_fn=<DivBackward0>)\n",
      "30:90 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "30:100 loss --> tensor([0.1401], grad_fn=<DivBackward0>)\n",
      "30:110 loss --> tensor([0.1174], grad_fn=<DivBackward0>)\n",
      "30:120 loss --> tensor([0.1314], grad_fn=<DivBackward0>)\n",
      "30:130 loss --> tensor([0.1074], grad_fn=<DivBackward0>)\n",
      "30:140 loss --> tensor([0.0943], grad_fn=<DivBackward0>)\n",
      "31:0 loss --> tensor([0.1456], grad_fn=<DivBackward0>)\n",
      "31:10 loss --> tensor([0.1727], grad_fn=<DivBackward0>)\n",
      "31:20 loss --> tensor([0.1765], grad_fn=<DivBackward0>)\n",
      "31:30 loss --> tensor([0.2947], grad_fn=<DivBackward0>)\n",
      "31:40 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "31:50 loss --> tensor([0.1256], grad_fn=<DivBackward0>)\n",
      "31:60 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "31:70 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "31:80 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "31:90 loss --> tensor([0.7890], grad_fn=<DivBackward0>)\n",
      "31:100 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "31:110 loss --> tensor([0.1219], grad_fn=<DivBackward0>)\n",
      "31:120 loss --> tensor([0.1342], grad_fn=<DivBackward0>)\n",
      "31:130 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "31:140 loss --> tensor([0.3037], grad_fn=<DivBackward0>)\n",
      "32:0 loss --> tensor([0.0918], grad_fn=<DivBackward0>)\n",
      "32:10 loss --> tensor([0.1039], grad_fn=<DivBackward0>)\n",
      "32:20 loss --> tensor([0.0852], grad_fn=<DivBackward0>)\n",
      "32:30 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "32:40 loss --> tensor([0.1182], grad_fn=<DivBackward0>)\n",
      "32:50 loss --> tensor([0.2883], grad_fn=<DivBackward0>)\n",
      "32:60 loss --> tensor([0.1487], grad_fn=<DivBackward0>)\n",
      "32:70 loss --> tensor([0.3298], grad_fn=<DivBackward0>)\n",
      "32:80 loss --> tensor([0.2597], grad_fn=<DivBackward0>)\n",
      "32:90 loss --> tensor([0.2056], grad_fn=<DivBackward0>)\n",
      "32:100 loss --> tensor([0.2085], grad_fn=<DivBackward0>)\n",
      "32:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "32:120 loss --> tensor([0.2013], grad_fn=<DivBackward0>)\n",
      "32:130 loss --> tensor([0.0907], grad_fn=<DivBackward0>)\n",
      "32:140 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "33:0 loss --> tensor([0.1353], grad_fn=<DivBackward0>)\n",
      "33:10 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "33:20 loss --> tensor([0.0904], grad_fn=<DivBackward0>)\n",
      "33:30 loss --> tensor([0.1138], grad_fn=<DivBackward0>)\n",
      "33:40 loss --> tensor([0.0739], grad_fn=<DivBackward0>)\n",
      "33:50 loss --> tensor([0.1927], grad_fn=<DivBackward0>)\n",
      "33:60 loss --> tensor([0.1201], grad_fn=<DivBackward0>)\n",
      "33:70 loss --> tensor([0.0894], grad_fn=<DivBackward0>)\n",
      "33:80 loss --> tensor([0.2045], grad_fn=<DivBackward0>)\n",
      "33:90 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "33:100 loss --> tensor([0.2642], grad_fn=<DivBackward0>)\n",
      "33:110 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "33:120 loss --> tensor([0.1246], grad_fn=<DivBackward0>)\n",
      "33:130 loss --> tensor([0.1243], grad_fn=<DivBackward0>)\n",
      "33:140 loss --> tensor([0.2314], grad_fn=<DivBackward0>)\n",
      "34:0 loss --> tensor([0.1745], grad_fn=<DivBackward0>)\n",
      "34:10 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "34:20 loss --> tensor([0.2813], grad_fn=<DivBackward0>)\n",
      "34:30 loss --> tensor([0.2197], grad_fn=<DivBackward0>)\n",
      "34:40 loss --> tensor([0.2510], grad_fn=<DivBackward0>)\n",
      "34:50 loss --> tensor([0.0774], grad_fn=<DivBackward0>)\n",
      "34:60 loss --> tensor([0.1177], grad_fn=<DivBackward0>)\n",
      "34:70 loss --> tensor([0.1260], grad_fn=<DivBackward0>)\n",
      "34:80 loss --> tensor([0.0923], grad_fn=<DivBackward0>)\n",
      "34:90 loss --> tensor([0.0764], grad_fn=<DivBackward0>)\n",
      "34:100 loss --> tensor([0.1421], grad_fn=<DivBackward0>)\n",
      "34:110 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "34:120 loss --> tensor([0.0772], grad_fn=<DivBackward0>)\n",
      "34:130 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "34:140 loss --> tensor([0.2277], grad_fn=<DivBackward0>)\n",
      "35:0 loss --> tensor([0.8505], grad_fn=<DivBackward0>)\n",
      "35:10 loss --> tensor([0.3195], grad_fn=<DivBackward0>)\n",
      "35:20 loss --> tensor([0.1292], grad_fn=<DivBackward0>)\n",
      "35:30 loss --> tensor([0.0888], grad_fn=<DivBackward0>)\n",
      "35:40 loss --> tensor([0.0875], grad_fn=<DivBackward0>)\n",
      "35:50 loss --> tensor([0.0966], grad_fn=<DivBackward0>)\n",
      "35:60 loss --> tensor([0.1363], grad_fn=<DivBackward0>)\n",
      "35:70 loss --> tensor([0.1410], grad_fn=<DivBackward0>)\n",
      "35:80 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "35:90 loss --> tensor([0.3423], grad_fn=<DivBackward0>)\n",
      "35:100 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "35:110 loss --> tensor([0.2190], grad_fn=<DivBackward0>)\n",
      "35:120 loss --> tensor([0.1662], grad_fn=<DivBackward0>)\n",
      "35:130 loss --> tensor([0.2361], grad_fn=<DivBackward0>)\n",
      "35:140 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "36:0 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "36:10 loss --> tensor([0.0690], grad_fn=<DivBackward0>)\n",
      "36:20 loss --> tensor([0.1189], grad_fn=<DivBackward0>)\n",
      "36:30 loss --> tensor([0.1005], grad_fn=<DivBackward0>)\n",
      "36:40 loss --> tensor([0.0604], grad_fn=<DivBackward0>)\n",
      "36:50 loss --> tensor([0.0631], grad_fn=<DivBackward0>)\n",
      "36:60 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "36:70 loss --> tensor([0.1114], grad_fn=<DivBackward0>)\n",
      "36:80 loss --> tensor([0.0975], grad_fn=<DivBackward0>)\n",
      "36:90 loss --> tensor([0.1018], grad_fn=<DivBackward0>)\n",
      "36:100 loss --> tensor([0.0887], grad_fn=<DivBackward0>)\n",
      "36:110 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "36:120 loss --> tensor([0.2512], grad_fn=<DivBackward0>)\n",
      "36:130 loss --> tensor([0.0786], grad_fn=<DivBackward0>)\n",
      "36:140 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "37:0 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "37:10 loss --> tensor([0.1330], grad_fn=<DivBackward0>)\n",
      "37:20 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "37:30 loss --> tensor([0.1012], grad_fn=<DivBackward0>)\n",
      "37:40 loss --> tensor([0.0704], grad_fn=<DivBackward0>)\n",
      "37:50 loss --> tensor([0.2084], grad_fn=<DivBackward0>)\n",
      "37:60 loss --> tensor([0.1598], grad_fn=<DivBackward0>)\n",
      "37:70 loss --> tensor([0.3623], grad_fn=<DivBackward0>)\n",
      "37:80 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "37:90 loss --> tensor([0.1809], grad_fn=<DivBackward0>)\n",
      "37:100 loss --> tensor([0.0755], grad_fn=<DivBackward0>)\n",
      "37:110 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "37:120 loss --> tensor([0.2436], grad_fn=<DivBackward0>)\n",
      "37:130 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "37:140 loss --> tensor([0.0765], grad_fn=<DivBackward0>)\n",
      "38:0 loss --> tensor([0.2501], grad_fn=<DivBackward0>)\n",
      "38:10 loss --> tensor([0.2694], grad_fn=<DivBackward0>)\n",
      "38:20 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "38:30 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "38:40 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "38:50 loss --> tensor([0.1377], grad_fn=<DivBackward0>)\n",
      "38:60 loss --> tensor([0.0838], grad_fn=<DivBackward0>)\n",
      "38:70 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "38:80 loss --> tensor([0.0823], grad_fn=<DivBackward0>)\n",
      "38:90 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "38:100 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "38:110 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "38:120 loss --> tensor([0.1386], grad_fn=<DivBackward0>)\n",
      "38:130 loss --> tensor([0.1328], grad_fn=<DivBackward0>)\n",
      "38:140 loss --> tensor([0.1798], grad_fn=<DivBackward0>)\n",
      "39:0 loss --> tensor([0.2380], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:10 loss --> tensor([0.1860], grad_fn=<DivBackward0>)\n",
      "39:20 loss --> tensor([0.0797], grad_fn=<DivBackward0>)\n",
      "39:30 loss --> tensor([0.0885], grad_fn=<DivBackward0>)\n",
      "39:40 loss --> tensor([0.1416], grad_fn=<DivBackward0>)\n",
      "39:50 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "39:60 loss --> tensor([0.1277], grad_fn=<DivBackward0>)\n",
      "39:70 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "39:80 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "39:90 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n",
      "39:100 loss --> tensor([0.0903], grad_fn=<DivBackward0>)\n",
      "39:110 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "39:120 loss --> tensor([0.0649], grad_fn=<DivBackward0>)\n",
      "39:130 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "39:140 loss --> tensor([0.0893], grad_fn=<DivBackward0>)\n",
      "40:0 loss --> tensor([0.0796], grad_fn=<DivBackward0>)\n",
      "40:10 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "40:20 loss --> tensor([0.1261], grad_fn=<DivBackward0>)\n",
      "40:30 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "40:40 loss --> tensor([0.0792], grad_fn=<DivBackward0>)\n",
      "40:50 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:60 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "40:70 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "40:80 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "40:90 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:100 loss --> tensor([0.1431], grad_fn=<DivBackward0>)\n",
      "40:110 loss --> tensor([0.1095], grad_fn=<DivBackward0>)\n",
      "40:120 loss --> tensor([0.2681], grad_fn=<DivBackward0>)\n",
      "40:130 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "40:140 loss --> tensor([0.1379], grad_fn=<DivBackward0>)\n",
      "41:0 loss --> tensor([0.1477], grad_fn=<DivBackward0>)\n",
      "41:10 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "41:20 loss --> tensor([0.1357], grad_fn=<DivBackward0>)\n",
      "41:30 loss --> tensor([0.0880], grad_fn=<DivBackward0>)\n",
      "41:40 loss --> tensor([0.1157], grad_fn=<DivBackward0>)\n",
      "41:50 loss --> tensor([0.1320], grad_fn=<DivBackward0>)\n",
      "41:60 loss --> tensor([0.1032], grad_fn=<DivBackward0>)\n",
      "41:70 loss --> tensor([0.0720], grad_fn=<DivBackward0>)\n",
      "41:80 loss --> tensor([0.1126], grad_fn=<DivBackward0>)\n",
      "41:90 loss --> tensor([0.1883], grad_fn=<DivBackward0>)\n",
      "41:100 loss --> tensor([0.1225], grad_fn=<DivBackward0>)\n",
      "41:110 loss --> tensor([0.0845], grad_fn=<DivBackward0>)\n",
      "41:120 loss --> tensor([0.8526], grad_fn=<DivBackward0>)\n",
      "41:130 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "41:140 loss --> tensor([0.1140], grad_fn=<DivBackward0>)\n",
      "42:0 loss --> tensor([0.1035], grad_fn=<DivBackward0>)\n",
      "42:10 loss --> tensor([0.2274], grad_fn=<DivBackward0>)\n",
      "42:20 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "42:30 loss --> tensor([0.0849], grad_fn=<DivBackward0>)\n",
      "42:40 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "42:50 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "42:60 loss --> tensor([0.0729], grad_fn=<DivBackward0>)\n",
      "42:70 loss --> tensor([0.0872], grad_fn=<DivBackward0>)\n",
      "42:80 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "42:90 loss --> tensor([0.1650], grad_fn=<DivBackward0>)\n",
      "42:100 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "42:110 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "42:120 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "42:130 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "42:140 loss --> tensor([0.1334], grad_fn=<DivBackward0>)\n",
      "43:0 loss --> tensor([0.1250], grad_fn=<DivBackward0>)\n",
      "43:10 loss --> tensor([0.0933], grad_fn=<DivBackward0>)\n",
      "43:20 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "43:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "43:40 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "43:50 loss --> tensor([0.0876], grad_fn=<DivBackward0>)\n",
      "43:60 loss --> tensor([0.0798], grad_fn=<DivBackward0>)\n",
      "43:70 loss --> tensor([0.1634], grad_fn=<DivBackward0>)\n",
      "43:80 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "43:90 loss --> tensor([0.2038], grad_fn=<DivBackward0>)\n",
      "43:100 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "43:110 loss --> tensor([0.0824], grad_fn=<DivBackward0>)\n",
      "43:120 loss --> tensor([0.0936], grad_fn=<DivBackward0>)\n",
      "43:130 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "43:140 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "44:0 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "44:10 loss --> tensor([0.0826], grad_fn=<DivBackward0>)\n",
      "44:20 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "44:30 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "44:40 loss --> tensor([0.0857], grad_fn=<DivBackward0>)\n",
      "44:50 loss --> tensor([0.1806], grad_fn=<DivBackward0>)\n",
      "44:60 loss --> tensor([0.2760], grad_fn=<DivBackward0>)\n",
      "44:70 loss --> tensor([0.0694], grad_fn=<DivBackward0>)\n",
      "44:80 loss --> tensor([0.0534], grad_fn=<DivBackward0>)\n",
      "44:90 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "44:100 loss --> tensor([0.1193], grad_fn=<DivBackward0>)\n",
      "44:110 loss --> tensor([0.1304], grad_fn=<DivBackward0>)\n",
      "44:120 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "44:130 loss --> tensor([0.1864], grad_fn=<DivBackward0>)\n",
      "44:140 loss --> tensor([0.0735], grad_fn=<DivBackward0>)\n",
      "45:0 loss --> tensor([0.0708], grad_fn=<DivBackward0>)\n",
      "45:10 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "45:20 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "45:30 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "45:40 loss --> tensor([0.1462], grad_fn=<DivBackward0>)\n",
      "45:50 loss --> tensor([0.1526], grad_fn=<DivBackward0>)\n",
      "45:60 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "45:70 loss --> tensor([0.1695], grad_fn=<DivBackward0>)\n",
      "45:80 loss --> tensor([0.1129], grad_fn=<DivBackward0>)\n",
      "45:90 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "45:100 loss --> tensor([0.0627], grad_fn=<DivBackward0>)\n",
      "45:110 loss --> tensor([0.0886], grad_fn=<DivBackward0>)\n",
      "45:120 loss --> tensor([0.0750], grad_fn=<DivBackward0>)\n",
      "45:130 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "45:140 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "46:0 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "46:10 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "46:20 loss --> tensor([0.0588], grad_fn=<DivBackward0>)\n",
      "46:30 loss --> tensor([0.1152], grad_fn=<DivBackward0>)\n",
      "46:40 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "46:50 loss --> tensor([0.0705], grad_fn=<DivBackward0>)\n",
      "46:60 loss --> tensor([0.0673], grad_fn=<DivBackward0>)\n",
      "46:70 loss --> tensor([0.1202], grad_fn=<DivBackward0>)\n",
      "46:80 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "46:90 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "46:100 loss --> tensor([0.0519], grad_fn=<DivBackward0>)\n",
      "46:110 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "46:120 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "46:130 loss --> tensor([0.0947], grad_fn=<DivBackward0>)\n",
      "46:140 loss --> tensor([0.0787], grad_fn=<DivBackward0>)\n",
      "47:0 loss --> tensor([0.1555], grad_fn=<DivBackward0>)\n",
      "47:10 loss --> tensor([0.0528], grad_fn=<DivBackward0>)\n",
      "47:20 loss --> tensor([0.1205], grad_fn=<DivBackward0>)\n",
      "47:30 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "47:40 loss --> tensor([0.1183], grad_fn=<DivBackward0>)\n",
      "47:50 loss --> tensor([0.1963], grad_fn=<DivBackward0>)\n",
      "47:60 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "47:70 loss --> tensor([0.0482], grad_fn=<DivBackward0>)\n",
      "47:80 loss --> tensor([0.0801], grad_fn=<DivBackward0>)\n",
      "47:90 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "47:100 loss --> tensor([0.1557], grad_fn=<DivBackward0>)\n",
      "47:110 loss --> tensor([0.2377], grad_fn=<DivBackward0>)\n",
      "47:120 loss --> tensor([0.0759], grad_fn=<DivBackward0>)\n",
      "47:130 loss --> tensor([0.2052], grad_fn=<DivBackward0>)\n",
      "47:140 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:0 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "48:10 loss --> tensor([0.0606], grad_fn=<DivBackward0>)\n",
      "48:20 loss --> tensor([0.0602], grad_fn=<DivBackward0>)\n",
      "48:30 loss --> tensor([0.0575], grad_fn=<DivBackward0>)\n",
      "48:40 loss --> tensor([0.0643], grad_fn=<DivBackward0>)\n",
      "48:50 loss --> tensor([0.0789], grad_fn=<DivBackward0>)\n",
      "48:60 loss --> tensor([0.0742], grad_fn=<DivBackward0>)\n",
      "48:70 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "48:80 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:90 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "48:100 loss --> tensor([0.3702], grad_fn=<DivBackward0>)\n",
      "48:110 loss --> tensor([0.0442], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48:120 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "48:130 loss --> tensor([0.1465], grad_fn=<DivBackward0>)\n",
      "48:140 loss --> tensor([0.0457], grad_fn=<DivBackward0>)\n",
      "49:0 loss --> tensor([0.0611], grad_fn=<DivBackward0>)\n",
      "49:10 loss --> tensor([0.1714], grad_fn=<DivBackward0>)\n",
      "49:20 loss --> tensor([0.0749], grad_fn=<DivBackward0>)\n",
      "49:30 loss --> tensor([0.1451], grad_fn=<DivBackward0>)\n",
      "49:40 loss --> tensor([0.0926], grad_fn=<DivBackward0>)\n",
      "49:50 loss --> tensor([0.0763], grad_fn=<DivBackward0>)\n",
      "49:60 loss --> tensor([0.0895], grad_fn=<DivBackward0>)\n",
      "49:70 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "49:80 loss --> tensor([0.0934], grad_fn=<DivBackward0>)\n",
      "49:90 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "49:100 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "49:110 loss --> tensor([0.0861], grad_fn=<DivBackward0>)\n",
      "49:120 loss --> tensor([0.0731], grad_fn=<DivBackward0>)\n",
      "49:130 loss --> tensor([0.1325], grad_fn=<DivBackward0>)\n",
      "49:140 loss --> tensor([0.1481], grad_fn=<DivBackward0>)\n",
      "50:0 loss --> tensor([0.1210], grad_fn=<DivBackward0>)\n",
      "50:10 loss --> tensor([0.0490], grad_fn=<DivBackward0>)\n",
      "50:20 loss --> tensor([0.0626], grad_fn=<DivBackward0>)\n",
      "50:30 loss --> tensor([0.0662], grad_fn=<DivBackward0>)\n",
      "50:40 loss --> tensor([0.1070], grad_fn=<DivBackward0>)\n",
      "50:50 loss --> tensor([0.0652], grad_fn=<DivBackward0>)\n",
      "50:60 loss --> tensor([0.0732], grad_fn=<DivBackward0>)\n",
      "50:70 loss --> tensor([0.0590], grad_fn=<DivBackward0>)\n",
      "50:80 loss --> tensor([0.0489], grad_fn=<DivBackward0>)\n",
      "50:90 loss --> tensor([0.2185], grad_fn=<DivBackward0>)\n",
      "50:100 loss --> tensor([0.1642], grad_fn=<DivBackward0>)\n",
      "50:110 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "50:120 loss --> tensor([0.0710], grad_fn=<DivBackward0>)\n",
      "50:130 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "50:140 loss --> tensor([0.0803], grad_fn=<DivBackward0>)\n",
      "51:0 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "51:10 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "51:20 loss --> tensor([0.0501], grad_fn=<DivBackward0>)\n",
      "51:30 loss --> tensor([0.0556], grad_fn=<DivBackward0>)\n",
      "51:40 loss --> tensor([0.0846], grad_fn=<DivBackward0>)\n",
      "51:50 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "51:60 loss --> tensor([0.0654], grad_fn=<DivBackward0>)\n",
      "51:70 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "51:80 loss --> tensor([0.0474], grad_fn=<DivBackward0>)\n",
      "51:90 loss --> tensor([0.0659], grad_fn=<DivBackward0>)\n",
      "51:100 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "51:110 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "51:120 loss --> tensor([0.0762], grad_fn=<DivBackward0>)\n",
      "51:130 loss --> tensor([0.0700], grad_fn=<DivBackward0>)\n",
      "51:140 loss --> tensor([0.1950], grad_fn=<DivBackward0>)\n",
      "52:0 loss --> tensor([0.1135], grad_fn=<DivBackward0>)\n",
      "52:10 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "52:20 loss --> tensor([0.5752], grad_fn=<DivBackward0>)\n",
      "52:30 loss --> tensor([0.0773], grad_fn=<DivBackward0>)\n",
      "52:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "52:50 loss --> tensor([0.0469], grad_fn=<DivBackward0>)\n",
      "52:60 loss --> tensor([0.0806], grad_fn=<DivBackward0>)\n",
      "52:70 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "52:80 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "52:90 loss --> tensor([0.1563], grad_fn=<DivBackward0>)\n",
      "52:100 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "52:110 loss --> tensor([0.1221], grad_fn=<DivBackward0>)\n",
      "52:120 loss --> tensor([0.2011], grad_fn=<DivBackward0>)\n",
      "52:130 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "52:140 loss --> tensor([0.0601], grad_fn=<DivBackward0>)\n",
      "53:0 loss --> tensor([0.0808], grad_fn=<DivBackward0>)\n",
      "53:10 loss --> tensor([0.0529], grad_fn=<DivBackward0>)\n",
      "53:20 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "53:30 loss --> tensor([0.0648], grad_fn=<DivBackward0>)\n",
      "53:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "53:50 loss --> tensor([0.0834], grad_fn=<DivBackward0>)\n",
      "53:60 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "53:70 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "53:80 loss --> tensor([0.0589], grad_fn=<DivBackward0>)\n",
      "53:90 loss --> tensor([0.0853], grad_fn=<DivBackward0>)\n",
      "53:100 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "53:110 loss --> tensor([0.0635], grad_fn=<DivBackward0>)\n",
      "53:120 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "53:130 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "53:140 loss --> tensor([0.0510], grad_fn=<DivBackward0>)\n",
      "54:0 loss --> tensor([0.0979], grad_fn=<DivBackward0>)\n",
      "54:10 loss --> tensor([0.0468], grad_fn=<DivBackward0>)\n",
      "54:20 loss --> tensor([0.0751], grad_fn=<DivBackward0>)\n",
      "54:30 loss --> tensor([0.1009], grad_fn=<DivBackward0>)\n",
      "54:40 loss --> tensor([0.0406], grad_fn=<DivBackward0>)\n",
      "54:50 loss --> tensor([0.0863], grad_fn=<DivBackward0>)\n",
      "54:60 loss --> tensor([0.0520], grad_fn=<DivBackward0>)\n",
      "54:70 loss --> tensor([0.1224], grad_fn=<DivBackward0>)\n",
      "54:80 loss --> tensor([0.0551], grad_fn=<DivBackward0>)\n",
      "54:90 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "54:100 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "54:110 loss --> tensor([0.0655], grad_fn=<DivBackward0>)\n",
      "54:120 loss --> tensor([0.0607], grad_fn=<DivBackward0>)\n",
      "54:130 loss --> tensor([0.0458], grad_fn=<DivBackward0>)\n",
      "54:140 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "55:0 loss --> tensor([0.0901], grad_fn=<DivBackward0>)\n",
      "55:10 loss --> tensor([0.0513], grad_fn=<DivBackward0>)\n",
      "55:20 loss --> tensor([0.0522], grad_fn=<DivBackward0>)\n",
      "55:30 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "55:40 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "55:50 loss --> tensor([0.0647], grad_fn=<DivBackward0>)\n",
      "55:60 loss --> tensor([0.0674], grad_fn=<DivBackward0>)\n",
      "55:70 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "55:80 loss --> tensor([0.1081], grad_fn=<DivBackward0>)\n",
      "55:90 loss --> tensor([0.0389], grad_fn=<DivBackward0>)\n",
      "55:100 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "55:110 loss --> tensor([0.0340], grad_fn=<DivBackward0>)\n",
      "55:120 loss --> tensor([0.0666], grad_fn=<DivBackward0>)\n",
      "55:130 loss --> tensor([0.0585], grad_fn=<DivBackward0>)\n",
      "55:140 loss --> tensor([0.2171], grad_fn=<DivBackward0>)\n",
      "56:0 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "56:10 loss --> tensor([0.0965], grad_fn=<DivBackward0>)\n",
      "56:20 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "56:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "56:40 loss --> tensor([0.0550], grad_fn=<DivBackward0>)\n",
      "56:50 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "56:60 loss --> tensor([0.0533], grad_fn=<DivBackward0>)\n",
      "56:70 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "56:80 loss --> tensor([0.0596], grad_fn=<DivBackward0>)\n",
      "56:90 loss --> tensor([0.0570], grad_fn=<DivBackward0>)\n",
      "56:100 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "56:110 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "56:120 loss --> tensor([0.0595], grad_fn=<DivBackward0>)\n",
      "56:130 loss --> tensor([0.0650], grad_fn=<DivBackward0>)\n",
      "56:140 loss --> tensor([0.0770], grad_fn=<DivBackward0>)\n",
      "57:0 loss --> tensor([0.0624], grad_fn=<DivBackward0>)\n",
      "57:10 loss --> tensor([0.0847], grad_fn=<DivBackward0>)\n",
      "57:20 loss --> tensor([3.4022], grad_fn=<DivBackward0>)\n",
      "57:30 loss --> tensor([9.9447], grad_fn=<DivBackward0>)\n",
      "57:40 loss --> tensor([16.4048], grad_fn=<DivBackward0>)\n",
      "57:50 loss --> tensor([21.5102], grad_fn=<DivBackward0>)\n",
      "57:60 loss --> tensor([21.6429], grad_fn=<DivBackward0>)\n",
      "57:70 loss --> tensor([11.5656], grad_fn=<DivBackward0>)\n",
      "57:80 loss --> tensor([12.1413], grad_fn=<DivBackward0>)\n",
      "57:90 loss --> tensor([14.7159], grad_fn=<DivBackward0>)\n",
      "57:100 loss --> tensor([4.5711], grad_fn=<DivBackward0>)\n",
      "57:110 loss --> tensor([10.8564], grad_fn=<DivBackward0>)\n",
      "57:120 loss --> tensor([4.1304], grad_fn=<DivBackward0>)\n",
      "57:130 loss --> tensor([13.4019], grad_fn=<DivBackward0>)\n",
      "57:140 loss --> tensor([13.7998], grad_fn=<DivBackward0>)\n",
      "58:0 loss --> tensor([5.2142], grad_fn=<DivBackward0>)\n",
      "58:10 loss --> tensor([3.1334], grad_fn=<DivBackward0>)\n",
      "58:20 loss --> tensor([5.7519], grad_fn=<DivBackward0>)\n",
      "58:30 loss --> tensor([2.8351], grad_fn=<DivBackward0>)\n",
      "58:40 loss --> tensor([6.1557], grad_fn=<DivBackward0>)\n",
      "58:50 loss --> tensor([2.6427], grad_fn=<DivBackward0>)\n",
      "58:60 loss --> tensor([3.2629], grad_fn=<DivBackward0>)\n",
      "58:70 loss --> tensor([1.2779], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58:80 loss --> tensor([4.4251], grad_fn=<DivBackward0>)\n",
      "58:90 loss --> tensor([2.0784], grad_fn=<DivBackward0>)\n",
      "58:100 loss --> tensor([3.4789], grad_fn=<DivBackward0>)\n",
      "58:110 loss --> tensor([2.0481], grad_fn=<DivBackward0>)\n",
      "58:120 loss --> tensor([3.2647], grad_fn=<DivBackward0>)\n",
      "58:130 loss --> tensor([2.9361], grad_fn=<DivBackward0>)\n",
      "58:140 loss --> tensor([1.0645], grad_fn=<DivBackward0>)\n",
      "59:0 loss --> tensor([1.4378], grad_fn=<DivBackward0>)\n",
      "59:10 loss --> tensor([0.8588], grad_fn=<DivBackward0>)\n",
      "59:20 loss --> tensor([1.3759], grad_fn=<DivBackward0>)\n",
      "59:30 loss --> tensor([2.0949], grad_fn=<DivBackward0>)\n",
      "59:40 loss --> tensor([0.6558], grad_fn=<DivBackward0>)\n",
      "59:50 loss --> tensor([1.0538], grad_fn=<DivBackward0>)\n",
      "59:60 loss --> tensor([0.8373], grad_fn=<DivBackward0>)\n",
      "59:70 loss --> tensor([0.8074], grad_fn=<DivBackward0>)\n",
      "59:80 loss --> tensor([0.9266], grad_fn=<DivBackward0>)\n",
      "59:90 loss --> tensor([1.1950], grad_fn=<DivBackward0>)\n",
      "59:100 loss --> tensor([0.8516], grad_fn=<DivBackward0>)\n",
      "59:110 loss --> tensor([1.5343], grad_fn=<DivBackward0>)\n",
      "59:120 loss --> tensor([0.8791], grad_fn=<DivBackward0>)\n",
      "59:130 loss --> tensor([0.6115], grad_fn=<DivBackward0>)\n",
      "59:140 loss --> tensor([0.8940], grad_fn=<DivBackward0>)\n",
      "60:0 loss --> tensor([2.8170], grad_fn=<DivBackward0>)\n",
      "60:10 loss --> tensor([1.8884], grad_fn=<DivBackward0>)\n",
      "60:20 loss --> tensor([2.8654], grad_fn=<DivBackward0>)\n",
      "60:30 loss --> tensor([0.4791], grad_fn=<DivBackward0>)\n",
      "60:40 loss --> tensor([0.2849], grad_fn=<DivBackward0>)\n",
      "60:50 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "60:60 loss --> tensor([0.9389], grad_fn=<DivBackward0>)\n",
      "60:70 loss --> tensor([0.8761], grad_fn=<DivBackward0>)\n",
      "60:80 loss --> tensor([1.0118], grad_fn=<DivBackward0>)\n",
      "60:90 loss --> tensor([0.6532], grad_fn=<DivBackward0>)\n",
      "60:100 loss --> tensor([0.8287], grad_fn=<DivBackward0>)\n",
      "60:110 loss --> tensor([0.6260], grad_fn=<DivBackward0>)\n",
      "60:120 loss --> tensor([1.5400], grad_fn=<DivBackward0>)\n",
      "60:130 loss --> tensor([0.4415], grad_fn=<DivBackward0>)\n",
      "60:140 loss --> tensor([1.0203], grad_fn=<DivBackward0>)\n",
      "61:0 loss --> tensor([0.2919], grad_fn=<DivBackward0>)\n",
      "61:10 loss --> tensor([0.3358], grad_fn=<DivBackward0>)\n",
      "61:20 loss --> tensor([0.8916], grad_fn=<DivBackward0>)\n",
      "61:30 loss --> tensor([1.2408], grad_fn=<DivBackward0>)\n",
      "61:40 loss --> tensor([0.3908], grad_fn=<DivBackward0>)\n",
      "61:50 loss --> tensor([0.4140], grad_fn=<DivBackward0>)\n",
      "61:60 loss --> tensor([0.1920], grad_fn=<DivBackward0>)\n",
      "61:70 loss --> tensor([2.2829], grad_fn=<DivBackward0>)\n",
      "61:80 loss --> tensor([0.8797], grad_fn=<DivBackward0>)\n",
      "61:90 loss --> tensor([1.3787], grad_fn=<DivBackward0>)\n",
      "61:100 loss --> tensor([1.5900], grad_fn=<DivBackward0>)\n",
      "61:110 loss --> tensor([0.4128], grad_fn=<DivBackward0>)\n",
      "61:120 loss --> tensor([0.7773], grad_fn=<DivBackward0>)\n",
      "61:130 loss --> tensor([1.6292], grad_fn=<DivBackward0>)\n",
      "61:140 loss --> tensor([0.4005], grad_fn=<DivBackward0>)\n",
      "62:0 loss --> tensor([1.5718], grad_fn=<DivBackward0>)\n",
      "62:10 loss --> tensor([1.6002], grad_fn=<DivBackward0>)\n",
      "62:20 loss --> tensor([0.6693], grad_fn=<DivBackward0>)\n",
      "62:30 loss --> tensor([0.3175], grad_fn=<DivBackward0>)\n",
      "62:40 loss --> tensor([0.4632], grad_fn=<DivBackward0>)\n",
      "62:50 loss --> tensor([0.3199], grad_fn=<DivBackward0>)\n",
      "62:60 loss --> tensor([0.3003], grad_fn=<DivBackward0>)\n",
      "62:70 loss --> tensor([0.2534], grad_fn=<DivBackward0>)\n",
      "62:80 loss --> tensor([0.5483], grad_fn=<DivBackward0>)\n",
      "62:90 loss --> tensor([1.7910], grad_fn=<DivBackward0>)\n",
      "62:100 loss --> tensor([0.4748], grad_fn=<DivBackward0>)\n",
      "62:110 loss --> tensor([2.4929], grad_fn=<DivBackward0>)\n",
      "62:120 loss --> tensor([0.7229], grad_fn=<DivBackward0>)\n",
      "62:130 loss --> tensor([0.3221], grad_fn=<DivBackward0>)\n",
      "62:140 loss --> tensor([0.2379], grad_fn=<DivBackward0>)\n",
      "63:0 loss --> tensor([0.3731], grad_fn=<DivBackward0>)\n",
      "63:10 loss --> tensor([0.3776], grad_fn=<DivBackward0>)\n",
      "63:20 loss --> tensor([0.3585], grad_fn=<DivBackward0>)\n",
      "63:30 loss --> tensor([0.1885], grad_fn=<DivBackward0>)\n",
      "63:40 loss --> tensor([0.5387], grad_fn=<DivBackward0>)\n",
      "63:50 loss --> tensor([0.2644], grad_fn=<DivBackward0>)\n",
      "63:60 loss --> tensor([0.7096], grad_fn=<DivBackward0>)\n",
      "63:70 loss --> tensor([0.1696], grad_fn=<DivBackward0>)\n",
      "63:80 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "63:90 loss --> tensor([0.2151], grad_fn=<DivBackward0>)\n",
      "63:100 loss --> tensor([0.2178], grad_fn=<DivBackward0>)\n",
      "63:110 loss --> tensor([0.2516], grad_fn=<DivBackward0>)\n",
      "63:120 loss --> tensor([0.2470], grad_fn=<DivBackward0>)\n",
      "63:130 loss --> tensor([0.8904], grad_fn=<DivBackward0>)\n",
      "63:140 loss --> tensor([0.2378], grad_fn=<DivBackward0>)\n",
      "64:0 loss --> tensor([0.1719], grad_fn=<DivBackward0>)\n",
      "64:10 loss --> tensor([0.7317], grad_fn=<DivBackward0>)\n",
      "64:20 loss --> tensor([0.5398], grad_fn=<DivBackward0>)\n",
      "64:30 loss --> tensor([0.2442], grad_fn=<DivBackward0>)\n",
      "64:40 loss --> tensor([0.1300], grad_fn=<DivBackward0>)\n",
      "64:50 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "64:60 loss --> tensor([0.2767], grad_fn=<DivBackward0>)\n",
      "64:70 loss --> tensor([0.1390], grad_fn=<DivBackward0>)\n",
      "64:80 loss --> tensor([0.2683], grad_fn=<DivBackward0>)\n",
      "64:90 loss --> tensor([0.3036], grad_fn=<DivBackward0>)\n",
      "64:100 loss --> tensor([0.3564], grad_fn=<DivBackward0>)\n",
      "64:110 loss --> tensor([0.4248], grad_fn=<DivBackward0>)\n",
      "64:120 loss --> tensor([0.2656], grad_fn=<DivBackward0>)\n",
      "64:130 loss --> tensor([0.7259], grad_fn=<DivBackward0>)\n",
      "64:140 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "65:0 loss --> tensor([0.8729], grad_fn=<DivBackward0>)\n",
      "65:10 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "65:20 loss --> tensor([0.1666], grad_fn=<DivBackward0>)\n",
      "65:30 loss --> tensor([0.4864], grad_fn=<DivBackward0>)\n",
      "65:40 loss --> tensor([0.2346], grad_fn=<DivBackward0>)\n",
      "65:50 loss --> tensor([0.7711], grad_fn=<DivBackward0>)\n",
      "65:60 loss --> tensor([0.2281], grad_fn=<DivBackward0>)\n",
      "65:70 loss --> tensor([0.5715], grad_fn=<DivBackward0>)\n",
      "65:80 loss --> tensor([1.4477], grad_fn=<DivBackward0>)\n",
      "65:90 loss --> tensor([0.1623], grad_fn=<DivBackward0>)\n",
      "65:100 loss --> tensor([0.3099], grad_fn=<DivBackward0>)\n",
      "65:110 loss --> tensor([0.2257], grad_fn=<DivBackward0>)\n",
      "65:120 loss --> tensor([0.3226], grad_fn=<DivBackward0>)\n",
      "65:130 loss --> tensor([0.3414], grad_fn=<DivBackward0>)\n",
      "65:140 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:0 loss --> tensor([1.1191], grad_fn=<DivBackward0>)\n",
      "66:10 loss --> tensor([0.4810], grad_fn=<DivBackward0>)\n",
      "66:20 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "66:30 loss --> tensor([0.2133], grad_fn=<DivBackward0>)\n",
      "66:40 loss --> tensor([0.2722], grad_fn=<DivBackward0>)\n",
      "66:50 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:60 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "66:70 loss --> tensor([0.1718], grad_fn=<DivBackward0>)\n",
      "66:80 loss --> tensor([0.2414], grad_fn=<DivBackward0>)\n",
      "66:90 loss --> tensor([0.4626], grad_fn=<DivBackward0>)\n",
      "66:100 loss --> tensor([0.3433], grad_fn=<DivBackward0>)\n",
      "66:110 loss --> tensor([0.1731], grad_fn=<DivBackward0>)\n",
      "66:120 loss --> tensor([0.7181], grad_fn=<DivBackward0>)\n",
      "66:130 loss --> tensor([0.1520], grad_fn=<DivBackward0>)\n",
      "66:140 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "67:0 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "67:10 loss --> tensor([0.3112], grad_fn=<DivBackward0>)\n",
      "67:20 loss --> tensor([0.1441], grad_fn=<DivBackward0>)\n",
      "67:30 loss --> tensor([0.1785], grad_fn=<DivBackward0>)\n",
      "67:40 loss --> tensor([0.3715], grad_fn=<DivBackward0>)\n",
      "67:50 loss --> tensor([0.1290], grad_fn=<DivBackward0>)\n",
      "67:60 loss --> tensor([0.1705], grad_fn=<DivBackward0>)\n",
      "67:70 loss --> tensor([0.6034], grad_fn=<DivBackward0>)\n",
      "67:80 loss --> tensor([0.1160], grad_fn=<DivBackward0>)\n",
      "67:90 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "67:100 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "67:110 loss --> tensor([0.2879], grad_fn=<DivBackward0>)\n",
      "67:120 loss --> tensor([0.1026], grad_fn=<DivBackward0>)\n",
      "67:130 loss --> tensor([0.3092], grad_fn=<DivBackward0>)\n",
      "67:140 loss --> tensor([0.1229], grad_fn=<DivBackward0>)\n",
      "68:0 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "68:10 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "68:20 loss --> tensor([0.3287], grad_fn=<DivBackward0>)\n",
      "68:30 loss --> tensor([0.6754], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68:40 loss --> tensor([0.1940], grad_fn=<DivBackward0>)\n",
      "68:50 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "68:60 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "68:70 loss --> tensor([0.1858], grad_fn=<DivBackward0>)\n",
      "68:80 loss --> tensor([0.1167], grad_fn=<DivBackward0>)\n",
      "68:90 loss --> tensor([0.9398], grad_fn=<DivBackward0>)\n",
      "68:100 loss --> tensor([0.1030], grad_fn=<DivBackward0>)\n",
      "68:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "68:120 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "68:130 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "68:140 loss --> tensor([0.4271], grad_fn=<DivBackward0>)\n",
      "69:0 loss --> tensor([0.2381], grad_fn=<DivBackward0>)\n",
      "69:10 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "69:20 loss --> tensor([0.1660], grad_fn=<DivBackward0>)\n",
      "69:30 loss --> tensor([0.2088], grad_fn=<DivBackward0>)\n",
      "69:40 loss --> tensor([0.1419], grad_fn=<DivBackward0>)\n",
      "69:50 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:60 loss --> tensor([0.1773], grad_fn=<DivBackward0>)\n",
      "69:70 loss --> tensor([0.1231], grad_fn=<DivBackward0>)\n",
      "69:80 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "69:90 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:100 loss --> tensor([0.0877], grad_fn=<DivBackward0>)\n",
      "69:110 loss --> tensor([0.3021], grad_fn=<DivBackward0>)\n",
      "69:120 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "69:130 loss --> tensor([0.1192], grad_fn=<DivBackward0>)\n",
      "69:140 loss --> tensor([0.1689], grad_fn=<DivBackward0>)\n",
      "70:0 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "70:10 loss --> tensor([0.7175], grad_fn=<DivBackward0>)\n",
      "70:20 loss --> tensor([0.7906], grad_fn=<DivBackward0>)\n",
      "70:30 loss --> tensor([0.2661], grad_fn=<DivBackward0>)\n",
      "70:40 loss --> tensor([0.0900], grad_fn=<DivBackward0>)\n",
      "70:50 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "70:60 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "70:70 loss --> tensor([0.0991], grad_fn=<DivBackward0>)\n",
      "70:80 loss --> tensor([0.1057], grad_fn=<DivBackward0>)\n",
      "70:90 loss --> tensor([0.3856], grad_fn=<DivBackward0>)\n",
      "70:100 loss --> tensor([0.1463], grad_fn=<DivBackward0>)\n",
      "70:110 loss --> tensor([0.1318], grad_fn=<DivBackward0>)\n",
      "70:120 loss --> tensor([0.0892], grad_fn=<DivBackward0>)\n",
      "70:130 loss --> tensor([0.1156], grad_fn=<DivBackward0>)\n",
      "70:140 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "71:0 loss --> tensor([0.2592], grad_fn=<DivBackward0>)\n",
      "71:10 loss --> tensor([0.0816], grad_fn=<DivBackward0>)\n",
      "71:20 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "71:30 loss --> tensor([0.2759], grad_fn=<DivBackward0>)\n",
      "71:40 loss --> tensor([0.2751], grad_fn=<DivBackward0>)\n",
      "71:50 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "71:60 loss --> tensor([0.0835], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-91:\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7be988c99af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpreds_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, acts, labels, act_lens, label_lens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         return self.ctc(acts, labels, act_lens, label_lens, self.size_average,\n\u001b[0;32m---> 82\u001b[0;31m                         self.length_average, self.blank)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, acts, labels, act_lens, label_lens, size_average, length_average, blank)\u001b[0m\n\u001b[1;32m     30\u001b[0m                   \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                   blank)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/utils/ffi/__init__.py\u001b[0m in \u001b[0;36msafe_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m                      for arg in args)\n\u001b[1;32m    201\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtypeof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lib.data.char as c\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import lib.utils as utils\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "MOMENTUM = 0.9\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 字符转换编码\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "# 损失函数\n",
    "criterion = CTCLoss()\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     crnn.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer = optim.SGD(\n",
    "    crnn.parameters(), lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(t_image,t_label) in enumerate(train_loader):\n",
    "        batch_size = t_image.size(0)\n",
    "        utils.loadData(image, t_image)\n",
    "        t, l = converter.encode(t_label)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('{}:{} loss --> {}'.format(epoch, step, cost))\n",
    "            torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 42])\n",
      "torch.Size([10])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int32)\n",
      "tensor([49], dtype=torch.int32)\n",
      "tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([12.0142], grad_fn=<_CTCBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds_size = torch.IntTensor([49])\n",
    "# length = torch.IntTensor([2])\n",
    "print(preds.size())\n",
    "print(text.size())\n",
    "print(text)\n",
    "print(preds_size)\n",
    "print(length)\n",
    "\n",
    "criterion(preds, text, preds_size, length)\n",
    "\n",
    "# prob size --> torch.Size([2, 1, 5])\n",
    "# labels size --> torch.Size([2])\n",
    "# prob sizes -->tensor([2], dtype=torch.int32)\n",
    "# label sizes -->tensor([2], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.char as c\n",
    "import lib.utils as utils\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "_,(v_image,v_text) = next(enumerate(train_loader))\n",
    "utils.loadData(image,v_image)\n",
    "preds_s = crnn(image)\n",
    "batch_size = v_image.size(0)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)] * batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 49, 1441])\n",
      "tensor([ 349,    0,    0,    0,    0,    0,    0,    0,    0,    0,  350,  350,\n",
      "         351,  351,  352,  352,    0,    0,  353,  354,  354,  355,  355,    0,\n",
      "           0,    0,    0,    0,  356,  356,  356,    0,    0,    0,    0,  357,\n",
      "         357,  357,  358,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,  423,  424,    0,  425,  425,    0,    0,    0,  426,\n",
      "         426,    0,    0,    0,    0,  427,  427,  427,  427,  428,  429,  429,\n",
      "         429,    0,    0,    0,  430,  430,  430,    0,    0,  431,  431,  431,\n",
      "         432,  432,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,  507,  507,  507,    0,    0,    0,  508,  508,\n",
      "         508,  509,  509,    0,    0,    0,    0,  510,  510,  510,  511,  511,\n",
      "         512,  512,    0,    0,    0,    0,    0,  513,  513,  514,  514,  515,\n",
      "           0,    0,    0,    0,  516,  516,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,  840,  840,  841,\n",
      "         842,  842,  843,  843,  843,  844,  845,  845,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,  846,  846,  846,  847,  847,    0,    0,\n",
      "           0,    0,  848,  848,  849,  849,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   70,   71,   72,   72,    0,\n",
      "           0,    0,    0,    0,    0,   73,   73,   74,   74,    0,    0,   75,\n",
      "          75,   75,   75,    0,    0,    0,    0,    0,    0,   76,   76,   77,\n",
      "           0,    0,    0,   78,   79,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 1038, 1038, 1038,    0,    0,    0,    0,\n",
      "           0, 1039, 1040, 1040, 1041,    0,    0, 1042, 1042, 1042, 1043, 1043,\n",
      "           0,    0,    0,    0,    0,    0,    0, 1044, 1044, 1045, 1046, 1046,\n",
      "           0,    0,    0,    0,    0,    0, 1047, 1047,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  454,\n",
      "         454,  455,  455,  456,  457,  457,  458,  458,  458,    0,    0,    0,\n",
      "           0,    0,    0,    0,  459,  459,  460,  460,    0,    0,    0,    0,\n",
      "           0,    0,  461,  461,  461,  461,  462,  462,  463,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,  611,  611,  612,  612,    0,\n",
      "           0,    0,    0,    0,    0,  613,  614,  614,  615,  615,    0,    0,\n",
      "           0,    0,    0,  616,  616,    0,    0,    0,    0,  617,  618,  618,\n",
      "         619,  619,    0,    0,    0,    0,    0,  620,  620,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,  162,  162,  162,  163,\n",
      "         163,  163,    0,    0,    0,    0,    0,    0,  164,  164,  164,  165,\n",
      "         165,  166,  166,  166,  167,  167,  167,  167,  168,  168,  168,  168,\n",
      "         169,  169,  169,  170,  171,  171,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0, 1361, 1361, 1361, 1362, 1363, 1363, 1364, 1364,\n",
      "        1364,    0,    0,    0,    0,    0, 1365, 1365, 1366, 1367, 1368, 1368,\n",
      "        1368,    0,    0,    0,    0, 1369, 1369, 1370,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "preds = preds_s.clone()\n",
    "\n",
    "preds = preds.permute(1,0,2)\n",
    "print(preds.size())\n",
    "_,preds = preds.max(2)\n",
    "preds = preds.view(-1)\n",
    "# print(preds)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)])) * batchSize\n",
    "sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "\n",
    "print(sim_preds)\n",
    "image_0 = v_image[0][0]\n",
    "image_0 = image_0.numpy()\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

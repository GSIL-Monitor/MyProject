{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "\n",
    "# https://github.com/BelBES/crnn-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\n",
      "Written 1000 / 1438\n",
      "Created dataset with 1438 samples\n"
     ]
    }
   ],
   "source": [
    "import lib.data.gen_data as gd\n",
    "import lib.data.dataset as ds\n",
    "import lib.data.char as char\n",
    "import importlib\n",
    "importlib.reload(gd)\n",
    "importlib.reload(char)\n",
    "\n",
    "# 生成文字图片\n",
    "gd.create_data(1000)\n",
    "\n",
    "# 根据生成文字图片，保存到lmdb\n",
    "ds.main_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.lmdb_dataset as lds\n",
    "import torch\n",
    "train_path = '/home/hecong/temp/data/ocr/lmdb'\n",
    "batchSize = 10\n",
    "sampler = None\n",
    "workers = 1\n",
    "imgH = 32\n",
    "imgW = 256\n",
    "\n",
    "train_dataset = lds.lmdbDataset(root=train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "    sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=lds.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('洁洪洒浇浊洞测洗活派', '狡狱狠贸怨急饶蚀饺饼', '羽观欢买红纤级约纪驰', '夕丸么广亡门义之尸弓', '勉狭狮独狡狱狠贸怨急')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "_,(image,label) = next(enumerate(train_loader))\n",
    "image_0 = image[0][0]\n",
    "image_0 = image_0.numpy()\n",
    "print(label)\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def data_parallel(model, input, ngpu):\n",
    "    if ngpu > 1 and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "        output = nn.parallel.data_parallel(model, input, range(ngpu))\n",
    "    else:\n",
    "        output = model(input)\n",
    "    return output\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut, ngpu):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = data_parallel(self.rnn, input,\n",
    "                                           self.ngpu)  # [T, b, h * 2]\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = data_parallel(self.embedding, \n",
    "                               t_rec,self.ngpu)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, ngpu, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "                                                                    \n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        # pool = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
    "        # 注意MaxPool2d 当stride = (2,1),表示只会根据H轴方向进行Pool，因为W方向是1。\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh, ngpu),\n",
    "            BidirectionalLSTM(nh, nh, nclass, ngpu))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "#         print('input size --> {}'.format(input.size()))\n",
    "        conv = data_parallel(self.cnn, input, self.ngpu)\n",
    "        b, c, h, w = conv.size()\n",
    "#         print('conv out size --> {}'.format(conv.size()))\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        \n",
    "        # rnn features\n",
    "        output = data_parallel(self.rnn, conv, self.ngpu)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 loss --> tensor([53.5975], grad_fn=<DivBackward0>)\n",
      "0:10 loss --> tensor([57.0985], grad_fn=<DivBackward0>)\n",
      "0:20 loss --> tensor([50.0112], grad_fn=<DivBackward0>)\n",
      "0:30 loss --> tensor([52.5952], grad_fn=<DivBackward0>)\n",
      "0:40 loss --> tensor([55.3976], grad_fn=<DivBackward0>)\n",
      "0:50 loss --> tensor([51.6368], grad_fn=<DivBackward0>)\n",
      "0:60 loss --> tensor([47.6827], grad_fn=<DivBackward0>)\n",
      "0:70 loss --> tensor([44.2092], grad_fn=<DivBackward0>)\n",
      "0:80 loss --> tensor([39.8187], grad_fn=<DivBackward0>)\n",
      "0:90 loss --> tensor([47.0806], grad_fn=<DivBackward0>)\n",
      "0:100 loss --> tensor([45.2231], grad_fn=<DivBackward0>)\n",
      "0:110 loss --> tensor([42.7563], grad_fn=<DivBackward0>)\n",
      "0:120 loss --> tensor([64.4479], grad_fn=<DivBackward0>)\n",
      "0:130 loss --> tensor([43.9465], grad_fn=<DivBackward0>)\n",
      "0:140 loss --> tensor([42.6062], grad_fn=<DivBackward0>)\n",
      "1:0 loss --> tensor([44.9383], grad_fn=<DivBackward0>)\n",
      "1:10 loss --> tensor([42.0999], grad_fn=<DivBackward0>)\n",
      "1:20 loss --> tensor([43.7860], grad_fn=<DivBackward0>)\n",
      "1:30 loss --> tensor([38.2796], grad_fn=<DivBackward0>)\n",
      "1:40 loss --> tensor([35.5139], grad_fn=<DivBackward0>)\n",
      "1:50 loss --> tensor([53.6258], grad_fn=<DivBackward0>)\n",
      "1:60 loss --> tensor([43.0368], grad_fn=<DivBackward0>)\n",
      "1:70 loss --> tensor([40.1389], grad_fn=<DivBackward0>)\n",
      "1:80 loss --> tensor([41.3033], grad_fn=<DivBackward0>)\n",
      "1:90 loss --> tensor([38.0543], grad_fn=<DivBackward0>)\n",
      "1:100 loss --> tensor([41.2383], grad_fn=<DivBackward0>)\n",
      "1:110 loss --> tensor([35.5826], grad_fn=<DivBackward0>)\n",
      "1:120 loss --> tensor([40.3198], grad_fn=<DivBackward0>)\n",
      "1:130 loss --> tensor([34.8085], grad_fn=<DivBackward0>)\n",
      "1:140 loss --> tensor([39.6848], grad_fn=<DivBackward0>)\n",
      "2:0 loss --> tensor([43.7701], grad_fn=<DivBackward0>)\n",
      "2:10 loss --> tensor([32.8932], grad_fn=<DivBackward0>)\n",
      "2:20 loss --> tensor([41.3389], grad_fn=<DivBackward0>)\n",
      "2:30 loss --> tensor([37.9055], grad_fn=<DivBackward0>)\n",
      "2:40 loss --> tensor([32.3746], grad_fn=<DivBackward0>)\n",
      "2:50 loss --> tensor([42.3556], grad_fn=<DivBackward0>)\n",
      "2:60 loss --> tensor([39.1594], grad_fn=<DivBackward0>)\n",
      "2:70 loss --> tensor([30.9823], grad_fn=<DivBackward0>)\n",
      "2:80 loss --> tensor([32.9804], grad_fn=<DivBackward0>)\n",
      "2:90 loss --> tensor([39.3804], grad_fn=<DivBackward0>)\n",
      "2:100 loss --> tensor([42.3663], grad_fn=<DivBackward0>)\n",
      "2:110 loss --> tensor([36.8905], grad_fn=<DivBackward0>)\n",
      "2:120 loss --> tensor([34.9654], grad_fn=<DivBackward0>)\n",
      "2:130 loss --> tensor([26.6693], grad_fn=<DivBackward0>)\n",
      "2:140 loss --> tensor([38.7210], grad_fn=<DivBackward0>)\n",
      "3:0 loss --> tensor([30.7170], grad_fn=<DivBackward0>)\n",
      "3:10 loss --> tensor([27.0392], grad_fn=<DivBackward0>)\n",
      "3:20 loss --> tensor([24.6356], grad_fn=<DivBackward0>)\n",
      "3:30 loss --> tensor([38.3049], grad_fn=<DivBackward0>)\n",
      "3:40 loss --> tensor([30.6526], grad_fn=<DivBackward0>)\n",
      "3:50 loss --> tensor([24.1882], grad_fn=<DivBackward0>)\n",
      "3:60 loss --> tensor([29.0762], grad_fn=<DivBackward0>)\n",
      "3:70 loss --> tensor([28.5791], grad_fn=<DivBackward0>)\n",
      "3:80 loss --> tensor([28.7481], grad_fn=<DivBackward0>)\n",
      "3:90 loss --> tensor([26.2437], grad_fn=<DivBackward0>)\n",
      "3:100 loss --> tensor([33.4534], grad_fn=<DivBackward0>)\n",
      "3:110 loss --> tensor([29.8826], grad_fn=<DivBackward0>)\n",
      "3:120 loss --> tensor([39.5565], grad_fn=<DivBackward0>)\n",
      "3:130 loss --> tensor([25.1397], grad_fn=<DivBackward0>)\n",
      "3:140 loss --> tensor([21.5763], grad_fn=<DivBackward0>)\n",
      "4:0 loss --> tensor([22.5060], grad_fn=<DivBackward0>)\n",
      "4:10 loss --> tensor([26.6279], grad_fn=<DivBackward0>)\n",
      "4:20 loss --> tensor([23.3890], grad_fn=<DivBackward0>)\n",
      "4:30 loss --> tensor([23.0096], grad_fn=<DivBackward0>)\n",
      "4:40 loss --> tensor([35.1095], grad_fn=<DivBackward0>)\n",
      "4:50 loss --> tensor([24.9968], grad_fn=<DivBackward0>)\n",
      "4:60 loss --> tensor([25.5238], grad_fn=<DivBackward0>)\n",
      "4:70 loss --> tensor([19.8257], grad_fn=<DivBackward0>)\n",
      "4:80 loss --> tensor([27.3376], grad_fn=<DivBackward0>)\n",
      "4:90 loss --> tensor([22.9874], grad_fn=<DivBackward0>)\n",
      "4:100 loss --> tensor([26.7430], grad_fn=<DivBackward0>)\n",
      "4:110 loss --> tensor([20.2543], grad_fn=<DivBackward0>)\n",
      "4:120 loss --> tensor([17.1577], grad_fn=<DivBackward0>)\n",
      "4:130 loss --> tensor([23.4858], grad_fn=<DivBackward0>)\n",
      "4:140 loss --> tensor([18.1587], grad_fn=<DivBackward0>)\n",
      "5:0 loss --> tensor([17.3480], grad_fn=<DivBackward0>)\n",
      "5:10 loss --> tensor([21.2981], grad_fn=<DivBackward0>)\n",
      "5:20 loss --> tensor([34.3481], grad_fn=<DivBackward0>)\n",
      "5:30 loss --> tensor([27.9732], grad_fn=<DivBackward0>)\n",
      "5:40 loss --> tensor([19.2680], grad_fn=<DivBackward0>)\n",
      "5:50 loss --> tensor([24.5330], grad_fn=<DivBackward0>)\n",
      "5:60 loss --> tensor([19.1145], grad_fn=<DivBackward0>)\n",
      "5:70 loss --> tensor([19.9866], grad_fn=<DivBackward0>)\n",
      "5:80 loss --> tensor([38.6709], grad_fn=<DivBackward0>)\n",
      "5:90 loss --> tensor([22.2117], grad_fn=<DivBackward0>)\n",
      "5:100 loss --> tensor([18.2306], grad_fn=<DivBackward0>)\n",
      "5:110 loss --> tensor([23.8929], grad_fn=<DivBackward0>)\n",
      "5:120 loss --> tensor([21.4290], grad_fn=<DivBackward0>)\n",
      "5:130 loss --> tensor([20.5149], grad_fn=<DivBackward0>)\n",
      "5:140 loss --> tensor([17.8162], grad_fn=<DivBackward0>)\n",
      "6:0 loss --> tensor([14.1739], grad_fn=<DivBackward0>)\n",
      "6:10 loss --> tensor([17.3465], grad_fn=<DivBackward0>)\n",
      "6:20 loss --> tensor([20.6543], grad_fn=<DivBackward0>)\n",
      "6:30 loss --> tensor([31.6036], grad_fn=<DivBackward0>)\n",
      "6:40 loss --> tensor([13.7372], grad_fn=<DivBackward0>)\n",
      "6:50 loss --> tensor([24.7455], grad_fn=<DivBackward0>)\n",
      "6:60 loss --> tensor([17.5986], grad_fn=<DivBackward0>)\n",
      "6:70 loss --> tensor([27.2095], grad_fn=<DivBackward0>)\n",
      "6:80 loss --> tensor([15.9244], grad_fn=<DivBackward0>)\n",
      "6:90 loss --> tensor([12.1201], grad_fn=<DivBackward0>)\n",
      "6:100 loss --> tensor([12.9672], grad_fn=<DivBackward0>)\n",
      "6:110 loss --> tensor([10.8590], grad_fn=<DivBackward0>)\n",
      "6:120 loss --> tensor([13.2917], grad_fn=<DivBackward0>)\n",
      "6:130 loss --> tensor([12.4567], grad_fn=<DivBackward0>)\n",
      "6:140 loss --> tensor([16.9746], grad_fn=<DivBackward0>)\n",
      "7:0 loss --> tensor([8.0265], grad_fn=<DivBackward0>)\n",
      "7:10 loss --> tensor([8.5661], grad_fn=<DivBackward0>)\n",
      "7:20 loss --> tensor([10.0561], grad_fn=<DivBackward0>)\n",
      "7:30 loss --> tensor([13.5132], grad_fn=<DivBackward0>)\n",
      "7:40 loss --> tensor([11.4051], grad_fn=<DivBackward0>)\n",
      "7:50 loss --> tensor([8.9594], grad_fn=<DivBackward0>)\n",
      "7:60 loss --> tensor([19.0482], grad_fn=<DivBackward0>)\n",
      "7:70 loss --> tensor([11.9351], grad_fn=<DivBackward0>)\n",
      "7:80 loss --> tensor([7.9520], grad_fn=<DivBackward0>)\n",
      "7:90 loss --> tensor([10.0523], grad_fn=<DivBackward0>)\n",
      "7:100 loss --> tensor([10.2835], grad_fn=<DivBackward0>)\n",
      "7:110 loss --> tensor([9.9927], grad_fn=<DivBackward0>)\n",
      "7:120 loss --> tensor([12.1256], grad_fn=<DivBackward0>)\n",
      "7:130 loss --> tensor([9.6194], grad_fn=<DivBackward0>)\n",
      "7:140 loss --> tensor([11.7198], grad_fn=<DivBackward0>)\n",
      "8:0 loss --> tensor([9.6944], grad_fn=<DivBackward0>)\n",
      "8:10 loss --> tensor([10.8409], grad_fn=<DivBackward0>)\n",
      "8:20 loss --> tensor([7.9625], grad_fn=<DivBackward0>)\n",
      "8:30 loss --> tensor([11.9340], grad_fn=<DivBackward0>)\n",
      "8:40 loss --> tensor([9.1434], grad_fn=<DivBackward0>)\n",
      "8:50 loss --> tensor([9.3728], grad_fn=<DivBackward0>)\n",
      "8:60 loss --> tensor([11.1217], grad_fn=<DivBackward0>)\n",
      "8:70 loss --> tensor([14.3963], grad_fn=<DivBackward0>)\n",
      "8:80 loss --> tensor([14.3312], grad_fn=<DivBackward0>)\n",
      "8:90 loss --> tensor([12.4508], grad_fn=<DivBackward0>)\n",
      "8:100 loss --> tensor([15.6014], grad_fn=<DivBackward0>)\n",
      "8:110 loss --> tensor([7.0955], grad_fn=<DivBackward0>)\n",
      "8:120 loss --> tensor([6.3157], grad_fn=<DivBackward0>)\n",
      "8:130 loss --> tensor([11.2243], grad_fn=<DivBackward0>)\n",
      "8:140 loss --> tensor([8.6178], grad_fn=<DivBackward0>)\n",
      "9:0 loss --> tensor([6.5377], grad_fn=<DivBackward0>)\n",
      "9:10 loss --> tensor([12.6795], grad_fn=<DivBackward0>)\n",
      "9:20 loss --> tensor([6.7761], grad_fn=<DivBackward0>)\n",
      "9:30 loss --> tensor([9.0038], grad_fn=<DivBackward0>)\n",
      "9:40 loss --> tensor([6.1930], grad_fn=<DivBackward0>)\n",
      "9:50 loss --> tensor([8.9495], grad_fn=<DivBackward0>)\n",
      "9:60 loss --> tensor([4.8571], grad_fn=<DivBackward0>)\n",
      "9:70 loss --> tensor([24.1581], grad_fn=<DivBackward0>)\n",
      "9:80 loss --> tensor([8.5825], grad_fn=<DivBackward0>)\n",
      "9:90 loss --> tensor([7.4008], grad_fn=<DivBackward0>)\n",
      "9:100 loss --> tensor([8.5395], grad_fn=<DivBackward0>)\n",
      "9:110 loss --> tensor([12.3562], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:120 loss --> tensor([5.2874], grad_fn=<DivBackward0>)\n",
      "9:130 loss --> tensor([10.8011], grad_fn=<DivBackward0>)\n",
      "9:140 loss --> tensor([7.2340], grad_fn=<DivBackward0>)\n",
      "10:0 loss --> tensor([8.4660], grad_fn=<DivBackward0>)\n",
      "10:10 loss --> tensor([6.0446], grad_fn=<DivBackward0>)\n",
      "10:20 loss --> tensor([4.4115], grad_fn=<DivBackward0>)\n",
      "10:30 loss --> tensor([4.9802], grad_fn=<DivBackward0>)\n",
      "10:40 loss --> tensor([6.1529], grad_fn=<DivBackward0>)\n",
      "10:50 loss --> tensor([3.6870], grad_fn=<DivBackward0>)\n",
      "10:60 loss --> tensor([6.3238], grad_fn=<DivBackward0>)\n",
      "10:70 loss --> tensor([3.9593], grad_fn=<DivBackward0>)\n",
      "10:80 loss --> tensor([7.0191], grad_fn=<DivBackward0>)\n",
      "10:90 loss --> tensor([9.9550], grad_fn=<DivBackward0>)\n",
      "10:100 loss --> tensor([3.3470], grad_fn=<DivBackward0>)\n",
      "10:110 loss --> tensor([7.5360], grad_fn=<DivBackward0>)\n",
      "10:120 loss --> tensor([9.0215], grad_fn=<DivBackward0>)\n",
      "10:130 loss --> tensor([7.3390], grad_fn=<DivBackward0>)\n",
      "10:140 loss --> tensor([5.1061], grad_fn=<DivBackward0>)\n",
      "11:0 loss --> tensor([4.3497], grad_fn=<DivBackward0>)\n",
      "11:10 loss --> tensor([5.4116], grad_fn=<DivBackward0>)\n",
      "11:20 loss --> tensor([11.0163], grad_fn=<DivBackward0>)\n",
      "11:30 loss --> tensor([3.6124], grad_fn=<DivBackward0>)\n",
      "11:40 loss --> tensor([4.8195], grad_fn=<DivBackward0>)\n",
      "11:50 loss --> tensor([5.7265], grad_fn=<DivBackward0>)\n",
      "11:60 loss --> tensor([4.4949], grad_fn=<DivBackward0>)\n",
      "11:70 loss --> tensor([5.5938], grad_fn=<DivBackward0>)\n",
      "11:80 loss --> tensor([7.6906], grad_fn=<DivBackward0>)\n",
      "11:90 loss --> tensor([3.8238], grad_fn=<DivBackward0>)\n",
      "11:100 loss --> tensor([4.9280], grad_fn=<DivBackward0>)\n",
      "11:110 loss --> tensor([3.8688], grad_fn=<DivBackward0>)\n",
      "11:120 loss --> tensor([3.3642], grad_fn=<DivBackward0>)\n",
      "11:130 loss --> tensor([5.1959], grad_fn=<DivBackward0>)\n",
      "11:140 loss --> tensor([5.3433], grad_fn=<DivBackward0>)\n",
      "12:0 loss --> tensor([3.0339], grad_fn=<DivBackward0>)\n",
      "12:10 loss --> tensor([3.9346], grad_fn=<DivBackward0>)\n",
      "12:20 loss --> tensor([5.5083], grad_fn=<DivBackward0>)\n",
      "12:30 loss --> tensor([4.2467], grad_fn=<DivBackward0>)\n",
      "12:40 loss --> tensor([9.7782], grad_fn=<DivBackward0>)\n",
      "12:50 loss --> tensor([3.4957], grad_fn=<DivBackward0>)\n",
      "12:60 loss --> tensor([4.8109], grad_fn=<DivBackward0>)\n",
      "12:70 loss --> tensor([3.6909], grad_fn=<DivBackward0>)\n",
      "12:80 loss --> tensor([3.3436], grad_fn=<DivBackward0>)\n",
      "12:90 loss --> tensor([8.6289], grad_fn=<DivBackward0>)\n",
      "12:100 loss --> tensor([4.6796], grad_fn=<DivBackward0>)\n",
      "12:110 loss --> tensor([7.5757], grad_fn=<DivBackward0>)\n",
      "12:120 loss --> tensor([4.0483], grad_fn=<DivBackward0>)\n",
      "12:130 loss --> tensor([5.6045], grad_fn=<DivBackward0>)\n",
      "12:140 loss --> tensor([8.9995], grad_fn=<DivBackward0>)\n",
      "13:0 loss --> tensor([19.2985], grad_fn=<DivBackward0>)\n",
      "13:10 loss --> tensor([4.5899], grad_fn=<DivBackward0>)\n",
      "13:20 loss --> tensor([5.9778], grad_fn=<DivBackward0>)\n",
      "13:30 loss --> tensor([5.8460], grad_fn=<DivBackward0>)\n",
      "13:40 loss --> tensor([2.4626], grad_fn=<DivBackward0>)\n",
      "13:50 loss --> tensor([4.2832], grad_fn=<DivBackward0>)\n",
      "13:60 loss --> tensor([3.2530], grad_fn=<DivBackward0>)\n",
      "13:70 loss --> tensor([5.1806], grad_fn=<DivBackward0>)\n",
      "13:80 loss --> tensor([2.5861], grad_fn=<DivBackward0>)\n",
      "13:90 loss --> tensor([3.0346], grad_fn=<DivBackward0>)\n",
      "13:100 loss --> tensor([4.4632], grad_fn=<DivBackward0>)\n",
      "13:110 loss --> tensor([3.1596], grad_fn=<DivBackward0>)\n",
      "13:120 loss --> tensor([1.3999], grad_fn=<DivBackward0>)\n",
      "13:130 loss --> tensor([3.0979], grad_fn=<DivBackward0>)\n",
      "13:140 loss --> tensor([3.1437], grad_fn=<DivBackward0>)\n",
      "14:0 loss --> tensor([4.4563], grad_fn=<DivBackward0>)\n",
      "14:10 loss --> tensor([2.9229], grad_fn=<DivBackward0>)\n",
      "14:20 loss --> tensor([1.1676], grad_fn=<DivBackward0>)\n",
      "14:30 loss --> tensor([2.9975], grad_fn=<DivBackward0>)\n",
      "14:40 loss --> tensor([3.2824], grad_fn=<DivBackward0>)\n",
      "14:50 loss --> tensor([2.6737], grad_fn=<DivBackward0>)\n",
      "14:60 loss --> tensor([2.2830], grad_fn=<DivBackward0>)\n",
      "14:70 loss --> tensor([2.0771], grad_fn=<DivBackward0>)\n",
      "14:80 loss --> tensor([2.6285], grad_fn=<DivBackward0>)\n",
      "14:90 loss --> tensor([2.6516], grad_fn=<DivBackward0>)\n",
      "14:100 loss --> tensor([1.9382], grad_fn=<DivBackward0>)\n",
      "14:110 loss --> tensor([3.3012], grad_fn=<DivBackward0>)\n",
      "14:120 loss --> tensor([1.6376], grad_fn=<DivBackward0>)\n",
      "14:130 loss --> tensor([2.8184], grad_fn=<DivBackward0>)\n",
      "14:140 loss --> tensor([1.5894], grad_fn=<DivBackward0>)\n",
      "15:0 loss --> tensor([1.8524], grad_fn=<DivBackward0>)\n",
      "15:10 loss --> tensor([2.1698], grad_fn=<DivBackward0>)\n",
      "15:20 loss --> tensor([1.6925], grad_fn=<DivBackward0>)\n",
      "15:30 loss --> tensor([2.0742], grad_fn=<DivBackward0>)\n",
      "15:40 loss --> tensor([1.2067], grad_fn=<DivBackward0>)\n",
      "15:50 loss --> tensor([2.6200], grad_fn=<DivBackward0>)\n",
      "15:60 loss --> tensor([2.2156], grad_fn=<DivBackward0>)\n",
      "15:70 loss --> tensor([1.2239], grad_fn=<DivBackward0>)\n",
      "15:80 loss --> tensor([2.2353], grad_fn=<DivBackward0>)\n",
      "15:90 loss --> tensor([1.3490], grad_fn=<DivBackward0>)\n",
      "15:100 loss --> tensor([2.3179], grad_fn=<DivBackward0>)\n",
      "15:110 loss --> tensor([20.0866], grad_fn=<DivBackward0>)\n",
      "15:120 loss --> tensor([5.7045], grad_fn=<DivBackward0>)\n",
      "15:130 loss --> tensor([6.6904], grad_fn=<DivBackward0>)\n",
      "15:140 loss --> tensor([5.1944], grad_fn=<DivBackward0>)\n",
      "16:0 loss --> tensor([4.2282], grad_fn=<DivBackward0>)\n",
      "16:10 loss --> tensor([2.5643], grad_fn=<DivBackward0>)\n",
      "16:20 loss --> tensor([9.7895], grad_fn=<DivBackward0>)\n",
      "16:30 loss --> tensor([2.6813], grad_fn=<DivBackward0>)\n",
      "16:40 loss --> tensor([2.1947], grad_fn=<DivBackward0>)\n",
      "16:50 loss --> tensor([1.7382], grad_fn=<DivBackward0>)\n",
      "16:60 loss --> tensor([2.2201], grad_fn=<DivBackward0>)\n",
      "16:70 loss --> tensor([4.7990], grad_fn=<DivBackward0>)\n",
      "16:80 loss --> tensor([4.0251], grad_fn=<DivBackward0>)\n",
      "16:90 loss --> tensor([2.1778], grad_fn=<DivBackward0>)\n",
      "16:100 loss --> tensor([2.6760], grad_fn=<DivBackward0>)\n",
      "16:110 loss --> tensor([2.0234], grad_fn=<DivBackward0>)\n",
      "16:120 loss --> tensor([1.6357], grad_fn=<DivBackward0>)\n",
      "16:130 loss --> tensor([2.9600], grad_fn=<DivBackward0>)\n",
      "16:140 loss --> tensor([2.7500], grad_fn=<DivBackward0>)\n",
      "17:0 loss --> tensor([2.4946], grad_fn=<DivBackward0>)\n",
      "17:10 loss --> tensor([2.4937], grad_fn=<DivBackward0>)\n",
      "17:20 loss --> tensor([3.4909], grad_fn=<DivBackward0>)\n",
      "17:30 loss --> tensor([2.7018], grad_fn=<DivBackward0>)\n",
      "17:40 loss --> tensor([1.8897], grad_fn=<DivBackward0>)\n",
      "17:50 loss --> tensor([39.6618], grad_fn=<DivBackward0>)\n",
      "17:60 loss --> tensor([2.8666], grad_fn=<DivBackward0>)\n",
      "17:70 loss --> tensor([1.9321], grad_fn=<DivBackward0>)\n",
      "17:80 loss --> tensor([3.9888], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f3871089e48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 17762) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-579a82e1aaec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lib.data.char as c\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import lib.utils as utils\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "MOMENTUM = 0.9\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 字符转换编码\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "# 损失函数\n",
    "criterion = CTCLoss()\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     crnn.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer = optim.SGD(\n",
    "    crnn.parameters(), lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(t_image,t_label) in enumerate(train_loader):\n",
    "        batch_size = t_image.size(0)\n",
    "        utils.loadData(image, t_image)\n",
    "        t, l = converter.encode(t_label)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('{}:{} loss --> {}'.format(epoch, step, cost))\n",
    "            torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 42])\n",
      "torch.Size([10])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int32)\n",
      "tensor([49], dtype=torch.int32)\n",
      "tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([12.0142], grad_fn=<_CTCBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds_size = torch.IntTensor([49])\n",
    "# length = torch.IntTensor([2])\n",
    "print(preds.size())\n",
    "print(text.size())\n",
    "print(text)\n",
    "print(preds_size)\n",
    "print(length)\n",
    "\n",
    "criterion(preds, text, preds_size, length)\n",
    "\n",
    "# prob size --> torch.Size([2, 1, 5])\n",
    "# labels size --> torch.Size([2])\n",
    "# prob sizes -->tensor([2], dtype=torch.int32)\n",
    "# label sizes -->tensor([2], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.char as c\n",
    "import lib.utils as utils\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "_,(v_image,v_text) = next(enumerate(train_loader))\n",
    "utils.loadData(image,v_image)\n",
    "preds_s = crnn(image)\n",
    "batch_size = v_image.size(0)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)] * batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 49, 1441])\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,  919,  919,  920,\n",
      "         920,  921,  921,  922,  922,    0,    0,    0,    0,  923,  923,  923,\n",
      "         923,  924,  924,  924,  925,  926,  927,    0,    0,    0,    0,  928,\n",
      "         928,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,  518,  519,  520,  520,\n",
      "           0,    0,  521,  521,  521,  521,    0,    0,    0,  522,  522,  522,\n",
      "         522,    0,    0,    0,  523,  523,    0,    0,  524,  524,  524,  525,\n",
      "         525,  526,  526,  527,  527,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,  466,    0,    0,    0,  467,  468,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,  469,  469,  470,  471,    0,\n",
      "           0,    0,    0,    0,    0,    0,  472,  472,  473,    0,    0,    0,\n",
      "         474,  474,  475,  475,  475,  475,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0, 1137, 1138, 1138,    0,    0, 1139, 1140, 1141,    0,\n",
      "           0,    0,    0,    0,    0, 1142, 1142, 1142,    0,    0,    0,    0,\n",
      "           0, 1143, 1144, 1145,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0, 1142, 1142,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0, 1143, 1143, 1144, 1145,\n",
      "        1146, 1147, 1147, 1148, 1148, 1149, 1149,    0,    0,    0,    0, 1150,\n",
      "        1150, 1151, 1151,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0, 1313,    0,    0,    0,    0,\n",
      "        1314, 1314, 1314, 1315, 1315,    0,    0,    0,    0,    0,    0,    0,\n",
      "        1316, 1316, 1316, 1317,    0,    0, 1318, 1318, 1319, 1319, 1320, 1321,\n",
      "        1321, 1321,    0,    0,    0, 1322, 1322, 1322,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,  447,  447,    0,\n",
      "           0,    0,    0,    0,    0,  448,  448,  449,  449,  450,    0,    0,\n",
      "           0,  451,  451,  452,  452,    0,  453,  453,  453,  453,  453,    0,\n",
      "           0,  454,  454,  454,  454,  455,  456,  456,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0, 1301, 1301,    0,\n",
      "        1302, 1303, 1303, 1304,    0,    0,    0, 1305, 1305, 1305, 1305, 1305,\n",
      "           0,    0,    0, 1306, 1306, 1307, 1307, 1308, 1308,    0,    0,    0,\n",
      "           0, 1309, 1309, 1309,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  716,  382,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  383,\n",
      "         384,  384,  384,    0,    0,    0,    0,    0,    0,    0,  385,  385,\n",
      "         386,  386,  387,  387,  388,  389,  389,  390,  390,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0, 1048, 1049, 1049, 1050, 1050,    0,    0,    0,    0, 1051, 1051,\n",
      "        1052, 1052, 1052, 1052, 1052, 1053, 1053, 1053, 1053,    0,    0,    0,\n",
      "           0, 1054, 1055, 1055,    0,    0, 1056, 1056, 1057, 1057,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "供使例版侄侦侧凭侨佩观欢买红纤级约纪驰巡充妄闭问闯羊并关米灯柄栋相查柏柳柱柿栏柳柱柿栏树要咸威歪研逆总炼炸炮烂剃洁洪洒负各名多争色壮冲冰庄阀阁差养美姜叛送类忘吊吃因吸吗屿帆岁回该详建肃录隶居届刷屈\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "preds = preds_s.clone()\n",
    "\n",
    "preds = preds.permute(1,0,2)\n",
    "print(preds.size())\n",
    "_,preds = preds.max(2)\n",
    "preds = preds.view(-1)\n",
    "print(preds)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)])) * batchSize\n",
    "sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "\n",
    "print(sim_preds)\n",
    "image_0 = v_image[1][0]\n",
    "image_0 = image_0.numpy()\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "\n",
    "# https://github.com/BelBES/crnn-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\n",
      "Written 1000 / 1438\n",
      "Created dataset with 1438 samples\n"
     ]
    }
   ],
   "source": [
    "import lib.data.gen_data as gd\n",
    "import lib.data.dataset as ds\n",
    "import lib.data.char as char\n",
    "import importlib\n",
    "importlib.reload(gd)\n",
    "importlib.reload(char)\n",
    "\n",
    "# 生成文字图片\n",
    "gd.create_data(1000)\n",
    "\n",
    "# 根据生成文字图片，保存到lmdb\n",
    "ds.main_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.lmdb_dataset as lds\n",
    "import torch\n",
    "train_path = '/home/hecong/temp/data/ocr/lmdb'\n",
    "batchSize = 10\n",
    "sampler = None\n",
    "workers = 1\n",
    "imgH = 32\n",
    "imgW = 256\n",
    "\n",
    "train_dataset = lds.lmdbDataset(root=train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "    sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=lds.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('洁洪洒浇浊洞测洗活派', '狡狱狠贸怨急饶蚀饺饼', '羽观欢买红纤级约纪驰', '夕丸么广亡门义之尸弓', '勉狭狮独狡狱狠贸怨急')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "_,(image,label) = next(enumerate(train_loader))\n",
    "image_0 = image[0][0]\n",
    "image_0 = image_0.numpy()\n",
    "print(label)\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def data_parallel(model, input, ngpu):\n",
    "    if ngpu > 1 and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "        output = nn.parallel.data_parallel(model, input, range(ngpu))\n",
    "    else:\n",
    "        output = model(input)\n",
    "    return output\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut, ngpu):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = data_parallel(self.rnn, input,\n",
    "                                           self.ngpu)  # [T, b, h * 2]\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = data_parallel(self.embedding, \n",
    "                               t_rec,self.ngpu)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, ngpu, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "                                                                    \n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        # pool = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
    "        # 注意MaxPool2d 当stride = (2,1),表示只会根据H轴方向进行Pool，因为W方向是1。\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh, ngpu),\n",
    "            BidirectionalLSTM(nh, nh, nclass, ngpu))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "#         print('input size --> {}'.format(input.size()))\n",
    "        conv = data_parallel(self.cnn, input, self.ngpu)\n",
    "        b, c, h, w = conv.size()\n",
    "#         print('conv out size --> {}'.format(conv.size()))\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        \n",
    "        # rnn features\n",
    "        output = data_parallel(self.rnn, conv, self.ngpu)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 loss --> tensor([2.0093], grad_fn=<DivBackward0>)\n",
      "0:10 loss --> tensor([50.4357], grad_fn=<DivBackward0>)\n",
      "0:20 loss --> tensor([32.7192], grad_fn=<DivBackward0>)\n",
      "0:30 loss --> tensor([29.6999], grad_fn=<DivBackward0>)\n",
      "0:40 loss --> tensor([30.8185], grad_fn=<DivBackward0>)\n",
      "0:50 loss --> tensor([17.9923], grad_fn=<DivBackward0>)\n",
      "0:60 loss --> tensor([21.9073], grad_fn=<DivBackward0>)\n",
      "0:70 loss --> tensor([19.4085], grad_fn=<DivBackward0>)\n",
      "0:80 loss --> tensor([14.9581], grad_fn=<DivBackward0>)\n",
      "0:90 loss --> tensor([10.8917], grad_fn=<DivBackward0>)\n",
      "0:100 loss --> tensor([17.9379], grad_fn=<DivBackward0>)\n",
      "0:110 loss --> tensor([29.3240], grad_fn=<DivBackward0>)\n",
      "0:120 loss --> tensor([28.2609], grad_fn=<DivBackward0>)\n",
      "0:130 loss --> tensor([18.1639], grad_fn=<DivBackward0>)\n",
      "0:140 loss --> tensor([18.5935], grad_fn=<DivBackward0>)\n",
      "1:0 loss --> tensor([10.9817], grad_fn=<DivBackward0>)\n",
      "1:10 loss --> tensor([28.2779], grad_fn=<DivBackward0>)\n",
      "1:20 loss --> tensor([12.5013], grad_fn=<DivBackward0>)\n",
      "1:30 loss --> tensor([22.7310], grad_fn=<DivBackward0>)\n",
      "1:40 loss --> tensor([11.0990], grad_fn=<DivBackward0>)\n",
      "1:50 loss --> tensor([9.0520], grad_fn=<DivBackward0>)\n",
      "1:60 loss --> tensor([15.1589], grad_fn=<DivBackward0>)\n",
      "1:70 loss --> tensor([16.1022], grad_fn=<DivBackward0>)\n",
      "1:80 loss --> tensor([21.4937], grad_fn=<DivBackward0>)\n",
      "1:90 loss --> tensor([24.7296], grad_fn=<DivBackward0>)\n",
      "1:100 loss --> tensor([9.4360], grad_fn=<DivBackward0>)\n",
      "1:110 loss --> tensor([10.3729], grad_fn=<DivBackward0>)\n",
      "1:120 loss --> tensor([9.8497], grad_fn=<DivBackward0>)\n",
      "1:130 loss --> tensor([8.0697], grad_fn=<DivBackward0>)\n",
      "1:140 loss --> tensor([9.7845], grad_fn=<DivBackward0>)\n",
      "2:0 loss --> tensor([7.6384], grad_fn=<DivBackward0>)\n",
      "2:10 loss --> tensor([8.4288], grad_fn=<DivBackward0>)\n",
      "2:20 loss --> tensor([7.4787], grad_fn=<DivBackward0>)\n",
      "2:30 loss --> tensor([5.1242], grad_fn=<DivBackward0>)\n",
      "2:40 loss --> tensor([5.4999], grad_fn=<DivBackward0>)\n",
      "2:50 loss --> tensor([5.6282], grad_fn=<DivBackward0>)\n",
      "2:60 loss --> tensor([4.4453], grad_fn=<DivBackward0>)\n",
      "2:70 loss --> tensor([9.8130], grad_fn=<DivBackward0>)\n",
      "2:80 loss --> tensor([5.6961], grad_fn=<DivBackward0>)\n",
      "2:90 loss --> tensor([10.1451], grad_fn=<DivBackward0>)\n",
      "2:100 loss --> tensor([4.4921], grad_fn=<DivBackward0>)\n",
      "2:110 loss --> tensor([3.9594], grad_fn=<DivBackward0>)\n",
      "2:120 loss --> tensor([19.1258], grad_fn=<DivBackward0>)\n",
      "2:130 loss --> tensor([3.0576], grad_fn=<DivBackward0>)\n",
      "2:140 loss --> tensor([6.7883], grad_fn=<DivBackward0>)\n",
      "3:0 loss --> tensor([6.2436], grad_fn=<DivBackward0>)\n",
      "3:10 loss --> tensor([5.2135], grad_fn=<DivBackward0>)\n",
      "3:20 loss --> tensor([5.4170], grad_fn=<DivBackward0>)\n",
      "3:30 loss --> tensor([11.4397], grad_fn=<DivBackward0>)\n",
      "3:40 loss --> tensor([5.0069], grad_fn=<DivBackward0>)\n",
      "3:50 loss --> tensor([9.2299], grad_fn=<DivBackward0>)\n",
      "3:60 loss --> tensor([2.7963], grad_fn=<DivBackward0>)\n",
      "3:70 loss --> tensor([2.7076], grad_fn=<DivBackward0>)\n",
      "3:80 loss --> tensor([12.6714], grad_fn=<DivBackward0>)\n",
      "3:90 loss --> tensor([1.5503], grad_fn=<DivBackward0>)\n",
      "3:100 loss --> tensor([12.2525], grad_fn=<DivBackward0>)\n",
      "3:110 loss --> tensor([4.6674], grad_fn=<DivBackward0>)\n",
      "3:120 loss --> tensor([2.8498], grad_fn=<DivBackward0>)\n",
      "3:130 loss --> tensor([2.8989], grad_fn=<DivBackward0>)\n",
      "3:140 loss --> tensor([6.5617], grad_fn=<DivBackward0>)\n",
      "4:0 loss --> tensor([8.6493], grad_fn=<DivBackward0>)\n",
      "4:10 loss --> tensor([2.2715], grad_fn=<DivBackward0>)\n",
      "4:20 loss --> tensor([4.1219], grad_fn=<DivBackward0>)\n",
      "4:30 loss --> tensor([5.0218], grad_fn=<DivBackward0>)\n",
      "4:40 loss --> tensor([3.0168], grad_fn=<DivBackward0>)\n",
      "4:50 loss --> tensor([1.8082], grad_fn=<DivBackward0>)\n",
      "4:60 loss --> tensor([2.5561], grad_fn=<DivBackward0>)\n",
      "4:70 loss --> tensor([2.4951], grad_fn=<DivBackward0>)\n",
      "4:80 loss --> tensor([8.2852], grad_fn=<DivBackward0>)\n",
      "4:90 loss --> tensor([4.6325], grad_fn=<DivBackward0>)\n",
      "4:100 loss --> tensor([3.0762], grad_fn=<DivBackward0>)\n",
      "4:110 loss --> tensor([5.6552], grad_fn=<DivBackward0>)\n",
      "4:120 loss --> tensor([1.9165], grad_fn=<DivBackward0>)\n",
      "4:130 loss --> tensor([5.8676], grad_fn=<DivBackward0>)\n",
      "4:140 loss --> tensor([3.0931], grad_fn=<DivBackward0>)\n",
      "5:0 loss --> tensor([1.3279], grad_fn=<DivBackward0>)\n",
      "5:10 loss --> tensor([4.6664], grad_fn=<DivBackward0>)\n",
      "5:20 loss --> tensor([3.6021], grad_fn=<DivBackward0>)\n",
      "5:30 loss --> tensor([10.9734], grad_fn=<DivBackward0>)\n",
      "5:40 loss --> tensor([3.4750], grad_fn=<DivBackward0>)\n",
      "5:50 loss --> tensor([2.8559], grad_fn=<DivBackward0>)\n",
      "5:60 loss --> tensor([3.5577], grad_fn=<DivBackward0>)\n",
      "5:70 loss --> tensor([4.4905], grad_fn=<DivBackward0>)\n",
      "5:80 loss --> tensor([2.9750], grad_fn=<DivBackward0>)\n",
      "5:90 loss --> tensor([2.9978], grad_fn=<DivBackward0>)\n",
      "5:100 loss --> tensor([5.9025], grad_fn=<DivBackward0>)\n",
      "5:110 loss --> tensor([2.3720], grad_fn=<DivBackward0>)\n",
      "5:120 loss --> tensor([2.4343], grad_fn=<DivBackward0>)\n",
      "5:130 loss --> tensor([28.6452], grad_fn=<DivBackward0>)\n",
      "5:140 loss --> tensor([6.8061], grad_fn=<DivBackward0>)\n",
      "6:0 loss --> tensor([14.3352], grad_fn=<DivBackward0>)\n",
      "6:10 loss --> tensor([9.9875], grad_fn=<DivBackward0>)\n",
      "6:20 loss --> tensor([3.7326], grad_fn=<DivBackward0>)\n",
      "6:30 loss --> tensor([6.0018], grad_fn=<DivBackward0>)\n",
      "6:40 loss --> tensor([5.0011], grad_fn=<DivBackward0>)\n",
      "6:50 loss --> tensor([4.4327], grad_fn=<DivBackward0>)\n",
      "6:60 loss --> tensor([5.4890], grad_fn=<DivBackward0>)\n",
      "6:70 loss --> tensor([4.0963], grad_fn=<DivBackward0>)\n",
      "6:80 loss --> tensor([3.7195], grad_fn=<DivBackward0>)\n",
      "6:90 loss --> tensor([5.6513], grad_fn=<DivBackward0>)\n",
      "6:100 loss --> tensor([1.8962], grad_fn=<DivBackward0>)\n",
      "6:110 loss --> tensor([4.0270], grad_fn=<DivBackward0>)\n",
      "6:120 loss --> tensor([1.8950], grad_fn=<DivBackward0>)\n",
      "6:130 loss --> tensor([3.9196], grad_fn=<DivBackward0>)\n",
      "6:140 loss --> tensor([3.2349], grad_fn=<DivBackward0>)\n",
      "7:0 loss --> tensor([1.7780], grad_fn=<DivBackward0>)\n",
      "7:10 loss --> tensor([1.7843], grad_fn=<DivBackward0>)\n",
      "7:20 loss --> tensor([3.7219], grad_fn=<DivBackward0>)\n",
      "7:30 loss --> tensor([3.1673], grad_fn=<DivBackward0>)\n",
      "7:40 loss --> tensor([2.0574], grad_fn=<DivBackward0>)\n",
      "7:50 loss --> tensor([1.9338], grad_fn=<DivBackward0>)\n",
      "7:60 loss --> tensor([1.6865], grad_fn=<DivBackward0>)\n",
      "7:70 loss --> tensor([48.5906], grad_fn=<DivBackward0>)\n",
      "7:80 loss --> tensor([4.2701], grad_fn=<DivBackward0>)\n",
      "7:90 loss --> tensor([5.7861], grad_fn=<DivBackward0>)\n",
      "7:100 loss --> tensor([4.8526], grad_fn=<DivBackward0>)\n",
      "7:110 loss --> tensor([3.1499], grad_fn=<DivBackward0>)\n",
      "7:120 loss --> tensor([1.8716], grad_fn=<DivBackward0>)\n",
      "7:130 loss --> tensor([6.7886], grad_fn=<DivBackward0>)\n",
      "7:140 loss --> tensor([3.9944], grad_fn=<DivBackward0>)\n",
      "8:0 loss --> tensor([2.5778], grad_fn=<DivBackward0>)\n",
      "8:10 loss --> tensor([1.6472], grad_fn=<DivBackward0>)\n",
      "8:20 loss --> tensor([1.8341], grad_fn=<DivBackward0>)\n",
      "8:30 loss --> tensor([0.8777], grad_fn=<DivBackward0>)\n",
      "8:40 loss --> tensor([2.0932], grad_fn=<DivBackward0>)\n",
      "8:50 loss --> tensor([2.4232], grad_fn=<DivBackward0>)\n",
      "8:60 loss --> tensor([1.4747], grad_fn=<DivBackward0>)\n",
      "8:70 loss --> tensor([2.6397], grad_fn=<DivBackward0>)\n",
      "8:80 loss --> tensor([1.4562], grad_fn=<DivBackward0>)\n",
      "8:90 loss --> tensor([5.6277], grad_fn=<DivBackward0>)\n",
      "8:100 loss --> tensor([2.2702], grad_fn=<DivBackward0>)\n",
      "8:110 loss --> tensor([4.9212], grad_fn=<DivBackward0>)\n",
      "8:120 loss --> tensor([3.8063], grad_fn=<DivBackward0>)\n",
      "8:130 loss --> tensor([4.5863], grad_fn=<DivBackward0>)\n",
      "8:140 loss --> tensor([7.3029], grad_fn=<DivBackward0>)\n",
      "9:0 loss --> tensor([1.6593], grad_fn=<DivBackward0>)\n",
      "9:10 loss --> tensor([4.7982], grad_fn=<DivBackward0>)\n",
      "9:20 loss --> tensor([6.8615], grad_fn=<DivBackward0>)\n",
      "9:30 loss --> tensor([6.2552], grad_fn=<DivBackward0>)\n",
      "9:40 loss --> tensor([0.7976], grad_fn=<DivBackward0>)\n",
      "9:50 loss --> tensor([1.9928], grad_fn=<DivBackward0>)\n",
      "9:60 loss --> tensor([2.1863], grad_fn=<DivBackward0>)\n",
      "9:70 loss --> tensor([2.1109], grad_fn=<DivBackward0>)\n",
      "9:80 loss --> tensor([2.0646], grad_fn=<DivBackward0>)\n",
      "9:90 loss --> tensor([1.9391], grad_fn=<DivBackward0>)\n",
      "9:100 loss --> tensor([1.0768], grad_fn=<DivBackward0>)\n",
      "9:110 loss --> tensor([1.5237], grad_fn=<DivBackward0>)\n",
      "9:120 loss --> tensor([1.2981], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:130 loss --> tensor([1.2131], grad_fn=<DivBackward0>)\n",
      "9:140 loss --> tensor([2.1582], grad_fn=<DivBackward0>)\n",
      "10:0 loss --> tensor([0.7925], grad_fn=<DivBackward0>)\n",
      "10:10 loss --> tensor([1.0252], grad_fn=<DivBackward0>)\n",
      "10:20 loss --> tensor([0.9202], grad_fn=<DivBackward0>)\n",
      "10:30 loss --> tensor([2.2983], grad_fn=<DivBackward0>)\n",
      "10:40 loss --> tensor([1.7586], grad_fn=<DivBackward0>)\n",
      "10:50 loss --> tensor([1.0025], grad_fn=<DivBackward0>)\n",
      "10:60 loss --> tensor([1.1002], grad_fn=<DivBackward0>)\n",
      "10:70 loss --> tensor([1.6339], grad_fn=<DivBackward0>)\n",
      "10:80 loss --> tensor([1.1685], grad_fn=<DivBackward0>)\n",
      "10:90 loss --> tensor([1.7231], grad_fn=<DivBackward0>)\n",
      "10:100 loss --> tensor([1.5969], grad_fn=<DivBackward0>)\n",
      "10:110 loss --> tensor([1.5072], grad_fn=<DivBackward0>)\n",
      "10:120 loss --> tensor([1.4141], grad_fn=<DivBackward0>)\n",
      "10:130 loss --> tensor([0.9110], grad_fn=<DivBackward0>)\n",
      "10:140 loss --> tensor([0.9626], grad_fn=<DivBackward0>)\n",
      "11:0 loss --> tensor([1.0419], grad_fn=<DivBackward0>)\n",
      "11:10 loss --> tensor([0.6547], grad_fn=<DivBackward0>)\n",
      "11:20 loss --> tensor([0.7174], grad_fn=<DivBackward0>)\n",
      "11:30 loss --> tensor([0.4620], grad_fn=<DivBackward0>)\n",
      "11:40 loss --> tensor([1.2781], grad_fn=<DivBackward0>)\n",
      "11:50 loss --> tensor([6.5450], grad_fn=<DivBackward0>)\n",
      "11:60 loss --> tensor([7.3881], grad_fn=<DivBackward0>)\n",
      "11:70 loss --> tensor([2.4166], grad_fn=<DivBackward0>)\n",
      "11:80 loss --> tensor([2.4075], grad_fn=<DivBackward0>)\n",
      "11:90 loss --> tensor([1.7167], grad_fn=<DivBackward0>)\n",
      "11:100 loss --> tensor([0.8139], grad_fn=<DivBackward0>)\n",
      "11:110 loss --> tensor([1.8643], grad_fn=<DivBackward0>)\n",
      "11:120 loss --> tensor([1.5442], grad_fn=<DivBackward0>)\n",
      "11:130 loss --> tensor([2.2063], grad_fn=<DivBackward0>)\n",
      "11:140 loss --> tensor([1.2729], grad_fn=<DivBackward0>)\n",
      "12:0 loss --> tensor([0.7332], grad_fn=<DivBackward0>)\n",
      "12:10 loss --> tensor([1.1601], grad_fn=<DivBackward0>)\n",
      "12:20 loss --> tensor([0.6064], grad_fn=<DivBackward0>)\n",
      "12:30 loss --> tensor([1.3589], grad_fn=<DivBackward0>)\n",
      "12:40 loss --> tensor([2.3803], grad_fn=<DivBackward0>)\n",
      "12:50 loss --> tensor([1.9258], grad_fn=<DivBackward0>)\n",
      "12:60 loss --> tensor([1.3479], grad_fn=<DivBackward0>)\n",
      "12:70 loss --> tensor([0.5424], grad_fn=<DivBackward0>)\n",
      "12:80 loss --> tensor([1.5136], grad_fn=<DivBackward0>)\n",
      "12:90 loss --> tensor([0.8215], grad_fn=<DivBackward0>)\n",
      "12:100 loss --> tensor([1.0844], grad_fn=<DivBackward0>)\n",
      "12:110 loss --> tensor([0.7729], grad_fn=<DivBackward0>)\n",
      "12:120 loss --> tensor([0.7835], grad_fn=<DivBackward0>)\n",
      "12:130 loss --> tensor([1.4401], grad_fn=<DivBackward0>)\n",
      "12:140 loss --> tensor([0.6424], grad_fn=<DivBackward0>)\n",
      "13:0 loss --> tensor([0.5060], grad_fn=<DivBackward0>)\n",
      "13:10 loss --> tensor([0.8054], grad_fn=<DivBackward0>)\n",
      "13:20 loss --> tensor([1.5803], grad_fn=<DivBackward0>)\n",
      "13:30 loss --> tensor([0.8691], grad_fn=<DivBackward0>)\n",
      "13:40 loss --> tensor([0.7022], grad_fn=<DivBackward0>)\n",
      "13:50 loss --> tensor([2.0343], grad_fn=<DivBackward0>)\n",
      "13:60 loss --> tensor([0.5828], grad_fn=<DivBackward0>)\n",
      "13:70 loss --> tensor([1.5815], grad_fn=<DivBackward0>)\n",
      "13:80 loss --> tensor([0.4141], grad_fn=<DivBackward0>)\n",
      "13:90 loss --> tensor([0.7840], grad_fn=<DivBackward0>)\n",
      "13:100 loss --> tensor([0.8207], grad_fn=<DivBackward0>)\n",
      "13:110 loss --> tensor([0.4739], grad_fn=<DivBackward0>)\n",
      "13:120 loss --> tensor([0.7440], grad_fn=<DivBackward0>)\n",
      "13:130 loss --> tensor([0.4984], grad_fn=<DivBackward0>)\n",
      "13:140 loss --> tensor([3.0751], grad_fn=<DivBackward0>)\n",
      "14:0 loss --> tensor([1.7687], grad_fn=<DivBackward0>)\n",
      "14:10 loss --> tensor([0.6994], grad_fn=<DivBackward0>)\n",
      "14:20 loss --> tensor([0.5382], grad_fn=<DivBackward0>)\n",
      "14:30 loss --> tensor([0.5635], grad_fn=<DivBackward0>)\n",
      "14:40 loss --> tensor([1.2344], grad_fn=<DivBackward0>)\n",
      "14:50 loss --> tensor([0.4917], grad_fn=<DivBackward0>)\n",
      "14:60 loss --> tensor([0.5344], grad_fn=<DivBackward0>)\n",
      "14:70 loss --> tensor([0.7996], grad_fn=<DivBackward0>)\n",
      "14:80 loss --> tensor([1.0381], grad_fn=<DivBackward0>)\n",
      "14:90 loss --> tensor([0.5895], grad_fn=<DivBackward0>)\n",
      "14:100 loss --> tensor([0.2939], grad_fn=<DivBackward0>)\n",
      "14:110 loss --> tensor([0.6370], grad_fn=<DivBackward0>)\n",
      "14:120 loss --> tensor([0.3712], grad_fn=<DivBackward0>)\n",
      "14:130 loss --> tensor([0.9777], grad_fn=<DivBackward0>)\n",
      "14:140 loss --> tensor([0.7015], grad_fn=<DivBackward0>)\n",
      "15:0 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "15:10 loss --> tensor([0.5328], grad_fn=<DivBackward0>)\n",
      "15:20 loss --> tensor([0.3816], grad_fn=<DivBackward0>)\n",
      "15:30 loss --> tensor([0.4990], grad_fn=<DivBackward0>)\n",
      "15:40 loss --> tensor([0.3379], grad_fn=<DivBackward0>)\n",
      "15:50 loss --> tensor([0.3161], grad_fn=<DivBackward0>)\n",
      "15:60 loss --> tensor([0.8670], grad_fn=<DivBackward0>)\n",
      "15:70 loss --> tensor([1.2586], grad_fn=<DivBackward0>)\n",
      "15:80 loss --> tensor([0.3472], grad_fn=<DivBackward0>)\n",
      "15:90 loss --> tensor([0.3999], grad_fn=<DivBackward0>)\n",
      "15:100 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "15:110 loss --> tensor([0.2552], grad_fn=<DivBackward0>)\n",
      "15:120 loss --> tensor([0.5391], grad_fn=<DivBackward0>)\n",
      "15:130 loss --> tensor([0.5850], grad_fn=<DivBackward0>)\n",
      "15:140 loss --> tensor([0.3354], grad_fn=<DivBackward0>)\n",
      "16:0 loss --> tensor([0.8459], grad_fn=<DivBackward0>)\n",
      "16:10 loss --> tensor([0.2451], grad_fn=<DivBackward0>)\n",
      "16:20 loss --> tensor([0.6537], grad_fn=<DivBackward0>)\n",
      "16:30 loss --> tensor([0.8570], grad_fn=<DivBackward0>)\n",
      "16:40 loss --> tensor([0.2404], grad_fn=<DivBackward0>)\n",
      "16:50 loss --> tensor([0.5455], grad_fn=<DivBackward0>)\n",
      "16:60 loss --> tensor([0.3966], grad_fn=<DivBackward0>)\n",
      "16:70 loss --> tensor([0.5658], grad_fn=<DivBackward0>)\n",
      "16:80 loss --> tensor([0.3174], grad_fn=<DivBackward0>)\n",
      "16:90 loss --> tensor([0.3947], grad_fn=<DivBackward0>)\n",
      "16:100 loss --> tensor([0.5657], grad_fn=<DivBackward0>)\n",
      "16:110 loss --> tensor([0.3207], grad_fn=<DivBackward0>)\n",
      "16:120 loss --> tensor([0.3135], grad_fn=<DivBackward0>)\n",
      "16:130 loss --> tensor([0.4019], grad_fn=<DivBackward0>)\n",
      "16:140 loss --> tensor([0.2041], grad_fn=<DivBackward0>)\n",
      "17:0 loss --> tensor([0.7636], grad_fn=<DivBackward0>)\n",
      "17:10 loss --> tensor([0.5931], grad_fn=<DivBackward0>)\n",
      "17:20 loss --> tensor([0.2087], grad_fn=<DivBackward0>)\n",
      "17:30 loss --> tensor([2.0180], grad_fn=<DivBackward0>)\n",
      "17:40 loss --> tensor([0.4556], grad_fn=<DivBackward0>)\n",
      "17:50 loss --> tensor([0.4006], grad_fn=<DivBackward0>)\n",
      "17:60 loss --> tensor([0.2948], grad_fn=<DivBackward0>)\n",
      "17:70 loss --> tensor([0.2974], grad_fn=<DivBackward0>)\n",
      "17:80 loss --> tensor([0.2688], grad_fn=<DivBackward0>)\n",
      "17:90 loss --> tensor([0.2111], grad_fn=<DivBackward0>)\n",
      "17:100 loss --> tensor([0.3759], grad_fn=<DivBackward0>)\n",
      "17:110 loss --> tensor([0.2699], grad_fn=<DivBackward0>)\n",
      "17:120 loss --> tensor([0.4306], grad_fn=<DivBackward0>)\n",
      "17:130 loss --> tensor([0.2390], grad_fn=<DivBackward0>)\n",
      "17:140 loss --> tensor([0.3765], grad_fn=<DivBackward0>)\n",
      "18:0 loss --> tensor([0.3233], grad_fn=<DivBackward0>)\n",
      "18:10 loss --> tensor([0.3022], grad_fn=<DivBackward0>)\n",
      "18:20 loss --> tensor([0.2504], grad_fn=<DivBackward0>)\n",
      "18:30 loss --> tensor([0.2636], grad_fn=<DivBackward0>)\n",
      "18:40 loss --> tensor([0.3139], grad_fn=<DivBackward0>)\n",
      "18:50 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "18:60 loss --> tensor([0.1966], grad_fn=<DivBackward0>)\n",
      "18:70 loss --> tensor([0.1337], grad_fn=<DivBackward0>)\n",
      "18:80 loss --> tensor([0.2214], grad_fn=<DivBackward0>)\n",
      "18:90 loss --> tensor([0.1435], grad_fn=<DivBackward0>)\n",
      "18:100 loss --> tensor([0.2211], grad_fn=<DivBackward0>)\n",
      "18:110 loss --> tensor([0.5994], grad_fn=<DivBackward0>)\n",
      "18:120 loss --> tensor([0.2253], grad_fn=<DivBackward0>)\n",
      "18:130 loss --> tensor([0.1877], grad_fn=<DivBackward0>)\n",
      "18:140 loss --> tensor([0.2907], grad_fn=<DivBackward0>)\n",
      "19:0 loss --> tensor([0.3078], grad_fn=<DivBackward0>)\n",
      "19:10 loss --> tensor([0.6215], grad_fn=<DivBackward0>)\n",
      "19:20 loss --> tensor([0.1777], grad_fn=<DivBackward0>)\n",
      "19:30 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "19:40 loss --> tensor([0.1926], grad_fn=<DivBackward0>)\n",
      "19:50 loss --> tensor([0.2600], grad_fn=<DivBackward0>)\n",
      "19:60 loss --> tensor([0.2780], grad_fn=<DivBackward0>)\n",
      "19:70 loss --> tensor([0.3205], grad_fn=<DivBackward0>)\n",
      "19:80 loss --> tensor([0.3586], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:90 loss --> tensor([0.2520], grad_fn=<DivBackward0>)\n",
      "19:100 loss --> tensor([0.4761], grad_fn=<DivBackward0>)\n",
      "19:110 loss --> tensor([0.4002], grad_fn=<DivBackward0>)\n",
      "19:120 loss --> tensor([0.2502], grad_fn=<DivBackward0>)\n",
      "19:130 loss --> tensor([0.3875], grad_fn=<DivBackward0>)\n",
      "19:140 loss --> tensor([0.4349], grad_fn=<DivBackward0>)\n",
      "20:0 loss --> tensor([0.2993], grad_fn=<DivBackward0>)\n",
      "20:10 loss --> tensor([0.2748], grad_fn=<DivBackward0>)\n",
      "20:20 loss --> tensor([0.3232], grad_fn=<DivBackward0>)\n",
      "20:30 loss --> tensor([0.2080], grad_fn=<DivBackward0>)\n",
      "20:40 loss --> tensor([0.6131], grad_fn=<DivBackward0>)\n",
      "20:50 loss --> tensor([0.2125], grad_fn=<DivBackward0>)\n",
      "20:60 loss --> tensor([0.3636], grad_fn=<DivBackward0>)\n",
      "20:70 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "20:80 loss --> tensor([0.1873], grad_fn=<DivBackward0>)\n",
      "20:90 loss --> tensor([0.2647], grad_fn=<DivBackward0>)\n",
      "20:100 loss --> tensor([0.2375], grad_fn=<DivBackward0>)\n",
      "20:110 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "20:120 loss --> tensor([0.3103], grad_fn=<DivBackward0>)\n",
      "20:130 loss --> tensor([0.2169], grad_fn=<DivBackward0>)\n",
      "20:140 loss --> tensor([0.2205], grad_fn=<DivBackward0>)\n",
      "21:0 loss --> tensor([0.1945], grad_fn=<DivBackward0>)\n",
      "21:10 loss --> tensor([0.1795], grad_fn=<DivBackward0>)\n",
      "21:20 loss --> tensor([0.2733], grad_fn=<DivBackward0>)\n",
      "21:30 loss --> tensor([0.5972], grad_fn=<DivBackward0>)\n",
      "21:40 loss --> tensor([0.2206], grad_fn=<DivBackward0>)\n",
      "21:50 loss --> tensor([0.5153], grad_fn=<DivBackward0>)\n",
      "21:60 loss --> tensor([0.2282], grad_fn=<DivBackward0>)\n",
      "21:70 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "21:80 loss --> tensor([0.3529], grad_fn=<DivBackward0>)\n",
      "21:90 loss --> tensor([0.1691], grad_fn=<DivBackward0>)\n",
      "21:100 loss --> tensor([0.8482], grad_fn=<DivBackward0>)\n",
      "21:110 loss --> tensor([0.5682], grad_fn=<DivBackward0>)\n",
      "21:120 loss --> tensor([0.2313], grad_fn=<DivBackward0>)\n",
      "21:130 loss --> tensor([0.3063], grad_fn=<DivBackward0>)\n",
      "21:140 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "22:0 loss --> tensor([0.4840], grad_fn=<DivBackward0>)\n",
      "22:10 loss --> tensor([0.1278], grad_fn=<DivBackward0>)\n",
      "22:20 loss --> tensor([0.5040], grad_fn=<DivBackward0>)\n",
      "22:30 loss --> tensor([0.2317], grad_fn=<DivBackward0>)\n",
      "22:40 loss --> tensor([0.1825], grad_fn=<DivBackward0>)\n",
      "22:50 loss --> tensor([0.1236], grad_fn=<DivBackward0>)\n",
      "22:60 loss --> tensor([0.1399], grad_fn=<DivBackward0>)\n",
      "22:70 loss --> tensor([0.6543], grad_fn=<DivBackward0>)\n",
      "22:80 loss --> tensor([0.2270], grad_fn=<DivBackward0>)\n",
      "22:90 loss --> tensor([0.1622], grad_fn=<DivBackward0>)\n",
      "22:100 loss --> tensor([0.3706], grad_fn=<DivBackward0>)\n",
      "22:110 loss --> tensor([0.2215], grad_fn=<DivBackward0>)\n",
      "22:120 loss --> tensor([0.1461], grad_fn=<DivBackward0>)\n",
      "22:130 loss --> tensor([0.3308], grad_fn=<DivBackward0>)\n",
      "22:140 loss --> tensor([0.6865], grad_fn=<DivBackward0>)\n",
      "23:0 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "23:10 loss --> tensor([0.1716], grad_fn=<DivBackward0>)\n",
      "23:20 loss --> tensor([0.1999], grad_fn=<DivBackward0>)\n",
      "23:30 loss --> tensor([0.2259], grad_fn=<DivBackward0>)\n",
      "23:40 loss --> tensor([0.6879], grad_fn=<DivBackward0>)\n",
      "23:50 loss --> tensor([0.6804], grad_fn=<DivBackward0>)\n",
      "23:60 loss --> tensor([0.6476], grad_fn=<DivBackward0>)\n",
      "23:70 loss --> tensor([0.2104], grad_fn=<DivBackward0>)\n",
      "23:80 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "23:90 loss --> tensor([0.1758], grad_fn=<DivBackward0>)\n",
      "23:100 loss --> tensor([0.2035], grad_fn=<DivBackward0>)\n",
      "23:110 loss --> tensor([0.3017], grad_fn=<DivBackward0>)\n",
      "23:120 loss --> tensor([0.1398], grad_fn=<DivBackward0>)\n",
      "23:130 loss --> tensor([0.1866], grad_fn=<DivBackward0>)\n",
      "23:140 loss --> tensor([0.2867], grad_fn=<DivBackward0>)\n",
      "24:0 loss --> tensor([0.2228], grad_fn=<DivBackward0>)\n",
      "24:10 loss --> tensor([0.2641], grad_fn=<DivBackward0>)\n",
      "24:20 loss --> tensor([0.4042], grad_fn=<DivBackward0>)\n",
      "24:30 loss --> tensor([0.1942], grad_fn=<DivBackward0>)\n",
      "24:40 loss --> tensor([0.2116], grad_fn=<DivBackward0>)\n",
      "24:50 loss --> tensor([0.1308], grad_fn=<DivBackward0>)\n",
      "24:60 loss --> tensor([0.2126], grad_fn=<DivBackward0>)\n",
      "24:70 loss --> tensor([0.3621], grad_fn=<DivBackward0>)\n",
      "24:80 loss --> tensor([0.1701], grad_fn=<DivBackward0>)\n",
      "24:90 loss --> tensor([0.6316], grad_fn=<DivBackward0>)\n",
      "24:100 loss --> tensor([0.1923], grad_fn=<DivBackward0>)\n",
      "24:110 loss --> tensor([0.2499], grad_fn=<DivBackward0>)\n",
      "24:120 loss --> tensor([0.1392], grad_fn=<DivBackward0>)\n",
      "24:130 loss --> tensor([0.1432], grad_fn=<DivBackward0>)\n",
      "24:140 loss --> tensor([0.3798], grad_fn=<DivBackward0>)\n",
      "25:0 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "25:10 loss --> tensor([0.1204], grad_fn=<DivBackward0>)\n",
      "25:20 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "25:30 loss --> tensor([0.2967], grad_fn=<DivBackward0>)\n",
      "25:40 loss --> tensor([0.1840], grad_fn=<DivBackward0>)\n",
      "25:50 loss --> tensor([0.5188], grad_fn=<DivBackward0>)\n",
      "25:60 loss --> tensor([0.1903], grad_fn=<DivBackward0>)\n",
      "25:70 loss --> tensor([0.3976], grad_fn=<DivBackward0>)\n",
      "25:80 loss --> tensor([0.5232], grad_fn=<DivBackward0>)\n",
      "25:90 loss --> tensor([0.1803], grad_fn=<DivBackward0>)\n",
      "25:100 loss --> tensor([0.1965], grad_fn=<DivBackward0>)\n",
      "25:110 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "25:120 loss --> tensor([0.2039], grad_fn=<DivBackward0>)\n",
      "25:130 loss --> tensor([0.4120], grad_fn=<DivBackward0>)\n",
      "25:140 loss --> tensor([0.1892], grad_fn=<DivBackward0>)\n",
      "26:0 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "26:10 loss --> tensor([0.1450], grad_fn=<DivBackward0>)\n",
      "26:20 loss --> tensor([0.1686], grad_fn=<DivBackward0>)\n",
      "26:30 loss --> tensor([0.1820], grad_fn=<DivBackward0>)\n",
      "26:40 loss --> tensor([0.0995], grad_fn=<DivBackward0>)\n",
      "26:50 loss --> tensor([0.1749], grad_fn=<DivBackward0>)\n",
      "26:60 loss --> tensor([0.1148], grad_fn=<DivBackward0>)\n",
      "26:70 loss --> tensor([0.3823], grad_fn=<DivBackward0>)\n",
      "26:80 loss --> tensor([0.1242], grad_fn=<DivBackward0>)\n",
      "26:90 loss --> tensor([0.1387], grad_fn=<DivBackward0>)\n",
      "26:100 loss --> tensor([0.1987], grad_fn=<DivBackward0>)\n",
      "26:110 loss --> tensor([0.2785], grad_fn=<DivBackward0>)\n",
      "26:120 loss --> tensor([0.7383], grad_fn=<DivBackward0>)\n",
      "26:130 loss --> tensor([0.1110], grad_fn=<DivBackward0>)\n",
      "26:140 loss --> tensor([0.2265], grad_fn=<DivBackward0>)\n",
      "27:0 loss --> tensor([0.2480], grad_fn=<DivBackward0>)\n",
      "27:10 loss --> tensor([0.1217], grad_fn=<DivBackward0>)\n",
      "27:20 loss --> tensor([0.5772], grad_fn=<DivBackward0>)\n",
      "27:30 loss --> tensor([0.1190], grad_fn=<DivBackward0>)\n",
      "27:40 loss --> tensor([0.1580], grad_fn=<DivBackward0>)\n",
      "27:50 loss --> tensor([0.3171], grad_fn=<DivBackward0>)\n",
      "27:60 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "27:70 loss --> tensor([0.1429], grad_fn=<DivBackward0>)\n",
      "27:80 loss --> tensor([0.1151], grad_fn=<DivBackward0>)\n",
      "27:90 loss --> tensor([0.2224], grad_fn=<DivBackward0>)\n",
      "27:100 loss --> tensor([0.2588], grad_fn=<DivBackward0>)\n",
      "27:110 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "27:120 loss --> tensor([0.1347], grad_fn=<DivBackward0>)\n",
      "27:130 loss --> tensor([0.1507], grad_fn=<DivBackward0>)\n",
      "27:140 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "28:0 loss --> tensor([0.1605], grad_fn=<DivBackward0>)\n",
      "28:10 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "28:20 loss --> tensor([0.1793], grad_fn=<DivBackward0>)\n",
      "28:30 loss --> tensor([0.1291], grad_fn=<DivBackward0>)\n",
      "28:40 loss --> tensor([0.1934], grad_fn=<DivBackward0>)\n",
      "28:50 loss --> tensor([0.1760], grad_fn=<DivBackward0>)\n",
      "28:60 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "28:70 loss --> tensor([0.1467], grad_fn=<DivBackward0>)\n",
      "28:80 loss --> tensor([0.2403], grad_fn=<DivBackward0>)\n",
      "28:90 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "28:100 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "28:110 loss --> tensor([0.1700], grad_fn=<DivBackward0>)\n",
      "28:120 loss --> tensor([0.3202], grad_fn=<DivBackward0>)\n",
      "28:130 loss --> tensor([0.1458], grad_fn=<DivBackward0>)\n",
      "28:140 loss --> tensor([0.2102], grad_fn=<DivBackward0>)\n",
      "29:0 loss --> tensor([0.1132], grad_fn=<DivBackward0>)\n",
      "29:10 loss --> tensor([0.1841], grad_fn=<DivBackward0>)\n",
      "29:20 loss --> tensor([0.1211], grad_fn=<DivBackward0>)\n",
      "29:30 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "29:40 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29:50 loss --> tensor([0.1220], grad_fn=<DivBackward0>)\n",
      "29:60 loss --> tensor([0.1549], grad_fn=<DivBackward0>)\n",
      "29:70 loss --> tensor([0.1106], grad_fn=<DivBackward0>)\n",
      "29:80 loss --> tensor([0.1326], grad_fn=<DivBackward0>)\n",
      "29:90 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "29:100 loss --> tensor([0.2556], grad_fn=<DivBackward0>)\n",
      "29:110 loss --> tensor([0.0985], grad_fn=<DivBackward0>)\n",
      "29:120 loss --> tensor([0.1253], grad_fn=<DivBackward0>)\n",
      "29:130 loss --> tensor([0.1685], grad_fn=<DivBackward0>)\n",
      "29:140 loss --> tensor([0.2129], grad_fn=<DivBackward0>)\n",
      "30:0 loss --> tensor([0.3538], grad_fn=<DivBackward0>)\n",
      "30:10 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "30:20 loss --> tensor([0.1093], grad_fn=<DivBackward0>)\n",
      "30:30 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "30:40 loss --> tensor([0.1302], grad_fn=<DivBackward0>)\n",
      "30:50 loss --> tensor([0.2209], grad_fn=<DivBackward0>)\n",
      "30:60 loss --> tensor([0.2022], grad_fn=<DivBackward0>)\n",
      "30:70 loss --> tensor([0.1259], grad_fn=<DivBackward0>)\n",
      "30:80 loss --> tensor([0.1509], grad_fn=<DivBackward0>)\n",
      "30:90 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "30:100 loss --> tensor([0.1401], grad_fn=<DivBackward0>)\n",
      "30:110 loss --> tensor([0.1174], grad_fn=<DivBackward0>)\n",
      "30:120 loss --> tensor([0.1314], grad_fn=<DivBackward0>)\n",
      "30:130 loss --> tensor([0.1074], grad_fn=<DivBackward0>)\n",
      "30:140 loss --> tensor([0.0943], grad_fn=<DivBackward0>)\n",
      "31:0 loss --> tensor([0.1456], grad_fn=<DivBackward0>)\n",
      "31:10 loss --> tensor([0.1727], grad_fn=<DivBackward0>)\n",
      "31:20 loss --> tensor([0.1765], grad_fn=<DivBackward0>)\n",
      "31:30 loss --> tensor([0.2947], grad_fn=<DivBackward0>)\n",
      "31:40 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "31:50 loss --> tensor([0.1256], grad_fn=<DivBackward0>)\n",
      "31:60 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "31:70 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "31:80 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "31:90 loss --> tensor([0.7890], grad_fn=<DivBackward0>)\n",
      "31:100 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "31:110 loss --> tensor([0.1219], grad_fn=<DivBackward0>)\n",
      "31:120 loss --> tensor([0.1342], grad_fn=<DivBackward0>)\n",
      "31:130 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "31:140 loss --> tensor([0.3037], grad_fn=<DivBackward0>)\n",
      "32:0 loss --> tensor([0.0918], grad_fn=<DivBackward0>)\n",
      "32:10 loss --> tensor([0.1039], grad_fn=<DivBackward0>)\n",
      "32:20 loss --> tensor([0.0852], grad_fn=<DivBackward0>)\n",
      "32:30 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "32:40 loss --> tensor([0.1182], grad_fn=<DivBackward0>)\n",
      "32:50 loss --> tensor([0.2883], grad_fn=<DivBackward0>)\n",
      "32:60 loss --> tensor([0.1487], grad_fn=<DivBackward0>)\n",
      "32:70 loss --> tensor([0.3298], grad_fn=<DivBackward0>)\n",
      "32:80 loss --> tensor([0.2597], grad_fn=<DivBackward0>)\n",
      "32:90 loss --> tensor([0.2056], grad_fn=<DivBackward0>)\n",
      "32:100 loss --> tensor([0.2085], grad_fn=<DivBackward0>)\n",
      "32:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "32:120 loss --> tensor([0.2013], grad_fn=<DivBackward0>)\n",
      "32:130 loss --> tensor([0.0907], grad_fn=<DivBackward0>)\n",
      "32:140 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "33:0 loss --> tensor([0.1353], grad_fn=<DivBackward0>)\n",
      "33:10 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "33:20 loss --> tensor([0.0904], grad_fn=<DivBackward0>)\n",
      "33:30 loss --> tensor([0.1138], grad_fn=<DivBackward0>)\n",
      "33:40 loss --> tensor([0.0739], grad_fn=<DivBackward0>)\n",
      "33:50 loss --> tensor([0.1927], grad_fn=<DivBackward0>)\n",
      "33:60 loss --> tensor([0.1201], grad_fn=<DivBackward0>)\n",
      "33:70 loss --> tensor([0.0894], grad_fn=<DivBackward0>)\n",
      "33:80 loss --> tensor([0.2045], grad_fn=<DivBackward0>)\n",
      "33:90 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "33:100 loss --> tensor([0.2642], grad_fn=<DivBackward0>)\n",
      "33:110 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "33:120 loss --> tensor([0.1246], grad_fn=<DivBackward0>)\n",
      "33:130 loss --> tensor([0.1243], grad_fn=<DivBackward0>)\n",
      "33:140 loss --> tensor([0.2314], grad_fn=<DivBackward0>)\n",
      "34:0 loss --> tensor([0.1745], grad_fn=<DivBackward0>)\n",
      "34:10 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "34:20 loss --> tensor([0.2813], grad_fn=<DivBackward0>)\n",
      "34:30 loss --> tensor([0.2197], grad_fn=<DivBackward0>)\n",
      "34:40 loss --> tensor([0.2510], grad_fn=<DivBackward0>)\n",
      "34:50 loss --> tensor([0.0774], grad_fn=<DivBackward0>)\n",
      "34:60 loss --> tensor([0.1177], grad_fn=<DivBackward0>)\n",
      "34:70 loss --> tensor([0.1260], grad_fn=<DivBackward0>)\n",
      "34:80 loss --> tensor([0.0923], grad_fn=<DivBackward0>)\n",
      "34:90 loss --> tensor([0.0764], grad_fn=<DivBackward0>)\n",
      "34:100 loss --> tensor([0.1421], grad_fn=<DivBackward0>)\n",
      "34:110 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "34:120 loss --> tensor([0.0772], grad_fn=<DivBackward0>)\n",
      "34:130 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "34:140 loss --> tensor([0.2277], grad_fn=<DivBackward0>)\n",
      "35:0 loss --> tensor([0.8505], grad_fn=<DivBackward0>)\n",
      "35:10 loss --> tensor([0.3195], grad_fn=<DivBackward0>)\n",
      "35:20 loss --> tensor([0.1292], grad_fn=<DivBackward0>)\n",
      "35:30 loss --> tensor([0.0888], grad_fn=<DivBackward0>)\n",
      "35:40 loss --> tensor([0.0875], grad_fn=<DivBackward0>)\n",
      "35:50 loss --> tensor([0.0966], grad_fn=<DivBackward0>)\n",
      "35:60 loss --> tensor([0.1363], grad_fn=<DivBackward0>)\n",
      "35:70 loss --> tensor([0.1410], grad_fn=<DivBackward0>)\n",
      "35:80 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "35:90 loss --> tensor([0.3423], grad_fn=<DivBackward0>)\n",
      "35:100 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "35:110 loss --> tensor([0.2190], grad_fn=<DivBackward0>)\n",
      "35:120 loss --> tensor([0.1662], grad_fn=<DivBackward0>)\n",
      "35:130 loss --> tensor([0.2361], grad_fn=<DivBackward0>)\n",
      "35:140 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "36:0 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "36:10 loss --> tensor([0.0690], grad_fn=<DivBackward0>)\n",
      "36:20 loss --> tensor([0.1189], grad_fn=<DivBackward0>)\n",
      "36:30 loss --> tensor([0.1005], grad_fn=<DivBackward0>)\n",
      "36:40 loss --> tensor([0.0604], grad_fn=<DivBackward0>)\n",
      "36:50 loss --> tensor([0.0631], grad_fn=<DivBackward0>)\n",
      "36:60 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "36:70 loss --> tensor([0.1114], grad_fn=<DivBackward0>)\n",
      "36:80 loss --> tensor([0.0975], grad_fn=<DivBackward0>)\n",
      "36:90 loss --> tensor([0.1018], grad_fn=<DivBackward0>)\n",
      "36:100 loss --> tensor([0.0887], grad_fn=<DivBackward0>)\n",
      "36:110 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "36:120 loss --> tensor([0.2512], grad_fn=<DivBackward0>)\n",
      "36:130 loss --> tensor([0.0786], grad_fn=<DivBackward0>)\n",
      "36:140 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "37:0 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "37:10 loss --> tensor([0.1330], grad_fn=<DivBackward0>)\n",
      "37:20 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "37:30 loss --> tensor([0.1012], grad_fn=<DivBackward0>)\n",
      "37:40 loss --> tensor([0.0704], grad_fn=<DivBackward0>)\n",
      "37:50 loss --> tensor([0.2084], grad_fn=<DivBackward0>)\n",
      "37:60 loss --> tensor([0.1598], grad_fn=<DivBackward0>)\n",
      "37:70 loss --> tensor([0.3623], grad_fn=<DivBackward0>)\n",
      "37:80 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "37:90 loss --> tensor([0.1809], grad_fn=<DivBackward0>)\n",
      "37:100 loss --> tensor([0.0755], grad_fn=<DivBackward0>)\n",
      "37:110 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "37:120 loss --> tensor([0.2436], grad_fn=<DivBackward0>)\n",
      "37:130 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "37:140 loss --> tensor([0.0765], grad_fn=<DivBackward0>)\n",
      "38:0 loss --> tensor([0.2501], grad_fn=<DivBackward0>)\n",
      "38:10 loss --> tensor([0.2694], grad_fn=<DivBackward0>)\n",
      "38:20 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "38:30 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "38:40 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "38:50 loss --> tensor([0.1377], grad_fn=<DivBackward0>)\n",
      "38:60 loss --> tensor([0.0838], grad_fn=<DivBackward0>)\n",
      "38:70 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "38:80 loss --> tensor([0.0823], grad_fn=<DivBackward0>)\n",
      "38:90 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "38:100 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "38:110 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "38:120 loss --> tensor([0.1386], grad_fn=<DivBackward0>)\n",
      "38:130 loss --> tensor([0.1328], grad_fn=<DivBackward0>)\n",
      "38:140 loss --> tensor([0.1798], grad_fn=<DivBackward0>)\n",
      "39:0 loss --> tensor([0.2380], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:10 loss --> tensor([0.1860], grad_fn=<DivBackward0>)\n",
      "39:20 loss --> tensor([0.0797], grad_fn=<DivBackward0>)\n",
      "39:30 loss --> tensor([0.0885], grad_fn=<DivBackward0>)\n",
      "39:40 loss --> tensor([0.1416], grad_fn=<DivBackward0>)\n",
      "39:50 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "39:60 loss --> tensor([0.1277], grad_fn=<DivBackward0>)\n",
      "39:70 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "39:80 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "39:90 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n",
      "39:100 loss --> tensor([0.0903], grad_fn=<DivBackward0>)\n",
      "39:110 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "39:120 loss --> tensor([0.0649], grad_fn=<DivBackward0>)\n",
      "39:130 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "39:140 loss --> tensor([0.0893], grad_fn=<DivBackward0>)\n",
      "40:0 loss --> tensor([0.0796], grad_fn=<DivBackward0>)\n",
      "40:10 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "40:20 loss --> tensor([0.1261], grad_fn=<DivBackward0>)\n",
      "40:30 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "40:40 loss --> tensor([0.0792], grad_fn=<DivBackward0>)\n",
      "40:50 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:60 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "40:70 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "40:80 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "40:90 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:100 loss --> tensor([0.1431], grad_fn=<DivBackward0>)\n",
      "40:110 loss --> tensor([0.1095], grad_fn=<DivBackward0>)\n",
      "40:120 loss --> tensor([0.2681], grad_fn=<DivBackward0>)\n",
      "40:130 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "40:140 loss --> tensor([0.1379], grad_fn=<DivBackward0>)\n",
      "41:0 loss --> tensor([0.1477], grad_fn=<DivBackward0>)\n",
      "41:10 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "41:20 loss --> tensor([0.1357], grad_fn=<DivBackward0>)\n",
      "41:30 loss --> tensor([0.0880], grad_fn=<DivBackward0>)\n",
      "41:40 loss --> tensor([0.1157], grad_fn=<DivBackward0>)\n",
      "41:50 loss --> tensor([0.1320], grad_fn=<DivBackward0>)\n",
      "41:60 loss --> tensor([0.1032], grad_fn=<DivBackward0>)\n",
      "41:70 loss --> tensor([0.0720], grad_fn=<DivBackward0>)\n",
      "41:80 loss --> tensor([0.1126], grad_fn=<DivBackward0>)\n",
      "41:90 loss --> tensor([0.1883], grad_fn=<DivBackward0>)\n",
      "41:100 loss --> tensor([0.1225], grad_fn=<DivBackward0>)\n",
      "41:110 loss --> tensor([0.0845], grad_fn=<DivBackward0>)\n",
      "41:120 loss --> tensor([0.8526], grad_fn=<DivBackward0>)\n",
      "41:130 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "41:140 loss --> tensor([0.1140], grad_fn=<DivBackward0>)\n",
      "42:0 loss --> tensor([0.1035], grad_fn=<DivBackward0>)\n",
      "42:10 loss --> tensor([0.2274], grad_fn=<DivBackward0>)\n",
      "42:20 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "42:30 loss --> tensor([0.0849], grad_fn=<DivBackward0>)\n",
      "42:40 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "42:50 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "42:60 loss --> tensor([0.0729], grad_fn=<DivBackward0>)\n",
      "42:70 loss --> tensor([0.0872], grad_fn=<DivBackward0>)\n",
      "42:80 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "42:90 loss --> tensor([0.1650], grad_fn=<DivBackward0>)\n",
      "42:100 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "42:110 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "42:120 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "42:130 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "42:140 loss --> tensor([0.1334], grad_fn=<DivBackward0>)\n",
      "43:0 loss --> tensor([0.1250], grad_fn=<DivBackward0>)\n",
      "43:10 loss --> tensor([0.0933], grad_fn=<DivBackward0>)\n",
      "43:20 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "43:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "43:40 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "43:50 loss --> tensor([0.0876], grad_fn=<DivBackward0>)\n",
      "43:60 loss --> tensor([0.0798], grad_fn=<DivBackward0>)\n",
      "43:70 loss --> tensor([0.1634], grad_fn=<DivBackward0>)\n",
      "43:80 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "43:90 loss --> tensor([0.2038], grad_fn=<DivBackward0>)\n",
      "43:100 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "43:110 loss --> tensor([0.0824], grad_fn=<DivBackward0>)\n",
      "43:120 loss --> tensor([0.0936], grad_fn=<DivBackward0>)\n",
      "43:130 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "43:140 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "44:0 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "44:10 loss --> tensor([0.0826], grad_fn=<DivBackward0>)\n",
      "44:20 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "44:30 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "44:40 loss --> tensor([0.0857], grad_fn=<DivBackward0>)\n",
      "44:50 loss --> tensor([0.1806], grad_fn=<DivBackward0>)\n",
      "44:60 loss --> tensor([0.2760], grad_fn=<DivBackward0>)\n",
      "44:70 loss --> tensor([0.0694], grad_fn=<DivBackward0>)\n",
      "44:80 loss --> tensor([0.0534], grad_fn=<DivBackward0>)\n",
      "44:90 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "44:100 loss --> tensor([0.1193], grad_fn=<DivBackward0>)\n",
      "44:110 loss --> tensor([0.1304], grad_fn=<DivBackward0>)\n",
      "44:120 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "44:130 loss --> tensor([0.1864], grad_fn=<DivBackward0>)\n",
      "44:140 loss --> tensor([0.0735], grad_fn=<DivBackward0>)\n",
      "45:0 loss --> tensor([0.0708], grad_fn=<DivBackward0>)\n",
      "45:10 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "45:20 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "45:30 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "45:40 loss --> tensor([0.1462], grad_fn=<DivBackward0>)\n",
      "45:50 loss --> tensor([0.1526], grad_fn=<DivBackward0>)\n",
      "45:60 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "45:70 loss --> tensor([0.1695], grad_fn=<DivBackward0>)\n",
      "45:80 loss --> tensor([0.1129], grad_fn=<DivBackward0>)\n",
      "45:90 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "45:100 loss --> tensor([0.0627], grad_fn=<DivBackward0>)\n",
      "45:110 loss --> tensor([0.0886], grad_fn=<DivBackward0>)\n",
      "45:120 loss --> tensor([0.0750], grad_fn=<DivBackward0>)\n",
      "45:130 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "45:140 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "46:0 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "46:10 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "46:20 loss --> tensor([0.0588], grad_fn=<DivBackward0>)\n",
      "46:30 loss --> tensor([0.1152], grad_fn=<DivBackward0>)\n",
      "46:40 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "46:50 loss --> tensor([0.0705], grad_fn=<DivBackward0>)\n",
      "46:60 loss --> tensor([0.0673], grad_fn=<DivBackward0>)\n",
      "46:70 loss --> tensor([0.1202], grad_fn=<DivBackward0>)\n",
      "46:80 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "46:90 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "46:100 loss --> tensor([0.0519], grad_fn=<DivBackward0>)\n",
      "46:110 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "46:120 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "46:130 loss --> tensor([0.0947], grad_fn=<DivBackward0>)\n",
      "46:140 loss --> tensor([0.0787], grad_fn=<DivBackward0>)\n",
      "47:0 loss --> tensor([0.1555], grad_fn=<DivBackward0>)\n",
      "47:10 loss --> tensor([0.0528], grad_fn=<DivBackward0>)\n",
      "47:20 loss --> tensor([0.1205], grad_fn=<DivBackward0>)\n",
      "47:30 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "47:40 loss --> tensor([0.1183], grad_fn=<DivBackward0>)\n",
      "47:50 loss --> tensor([0.1963], grad_fn=<DivBackward0>)\n",
      "47:60 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "47:70 loss --> tensor([0.0482], grad_fn=<DivBackward0>)\n",
      "47:80 loss --> tensor([0.0801], grad_fn=<DivBackward0>)\n",
      "47:90 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "47:100 loss --> tensor([0.1557], grad_fn=<DivBackward0>)\n",
      "47:110 loss --> tensor([0.2377], grad_fn=<DivBackward0>)\n",
      "47:120 loss --> tensor([0.0759], grad_fn=<DivBackward0>)\n",
      "47:130 loss --> tensor([0.2052], grad_fn=<DivBackward0>)\n",
      "47:140 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:0 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "48:10 loss --> tensor([0.0606], grad_fn=<DivBackward0>)\n",
      "48:20 loss --> tensor([0.0602], grad_fn=<DivBackward0>)\n",
      "48:30 loss --> tensor([0.0575], grad_fn=<DivBackward0>)\n",
      "48:40 loss --> tensor([0.0643], grad_fn=<DivBackward0>)\n",
      "48:50 loss --> tensor([0.0789], grad_fn=<DivBackward0>)\n",
      "48:60 loss --> tensor([0.0742], grad_fn=<DivBackward0>)\n",
      "48:70 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "48:80 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:90 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "48:100 loss --> tensor([0.3702], grad_fn=<DivBackward0>)\n",
      "48:110 loss --> tensor([0.0442], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48:120 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "48:130 loss --> tensor([0.1465], grad_fn=<DivBackward0>)\n",
      "48:140 loss --> tensor([0.0457], grad_fn=<DivBackward0>)\n",
      "49:0 loss --> tensor([0.0611], grad_fn=<DivBackward0>)\n",
      "49:10 loss --> tensor([0.1714], grad_fn=<DivBackward0>)\n",
      "49:20 loss --> tensor([0.0749], grad_fn=<DivBackward0>)\n",
      "49:30 loss --> tensor([0.1451], grad_fn=<DivBackward0>)\n",
      "49:40 loss --> tensor([0.0926], grad_fn=<DivBackward0>)\n",
      "49:50 loss --> tensor([0.0763], grad_fn=<DivBackward0>)\n",
      "49:60 loss --> tensor([0.0895], grad_fn=<DivBackward0>)\n",
      "49:70 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "49:80 loss --> tensor([0.0934], grad_fn=<DivBackward0>)\n",
      "49:90 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "49:100 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "49:110 loss --> tensor([0.0861], grad_fn=<DivBackward0>)\n",
      "49:120 loss --> tensor([0.0731], grad_fn=<DivBackward0>)\n",
      "49:130 loss --> tensor([0.1325], grad_fn=<DivBackward0>)\n",
      "49:140 loss --> tensor([0.1481], grad_fn=<DivBackward0>)\n",
      "50:0 loss --> tensor([0.1210], grad_fn=<DivBackward0>)\n",
      "50:10 loss --> tensor([0.0490], grad_fn=<DivBackward0>)\n",
      "50:20 loss --> tensor([0.0626], grad_fn=<DivBackward0>)\n",
      "50:30 loss --> tensor([0.0662], grad_fn=<DivBackward0>)\n",
      "50:40 loss --> tensor([0.1070], grad_fn=<DivBackward0>)\n",
      "50:50 loss --> tensor([0.0652], grad_fn=<DivBackward0>)\n",
      "50:60 loss --> tensor([0.0732], grad_fn=<DivBackward0>)\n",
      "50:70 loss --> tensor([0.0590], grad_fn=<DivBackward0>)\n",
      "50:80 loss --> tensor([0.0489], grad_fn=<DivBackward0>)\n",
      "50:90 loss --> tensor([0.2185], grad_fn=<DivBackward0>)\n",
      "50:100 loss --> tensor([0.1642], grad_fn=<DivBackward0>)\n",
      "50:110 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "50:120 loss --> tensor([0.0710], grad_fn=<DivBackward0>)\n",
      "50:130 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "50:140 loss --> tensor([0.0803], grad_fn=<DivBackward0>)\n",
      "51:0 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "51:10 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "51:20 loss --> tensor([0.0501], grad_fn=<DivBackward0>)\n",
      "51:30 loss --> tensor([0.0556], grad_fn=<DivBackward0>)\n",
      "51:40 loss --> tensor([0.0846], grad_fn=<DivBackward0>)\n",
      "51:50 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "51:60 loss --> tensor([0.0654], grad_fn=<DivBackward0>)\n",
      "51:70 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "51:80 loss --> tensor([0.0474], grad_fn=<DivBackward0>)\n",
      "51:90 loss --> tensor([0.0659], grad_fn=<DivBackward0>)\n",
      "51:100 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "51:110 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "51:120 loss --> tensor([0.0762], grad_fn=<DivBackward0>)\n",
      "51:130 loss --> tensor([0.0700], grad_fn=<DivBackward0>)\n",
      "51:140 loss --> tensor([0.1950], grad_fn=<DivBackward0>)\n",
      "52:0 loss --> tensor([0.1135], grad_fn=<DivBackward0>)\n",
      "52:10 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "52:20 loss --> tensor([0.5752], grad_fn=<DivBackward0>)\n",
      "52:30 loss --> tensor([0.0773], grad_fn=<DivBackward0>)\n",
      "52:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "52:50 loss --> tensor([0.0469], grad_fn=<DivBackward0>)\n",
      "52:60 loss --> tensor([0.0806], grad_fn=<DivBackward0>)\n",
      "52:70 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "52:80 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "52:90 loss --> tensor([0.1563], grad_fn=<DivBackward0>)\n",
      "52:100 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "52:110 loss --> tensor([0.1221], grad_fn=<DivBackward0>)\n",
      "52:120 loss --> tensor([0.2011], grad_fn=<DivBackward0>)\n",
      "52:130 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "52:140 loss --> tensor([0.0601], grad_fn=<DivBackward0>)\n",
      "53:0 loss --> tensor([0.0808], grad_fn=<DivBackward0>)\n",
      "53:10 loss --> tensor([0.0529], grad_fn=<DivBackward0>)\n",
      "53:20 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "53:30 loss --> tensor([0.0648], grad_fn=<DivBackward0>)\n",
      "53:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "53:50 loss --> tensor([0.0834], grad_fn=<DivBackward0>)\n",
      "53:60 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "53:70 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "53:80 loss --> tensor([0.0589], grad_fn=<DivBackward0>)\n",
      "53:90 loss --> tensor([0.0853], grad_fn=<DivBackward0>)\n",
      "53:100 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "53:110 loss --> tensor([0.0635], grad_fn=<DivBackward0>)\n",
      "53:120 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "53:130 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "53:140 loss --> tensor([0.0510], grad_fn=<DivBackward0>)\n",
      "54:0 loss --> tensor([0.0979], grad_fn=<DivBackward0>)\n",
      "54:10 loss --> tensor([0.0468], grad_fn=<DivBackward0>)\n",
      "54:20 loss --> tensor([0.0751], grad_fn=<DivBackward0>)\n",
      "54:30 loss --> tensor([0.1009], grad_fn=<DivBackward0>)\n",
      "54:40 loss --> tensor([0.0406], grad_fn=<DivBackward0>)\n",
      "54:50 loss --> tensor([0.0863], grad_fn=<DivBackward0>)\n",
      "54:60 loss --> tensor([0.0520], grad_fn=<DivBackward0>)\n",
      "54:70 loss --> tensor([0.1224], grad_fn=<DivBackward0>)\n",
      "54:80 loss --> tensor([0.0551], grad_fn=<DivBackward0>)\n",
      "54:90 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "54:100 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "54:110 loss --> tensor([0.0655], grad_fn=<DivBackward0>)\n",
      "54:120 loss --> tensor([0.0607], grad_fn=<DivBackward0>)\n",
      "54:130 loss --> tensor([0.0458], grad_fn=<DivBackward0>)\n",
      "54:140 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "55:0 loss --> tensor([0.0901], grad_fn=<DivBackward0>)\n",
      "55:10 loss --> tensor([0.0513], grad_fn=<DivBackward0>)\n",
      "55:20 loss --> tensor([0.0522], grad_fn=<DivBackward0>)\n",
      "55:30 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "55:40 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "55:50 loss --> tensor([0.0647], grad_fn=<DivBackward0>)\n",
      "55:60 loss --> tensor([0.0674], grad_fn=<DivBackward0>)\n",
      "55:70 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "55:80 loss --> tensor([0.1081], grad_fn=<DivBackward0>)\n",
      "55:90 loss --> tensor([0.0389], grad_fn=<DivBackward0>)\n",
      "55:100 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "55:110 loss --> tensor([0.0340], grad_fn=<DivBackward0>)\n",
      "55:120 loss --> tensor([0.0666], grad_fn=<DivBackward0>)\n",
      "55:130 loss --> tensor([0.0585], grad_fn=<DivBackward0>)\n",
      "55:140 loss --> tensor([0.2171], grad_fn=<DivBackward0>)\n",
      "56:0 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "56:10 loss --> tensor([0.0965], grad_fn=<DivBackward0>)\n",
      "56:20 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "56:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "56:40 loss --> tensor([0.0550], grad_fn=<DivBackward0>)\n",
      "56:50 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "56:60 loss --> tensor([0.0533], grad_fn=<DivBackward0>)\n",
      "56:70 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "56:80 loss --> tensor([0.0596], grad_fn=<DivBackward0>)\n",
      "56:90 loss --> tensor([0.0570], grad_fn=<DivBackward0>)\n",
      "56:100 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "56:110 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "56:120 loss --> tensor([0.0595], grad_fn=<DivBackward0>)\n",
      "56:130 loss --> tensor([0.0650], grad_fn=<DivBackward0>)\n",
      "56:140 loss --> tensor([0.0770], grad_fn=<DivBackward0>)\n",
      "57:0 loss --> tensor([0.0624], grad_fn=<DivBackward0>)\n",
      "57:10 loss --> tensor([0.0847], grad_fn=<DivBackward0>)\n",
      "57:20 loss --> tensor([3.4022], grad_fn=<DivBackward0>)\n",
      "57:30 loss --> tensor([9.9447], grad_fn=<DivBackward0>)\n",
      "57:40 loss --> tensor([16.4048], grad_fn=<DivBackward0>)\n",
      "57:50 loss --> tensor([21.5102], grad_fn=<DivBackward0>)\n",
      "57:60 loss --> tensor([21.6429], grad_fn=<DivBackward0>)\n",
      "57:70 loss --> tensor([11.5656], grad_fn=<DivBackward0>)\n",
      "57:80 loss --> tensor([12.1413], grad_fn=<DivBackward0>)\n",
      "57:90 loss --> tensor([14.7159], grad_fn=<DivBackward0>)\n",
      "57:100 loss --> tensor([4.5711], grad_fn=<DivBackward0>)\n",
      "57:110 loss --> tensor([10.8564], grad_fn=<DivBackward0>)\n",
      "57:120 loss --> tensor([4.1304], grad_fn=<DivBackward0>)\n",
      "57:130 loss --> tensor([13.4019], grad_fn=<DivBackward0>)\n",
      "57:140 loss --> tensor([13.7998], grad_fn=<DivBackward0>)\n",
      "58:0 loss --> tensor([5.2142], grad_fn=<DivBackward0>)\n",
      "58:10 loss --> tensor([3.1334], grad_fn=<DivBackward0>)\n",
      "58:20 loss --> tensor([5.7519], grad_fn=<DivBackward0>)\n",
      "58:30 loss --> tensor([2.8351], grad_fn=<DivBackward0>)\n",
      "58:40 loss --> tensor([6.1557], grad_fn=<DivBackward0>)\n",
      "58:50 loss --> tensor([2.6427], grad_fn=<DivBackward0>)\n",
      "58:60 loss --> tensor([3.2629], grad_fn=<DivBackward0>)\n",
      "58:70 loss --> tensor([1.2779], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58:80 loss --> tensor([4.4251], grad_fn=<DivBackward0>)\n",
      "58:90 loss --> tensor([2.0784], grad_fn=<DivBackward0>)\n",
      "58:100 loss --> tensor([3.4789], grad_fn=<DivBackward0>)\n",
      "58:110 loss --> tensor([2.0481], grad_fn=<DivBackward0>)\n",
      "58:120 loss --> tensor([3.2647], grad_fn=<DivBackward0>)\n",
      "58:130 loss --> tensor([2.9361], grad_fn=<DivBackward0>)\n",
      "58:140 loss --> tensor([1.0645], grad_fn=<DivBackward0>)\n",
      "59:0 loss --> tensor([1.4378], grad_fn=<DivBackward0>)\n",
      "59:10 loss --> tensor([0.8588], grad_fn=<DivBackward0>)\n",
      "59:20 loss --> tensor([1.3759], grad_fn=<DivBackward0>)\n",
      "59:30 loss --> tensor([2.0949], grad_fn=<DivBackward0>)\n",
      "59:40 loss --> tensor([0.6558], grad_fn=<DivBackward0>)\n",
      "59:50 loss --> tensor([1.0538], grad_fn=<DivBackward0>)\n",
      "59:60 loss --> tensor([0.8373], grad_fn=<DivBackward0>)\n",
      "59:70 loss --> tensor([0.8074], grad_fn=<DivBackward0>)\n",
      "59:80 loss --> tensor([0.9266], grad_fn=<DivBackward0>)\n",
      "59:90 loss --> tensor([1.1950], grad_fn=<DivBackward0>)\n",
      "59:100 loss --> tensor([0.8516], grad_fn=<DivBackward0>)\n",
      "59:110 loss --> tensor([1.5343], grad_fn=<DivBackward0>)\n",
      "59:120 loss --> tensor([0.8791], grad_fn=<DivBackward0>)\n",
      "59:130 loss --> tensor([0.6115], grad_fn=<DivBackward0>)\n",
      "59:140 loss --> tensor([0.8940], grad_fn=<DivBackward0>)\n",
      "60:0 loss --> tensor([2.8170], grad_fn=<DivBackward0>)\n",
      "60:10 loss --> tensor([1.8884], grad_fn=<DivBackward0>)\n",
      "60:20 loss --> tensor([2.8654], grad_fn=<DivBackward0>)\n",
      "60:30 loss --> tensor([0.4791], grad_fn=<DivBackward0>)\n",
      "60:40 loss --> tensor([0.2849], grad_fn=<DivBackward0>)\n",
      "60:50 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "60:60 loss --> tensor([0.9389], grad_fn=<DivBackward0>)\n",
      "60:70 loss --> tensor([0.8761], grad_fn=<DivBackward0>)\n",
      "60:80 loss --> tensor([1.0118], grad_fn=<DivBackward0>)\n",
      "60:90 loss --> tensor([0.6532], grad_fn=<DivBackward0>)\n",
      "60:100 loss --> tensor([0.8287], grad_fn=<DivBackward0>)\n",
      "60:110 loss --> tensor([0.6260], grad_fn=<DivBackward0>)\n",
      "60:120 loss --> tensor([1.5400], grad_fn=<DivBackward0>)\n",
      "60:130 loss --> tensor([0.4415], grad_fn=<DivBackward0>)\n",
      "60:140 loss --> tensor([1.0203], grad_fn=<DivBackward0>)\n",
      "61:0 loss --> tensor([0.2919], grad_fn=<DivBackward0>)\n",
      "61:10 loss --> tensor([0.3358], grad_fn=<DivBackward0>)\n",
      "61:20 loss --> tensor([0.8916], grad_fn=<DivBackward0>)\n",
      "61:30 loss --> tensor([1.2408], grad_fn=<DivBackward0>)\n",
      "61:40 loss --> tensor([0.3908], grad_fn=<DivBackward0>)\n",
      "61:50 loss --> tensor([0.4140], grad_fn=<DivBackward0>)\n",
      "61:60 loss --> tensor([0.1920], grad_fn=<DivBackward0>)\n",
      "61:70 loss --> tensor([2.2829], grad_fn=<DivBackward0>)\n",
      "61:80 loss --> tensor([0.8797], grad_fn=<DivBackward0>)\n",
      "61:90 loss --> tensor([1.3787], grad_fn=<DivBackward0>)\n",
      "61:100 loss --> tensor([1.5900], grad_fn=<DivBackward0>)\n",
      "61:110 loss --> tensor([0.4128], grad_fn=<DivBackward0>)\n",
      "61:120 loss --> tensor([0.7773], grad_fn=<DivBackward0>)\n",
      "61:130 loss --> tensor([1.6292], grad_fn=<DivBackward0>)\n",
      "61:140 loss --> tensor([0.4005], grad_fn=<DivBackward0>)\n",
      "62:0 loss --> tensor([1.5718], grad_fn=<DivBackward0>)\n",
      "62:10 loss --> tensor([1.6002], grad_fn=<DivBackward0>)\n",
      "62:20 loss --> tensor([0.6693], grad_fn=<DivBackward0>)\n",
      "62:30 loss --> tensor([0.3175], grad_fn=<DivBackward0>)\n",
      "62:40 loss --> tensor([0.4632], grad_fn=<DivBackward0>)\n",
      "62:50 loss --> tensor([0.3199], grad_fn=<DivBackward0>)\n",
      "62:60 loss --> tensor([0.3003], grad_fn=<DivBackward0>)\n",
      "62:70 loss --> tensor([0.2534], grad_fn=<DivBackward0>)\n",
      "62:80 loss --> tensor([0.5483], grad_fn=<DivBackward0>)\n",
      "62:90 loss --> tensor([1.7910], grad_fn=<DivBackward0>)\n",
      "62:100 loss --> tensor([0.4748], grad_fn=<DivBackward0>)\n",
      "62:110 loss --> tensor([2.4929], grad_fn=<DivBackward0>)\n",
      "62:120 loss --> tensor([0.7229], grad_fn=<DivBackward0>)\n",
      "62:130 loss --> tensor([0.3221], grad_fn=<DivBackward0>)\n",
      "62:140 loss --> tensor([0.2379], grad_fn=<DivBackward0>)\n",
      "63:0 loss --> tensor([0.3731], grad_fn=<DivBackward0>)\n",
      "63:10 loss --> tensor([0.3776], grad_fn=<DivBackward0>)\n",
      "63:20 loss --> tensor([0.3585], grad_fn=<DivBackward0>)\n",
      "63:30 loss --> tensor([0.1885], grad_fn=<DivBackward0>)\n",
      "63:40 loss --> tensor([0.5387], grad_fn=<DivBackward0>)\n",
      "63:50 loss --> tensor([0.2644], grad_fn=<DivBackward0>)\n",
      "63:60 loss --> tensor([0.7096], grad_fn=<DivBackward0>)\n",
      "63:70 loss --> tensor([0.1696], grad_fn=<DivBackward0>)\n",
      "63:80 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "63:90 loss --> tensor([0.2151], grad_fn=<DivBackward0>)\n",
      "63:100 loss --> tensor([0.2178], grad_fn=<DivBackward0>)\n",
      "63:110 loss --> tensor([0.2516], grad_fn=<DivBackward0>)\n",
      "63:120 loss --> tensor([0.2470], grad_fn=<DivBackward0>)\n",
      "63:130 loss --> tensor([0.8904], grad_fn=<DivBackward0>)\n",
      "63:140 loss --> tensor([0.2378], grad_fn=<DivBackward0>)\n",
      "64:0 loss --> tensor([0.1719], grad_fn=<DivBackward0>)\n",
      "64:10 loss --> tensor([0.7317], grad_fn=<DivBackward0>)\n",
      "64:20 loss --> tensor([0.5398], grad_fn=<DivBackward0>)\n",
      "64:30 loss --> tensor([0.2442], grad_fn=<DivBackward0>)\n",
      "64:40 loss --> tensor([0.1300], grad_fn=<DivBackward0>)\n",
      "64:50 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "64:60 loss --> tensor([0.2767], grad_fn=<DivBackward0>)\n",
      "64:70 loss --> tensor([0.1390], grad_fn=<DivBackward0>)\n",
      "64:80 loss --> tensor([0.2683], grad_fn=<DivBackward0>)\n",
      "64:90 loss --> tensor([0.3036], grad_fn=<DivBackward0>)\n",
      "64:100 loss --> tensor([0.3564], grad_fn=<DivBackward0>)\n",
      "64:110 loss --> tensor([0.4248], grad_fn=<DivBackward0>)\n",
      "64:120 loss --> tensor([0.2656], grad_fn=<DivBackward0>)\n",
      "64:130 loss --> tensor([0.7259], grad_fn=<DivBackward0>)\n",
      "64:140 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "65:0 loss --> tensor([0.8729], grad_fn=<DivBackward0>)\n",
      "65:10 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "65:20 loss --> tensor([0.1666], grad_fn=<DivBackward0>)\n",
      "65:30 loss --> tensor([0.4864], grad_fn=<DivBackward0>)\n",
      "65:40 loss --> tensor([0.2346], grad_fn=<DivBackward0>)\n",
      "65:50 loss --> tensor([0.7711], grad_fn=<DivBackward0>)\n",
      "65:60 loss --> tensor([0.2281], grad_fn=<DivBackward0>)\n",
      "65:70 loss --> tensor([0.5715], grad_fn=<DivBackward0>)\n",
      "65:80 loss --> tensor([1.4477], grad_fn=<DivBackward0>)\n",
      "65:90 loss --> tensor([0.1623], grad_fn=<DivBackward0>)\n",
      "65:100 loss --> tensor([0.3099], grad_fn=<DivBackward0>)\n",
      "65:110 loss --> tensor([0.2257], grad_fn=<DivBackward0>)\n",
      "65:120 loss --> tensor([0.3226], grad_fn=<DivBackward0>)\n",
      "65:130 loss --> tensor([0.3414], grad_fn=<DivBackward0>)\n",
      "65:140 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:0 loss --> tensor([1.1191], grad_fn=<DivBackward0>)\n",
      "66:10 loss --> tensor([0.4810], grad_fn=<DivBackward0>)\n",
      "66:20 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "66:30 loss --> tensor([0.2133], grad_fn=<DivBackward0>)\n",
      "66:40 loss --> tensor([0.2722], grad_fn=<DivBackward0>)\n",
      "66:50 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:60 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "66:70 loss --> tensor([0.1718], grad_fn=<DivBackward0>)\n",
      "66:80 loss --> tensor([0.2414], grad_fn=<DivBackward0>)\n",
      "66:90 loss --> tensor([0.4626], grad_fn=<DivBackward0>)\n",
      "66:100 loss --> tensor([0.3433], grad_fn=<DivBackward0>)\n",
      "66:110 loss --> tensor([0.1731], grad_fn=<DivBackward0>)\n",
      "66:120 loss --> tensor([0.7181], grad_fn=<DivBackward0>)\n",
      "66:130 loss --> tensor([0.1520], grad_fn=<DivBackward0>)\n",
      "66:140 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "67:0 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "67:10 loss --> tensor([0.3112], grad_fn=<DivBackward0>)\n",
      "67:20 loss --> tensor([0.1441], grad_fn=<DivBackward0>)\n",
      "67:30 loss --> tensor([0.1785], grad_fn=<DivBackward0>)\n",
      "67:40 loss --> tensor([0.3715], grad_fn=<DivBackward0>)\n",
      "67:50 loss --> tensor([0.1290], grad_fn=<DivBackward0>)\n",
      "67:60 loss --> tensor([0.1705], grad_fn=<DivBackward0>)\n",
      "67:70 loss --> tensor([0.6034], grad_fn=<DivBackward0>)\n",
      "67:80 loss --> tensor([0.1160], grad_fn=<DivBackward0>)\n",
      "67:90 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "67:100 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "67:110 loss --> tensor([0.2879], grad_fn=<DivBackward0>)\n",
      "67:120 loss --> tensor([0.1026], grad_fn=<DivBackward0>)\n",
      "67:130 loss --> tensor([0.3092], grad_fn=<DivBackward0>)\n",
      "67:140 loss --> tensor([0.1229], grad_fn=<DivBackward0>)\n",
      "68:0 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "68:10 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "68:20 loss --> tensor([0.3287], grad_fn=<DivBackward0>)\n",
      "68:30 loss --> tensor([0.6754], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68:40 loss --> tensor([0.1940], grad_fn=<DivBackward0>)\n",
      "68:50 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "68:60 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "68:70 loss --> tensor([0.1858], grad_fn=<DivBackward0>)\n",
      "68:80 loss --> tensor([0.1167], grad_fn=<DivBackward0>)\n",
      "68:90 loss --> tensor([0.9398], grad_fn=<DivBackward0>)\n",
      "68:100 loss --> tensor([0.1030], grad_fn=<DivBackward0>)\n",
      "68:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "68:120 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "68:130 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "68:140 loss --> tensor([0.4271], grad_fn=<DivBackward0>)\n",
      "69:0 loss --> tensor([0.2381], grad_fn=<DivBackward0>)\n",
      "69:10 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "69:20 loss --> tensor([0.1660], grad_fn=<DivBackward0>)\n",
      "69:30 loss --> tensor([0.2088], grad_fn=<DivBackward0>)\n",
      "69:40 loss --> tensor([0.1419], grad_fn=<DivBackward0>)\n",
      "69:50 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:60 loss --> tensor([0.1773], grad_fn=<DivBackward0>)\n",
      "69:70 loss --> tensor([0.1231], grad_fn=<DivBackward0>)\n",
      "69:80 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "69:90 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:100 loss --> tensor([0.0877], grad_fn=<DivBackward0>)\n",
      "69:110 loss --> tensor([0.3021], grad_fn=<DivBackward0>)\n",
      "69:120 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "69:130 loss --> tensor([0.1192], grad_fn=<DivBackward0>)\n",
      "69:140 loss --> tensor([0.1689], grad_fn=<DivBackward0>)\n",
      "70:0 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "70:10 loss --> tensor([0.7175], grad_fn=<DivBackward0>)\n",
      "70:20 loss --> tensor([0.7906], grad_fn=<DivBackward0>)\n",
      "70:30 loss --> tensor([0.2661], grad_fn=<DivBackward0>)\n",
      "70:40 loss --> tensor([0.0900], grad_fn=<DivBackward0>)\n",
      "70:50 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "70:60 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "70:70 loss --> tensor([0.0991], grad_fn=<DivBackward0>)\n",
      "70:80 loss --> tensor([0.1057], grad_fn=<DivBackward0>)\n",
      "70:90 loss --> tensor([0.3856], grad_fn=<DivBackward0>)\n",
      "70:100 loss --> tensor([0.1463], grad_fn=<DivBackward0>)\n",
      "70:110 loss --> tensor([0.1318], grad_fn=<DivBackward0>)\n",
      "70:120 loss --> tensor([0.0892], grad_fn=<DivBackward0>)\n",
      "70:130 loss --> tensor([0.1156], grad_fn=<DivBackward0>)\n",
      "70:140 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "71:0 loss --> tensor([0.2592], grad_fn=<DivBackward0>)\n",
      "71:10 loss --> tensor([0.0816], grad_fn=<DivBackward0>)\n",
      "71:20 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "71:30 loss --> tensor([0.2759], grad_fn=<DivBackward0>)\n",
      "71:40 loss --> tensor([0.2751], grad_fn=<DivBackward0>)\n",
      "71:50 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "71:60 loss --> tensor([0.0835], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-91:\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7be988c99af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpreds_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, acts, labels, act_lens, label_lens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         return self.ctc(acts, labels, act_lens, label_lens, self.size_average,\n\u001b[0;32m---> 82\u001b[0;31m                         self.length_average, self.blank)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, acts, labels, act_lens, label_lens, size_average, length_average, blank)\u001b[0m\n\u001b[1;32m     30\u001b[0m                   \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                   blank)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/utils/ffi/__init__.py\u001b[0m in \u001b[0;36msafe_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m                      for arg in args)\n\u001b[1;32m    201\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtypeof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lib.data.char as c\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import lib.utils as utils\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "MOMENTUM = 0.9\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 字符转换编码\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "# 损失函数\n",
    "criterion = CTCLoss()\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     crnn.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer = optim.SGD(\n",
    "    crnn.parameters(), lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(t_image,t_label) in enumerate(train_loader):\n",
    "        batch_size = t_image.size(0)\n",
    "        utils.loadData(image, t_image)\n",
    "        t, l = converter.encode(t_label)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('{}:{} loss --> {}'.format(epoch, step, cost))\n",
    "            torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 42])\n",
      "torch.Size([10])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int32)\n",
      "tensor([49], dtype=torch.int32)\n",
      "tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([12.0142], grad_fn=<_CTCBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds_size = torch.IntTensor([49])\n",
    "# length = torch.IntTensor([2])\n",
    "print(preds.size())\n",
    "print(text.size())\n",
    "print(text)\n",
    "print(preds_size)\n",
    "print(length)\n",
    "\n",
    "criterion(preds, text, preds_size, length)\n",
    "\n",
    "# prob size --> torch.Size([2, 1, 5])\n",
    "# labels size --> torch.Size([2])\n",
    "# prob sizes -->tensor([2], dtype=torch.int32)\n",
    "# label sizes -->tensor([2], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.char as c\n",
    "import lib.utils as utils\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "_,(v_image,v_text) = next(enumerate(train_loader))\n",
    "utils.loadData(image,v_image)\n",
    "preds_s = crnn(image)\n",
    "batch_size = v_image.size(0)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)] * batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 49, 1441])\n",
      "tensor([1296,    0, 1297, 1297,    0,    0,    0, 1298, 1299, 1299,    0,    0,\n",
      "           0, 1300, 1300, 1300,    0,    0,    0,    0, 1301, 1301, 1301, 1301,\n",
      "        1301, 1302, 1302, 1303, 1304, 1304,    0,    0,    0,    0,    0,    0,\n",
      "           0, 1305, 1305,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,   73,   73,   74,   74,    0,    0,   75,   75,   75,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   76,   77,    0,    0,    0,\n",
      "          78,   79,   79,    0,    0,    0,   80,   80,   81,   81,    0,    0,\n",
      "           0,    0,    0,   82,   82,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,  819,  819,    0,    0,    0,    0,    0,    0,  820,\n",
      "         820,  821,  821,  822,  822,  822,    0,  823,  823,    0,  824,  825,\n",
      "         825,  826,  827,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,  828,  828,  828,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,  680,  680,  681,  681,    0,    0,    0,\n",
      "           0,    0,    0,  682,  682,  683,  683,  684,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,  685,  685,  686,  687,  687,\n",
      "         688,  688,  689,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,  398,  398,  398,    0,    0,    0,    0,  399,\n",
      "         399,  400,  401,  401,    0,  402,  402,  403,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,  404,  404,  405,  405,    0,    0,\n",
      "           0,    0,  406,  406,  407,  407,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,  975,  975,  976,  977,    0,    0,    0,\n",
      "           0,    0,    0,    0,  978,  978,  978,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,  979,  979,  980,  980,  980,  981,    0,\n",
      "         981,  982,  983,  983,  984,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0, 1046, 1046, 1046, 1047, 1047, 1048, 1049, 1049, 1050, 1050, 1050,\n",
      "        1051, 1051, 1051, 1051, 1051, 1052, 1052, 1052, 1052, 1053, 1053, 1053,\n",
      "        1053,    0,    0,    0, 1054, 1054, 1055, 1055,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,  746,    0,    0,\n",
      "         747,  747,  747,  747,    0,    0,  748,  748,  748,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,  749,  749,    0,    0,    0,\n",
      "         750,  751,  751,  752,  753,  753,  754,    0,  755,  755,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0, 1285,    0, 1286, 1286,\n",
      "        1286,    0,    0,    0,    0,    0, 1287, 1288, 1288,    0,    0,    0,\n",
      "        1289, 1289, 1290, 1290, 1291, 1291,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0, 1292, 1293, 1293, 1294, 1294,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,  918,  918,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  919,  920,\n",
      "         920,  920,  921,  921,  922,  922,    0,    0,    0,    0,  923,  923,\n",
      "         923,  923,  923,  924,  925,  925,  926,  927,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "亲音帝施闻阀阁差养美开夫天无元专云扎艺木抱垃拉拦拌幸招坡披拨谷妥含邻岔肝肚肠龟免先丢舌竹迁乔伟传乒乓享店夜庙府底剂剂郊废净诞询该详建肃录隶居届评补初社识诉诊词译君哀亭亮度迹庭疮疯疫疤侍供使例版侄侦侧凭侨\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "preds = preds_s.clone()\n",
    "\n",
    "preds = preds.permute(1,0,2)\n",
    "print(preds.size())\n",
    "_,preds = preds.max(2)\n",
    "preds = preds.view(-1)\n",
    "print(preds)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)])) * batchSize\n",
    "sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "\n",
    "print(sim_preds)\n",
    "image_0 = v_image[0][0]\n",
    "image_0 = image_0.numpy()\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

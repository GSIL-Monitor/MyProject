{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 生成测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6,
     19,
     25
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 5 samples\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/vision/issues/81\n",
    "\n",
    "# 将图片数据写入到LMDB数据库存中\n",
    "\n",
    "import lmdb\n",
    "import glob\n",
    "# 创建数据库\n",
    "# import lmdb  # install lmdb by \"pip install lmdb\"\n",
    "# env = lmdb.open('./data/lmdb', map_size=511627776)\n",
    "# env = lmdb.open('./data/lmdb', map_size=511627776)\n",
    "# from genLineText import GenTextImage\n",
    "def checkImageIsValid(imageBin):\n",
    "    if imageBin is None:\n",
    "        return False\n",
    "    \n",
    "    imageBuf = np.frombuffer(imageBin, dtype=np.uint8)\n",
    "    img = cv2.imdecode(imageBuf, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return False\n",
    "    imgH, imgW = img.shape[0], img.shape[1]\n",
    "    if imgH * imgW == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def writeCache(env, cache):\n",
    "    with env.begin(write=True) as txn:\n",
    "        for k, v in cache.items():\n",
    "            txn.put(k.encode(), v)\n",
    "            \n",
    "def createDataset(outputPath, imagePathList, labelList, lexiconList=None, checkValid=True):\n",
    "    \"\"\"\n",
    "    Create LMDB dataset for CRNN training.\n",
    "    ARGS:\n",
    "        outputPath    : LMDB output path\n",
    "        imagePathList : list of image path\n",
    "        labelList     : list of corresponding groundtruth texts\n",
    "        lexiconList   : (optional) list of lexicon lists\n",
    "        checkValid    : if true, check the validity of every image\n",
    "    \"\"\"\n",
    "    # print (len(imagePathList) , len(labelList))\n",
    "    assert (len(imagePathList) == len(labelList))\n",
    "    nSamples = len(imagePathList)\n",
    "    \n",
    "    env = lmdb.open(outputPath, map_size=511627776)\n",
    "\n",
    "    cache = {}\n",
    "    cnt = 1\n",
    "    for i in range(nSamples):\n",
    "        imagePath = imagePathList[i]\n",
    "        label = labelList[i]\n",
    "        if not os.path.exists(imagePath):\n",
    "            print('%s does not exist' % imagePath)\n",
    "            continue\n",
    "        with open(imagePath, 'rb') as f:\n",
    "            imageBin = f.read()\n",
    "        \n",
    "        if checkValid:\n",
    "            if not checkImageIsValid(imageBin):\n",
    "                print('%s is not a valid image' % imagePath)\n",
    "                continue\n",
    "\n",
    "        imageKey = 'image-%09d' % cnt\n",
    "        labelKey = 'label-%09d' % cnt\n",
    "        cache[imageKey] = imageBin\n",
    "        cache[labelKey] = label.encode()\n",
    "        if lexiconList:\n",
    "            lexiconKey = 'lexicon-%09d' % cnt\n",
    "            cache[lexiconKey] = ' '.join(lexiconList[i]).encode()\n",
    "        if cnt % 1000 == 0:\n",
    "            writeCache(env, cache)\n",
    "            cache = {}\n",
    "            print('Written %d / %d' % (cnt, nSamples))\n",
    "        cnt += 1\n",
    "    nSamples = cnt - 1\n",
    "    cache['num-samples'] = str(nSamples).encode()\n",
    "    writeCache(env, cache)\n",
    "    print('Created dataset with %d samples' % nSamples)\n",
    "\n",
    "\n",
    "def read_text(path):\n",
    "    with open(path) as f:\n",
    "        text = f.read()\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# outputPath = './data/lmdb/train'   # 训练数据\n",
    "outputPath = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\lmdb'   # 测试数据\n",
    "path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\image\\\\*.jpg'\n",
    "imagePathList = glob.glob(path)\n",
    "imgLabelLists = []\n",
    "for p in imagePathList:\n",
    "    try:\n",
    "        label = p.split('\\\\')[-1].split('_')[0]\n",
    "        imgLabelLists.append((p,label))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "imgLabelList = sorted(imgLabelLists, key=lambda x: len(x[1]))\n",
    "imgPaths = [p[0] for p in imgLabelList]\n",
    "txtLists = [p[1] for p in imgLabelList]\n",
    "createDataset(outputPath, imgPaths, txtLists, lexiconList=None, checkValid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "# 注意 dataset.alignCollate 将图片转成了灰度图，后期看怎么修改一下。\n",
    "# collate_fn，是用来处理不同情况下的输入dataset的封装，一般采用默认即可，除非你自定义的数据读取输出非常少见\n",
    "import common.dataset as dataset\n",
    "path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\lmdb'\n",
    "train_dataset = dataset.lmdbDataset(root=path, transform=dataset.resizeNormalize((32,32)))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    sampler=None\n",
    "#     collate_fn=dataset.alignCollate(imgH=32, imgW=32, keep_ratio=False)\n",
    ")\n",
    "\n",
    "# dataset 方法resizeNormalize 中用了transforms.ToTensor 会将数据做归一化处理，在正式用的时候也需要将数据调用该方法做归一化处理\n",
    "\n",
    "# 可参看 https://blog.csdn.net/victoriaw/article/details/72822005 数据预处理torchvision.transforms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 0.6196,  0.6078,  0.6039,  ...,  0.6039,  0.6000,  0.5725],\n",
      "          [ 0.6275,  0.6235,  0.6353,  ...,  0.5961,  0.6235,  0.6118],\n",
      "          [ 0.6157,  0.6392,  0.6235,  ...,  0.6118,  0.5922,  0.6196],\n",
      "          ...,\n",
      "          [ 0.5098,  0.5961,  0.6000,  ...,  0.5647,  0.5255,  0.5686],\n",
      "          [ 0.5765,  0.5843,  0.5569,  ...,  0.6157,  0.5569,  0.6235],\n",
      "          [ 0.5804,  0.5725,  0.5843,  ...,  0.6078,  0.5882,  0.5569]],\n",
      "\n",
      "         [[ 0.6431,  0.6314,  0.6275,  ...,  0.6392,  0.6353,  0.6078],\n",
      "          [ 0.6510,  0.6471,  0.6588,  ...,  0.6196,  0.6588,  0.6471],\n",
      "          [ 0.6392,  0.6627,  0.6471,  ...,  0.6353,  0.6275,  0.6549],\n",
      "          ...,\n",
      "          [ 0.5333,  0.6196,  0.6235,  ...,  0.5765,  0.5373,  0.5804],\n",
      "          [ 0.6000,  0.6078,  0.5804,  ...,  0.6275,  0.5686,  0.6353],\n",
      "          [ 0.6039,  0.5961,  0.6078,  ...,  0.6196,  0.6000,  0.5686]],\n",
      "\n",
      "         [[ 0.6627,  0.6510,  0.6471,  ...,  0.6549,  0.6510,  0.6235],\n",
      "          [ 0.6706,  0.6667,  0.6784,  ...,  0.6392,  0.6745,  0.6627],\n",
      "          [ 0.6588,  0.6824,  0.6667,  ...,  0.6549,  0.6431,  0.6706],\n",
      "          ...,\n",
      "          [ 0.5608,  0.6392,  0.6431,  ...,  0.5961,  0.5569,  0.6000],\n",
      "          [ 0.6275,  0.6353,  0.6078,  ...,  0.6471,  0.5882,  0.6549],\n",
      "          [ 0.6314,  0.6235,  0.6353,  ...,  0.6392,  0.6196,  0.5882]]]]), ('1',)]\n",
      "2\n",
      "(1, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "for idx,v in enumerate(train_loader):\n",
    "#     print(idx,v)\n",
    "#     print(v)\n",
    "    pass\n",
    "print(v)\n",
    "print(idx)\n",
    "print(np.array(v[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_1 = nn.Sequential(         # input shape (1, 32, 32)\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 16, 16)\n",
    "        )        \n",
    "        \n",
    "        self.conv_2 = nn.Sequential(         # input shape (16, 16, 16)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 16, 16)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 8, 8)\n",
    "        )        \n",
    "        \n",
    "        self.out = nn.Linear(32 * 8 * 8, 2)   # fully connected layer, output 2 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 8 * 8)\n",
    "        output = self.out(x)\n",
    "        return output       # return x for visualization    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss --> 0.27750226855278015\n",
      "loss --> 0.005474110133945942\n",
      "loss --> 0.0012895716354250908\n",
      "loss --> 0.0006567016243934631\n",
      "loss --> 0.0004239852132741362\n",
      "loss --> 0.0003070026286877692\n",
      "loss --> 0.0002378229983150959\n",
      "loss --> 0.00019256227824371308\n",
      "loss --> 0.0001609015162102878\n",
      "loss --> 0.00013762549497187138\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/tianweidadada/article/details/82630735   用 pytorch 进行分类（二分类，多分类）\n",
    "net = CNN()\n",
    "opitmizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "loss_fun = nn.MSELoss() \n",
    "epoches = 1000\n",
    "\n",
    "\n",
    "for i in range(epoches):\n",
    "    for step, values in enumerate(train_loader):\n",
    "        images = values[0]\n",
    "        # 二分类，target 在做损失的时候需要（0，1），（1，0）这样的格式\n",
    "        target = [ [1-int(x),int(x) ] for x in values[1]]\n",
    "        target =  Variable(torch.FloatTensor(target)) #变成 1*2的 tensor\n",
    "        preds = F.softmax(net(images),dim=1)\n",
    "        loss = loss_fun(preds,target)\n",
    "        opitmizer.zero_grad()\n",
    "        loss.backward()\n",
    "        opitmizer.step()\n",
    "    if i%100 == 0:\n",
    "        print('loss --> {}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "tensor([[ 0.0774,  0.9226]])\n",
      "time --> 0.009992837905883789\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms \n",
    "import time\n",
    "path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\image\\\\1_2.jpg'\n",
    "image = cv2.imread(path,cv2.IMREAD_COLOR)    \n",
    "if image.shape[0] != 32 or image.shape[1] != 32:\n",
    "    image = cv2.resize(image,(32,32))\n",
    "# aa[np.newaxis,:].shape, newaxis增加维度\n",
    "# np.r_[bb,bb].shape 添加行数据\n",
    "# image = image[np.newaxis,:]\n",
    "print(image.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(1):\n",
    "\n",
    "    imdata = transforms.ToTensor()(image)\n",
    "    imdata = imdata.unsqueeze(0)\n",
    "#     print(imdata.size())\n",
    "    preds = net(imdata)\n",
    "    preds = F.softmax(preds,dim=1)\n",
    "    print(preds)\n",
    "    \n",
    "print('time --> {}'.format((time.time()-start_time)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #  pytorch 基本网络操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  Word Embedding  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    http://www.pytorchtutorial.com/10-minute-pytorch-7/\n",
    "    \n",
    "    自然语言处理中词向量是很重要的，首先介绍一下词向量。\n",
    "    \n",
    "    之前做分类问题的时候大家应该都还记得我们会使用one-hot编码，比如一共有5类，那么属于第二类的话，它的编码就是(0, 1, 0, 0, 0)，对于分类问题，这样当然特别简明，但是对于单词，这样做就不行了，比如有1000个不同的词，那么使用one-hot这样的方法效率就很低了，所以我们必须要使用另外一种方式去定义每一个单词，这就引出了word embedding。\n",
    "    假如我们使用一个二维向量(a, b)来定义一个词，其中a，b分别代表这个词的一种属性，比如a代表是否喜欢玩飞盘，b代表是否喜欢玩毛线，并且这个数值越大表示越喜欢，这样我们就可以区分这三个词了，为什么呢？\n",
    "    比如对于cat，它的词向量就是(-1, 4)，对于kitty，它的词向量就是(-2, 5)，对于dog，它的词向量就是(3, -2)，对于boy，它的词向量就是(-2, -3)，我们怎么去定义他们之间的相似度呢，我们可以通过他们之间的夹角来定义他们的相似度。\n",
    "    对于一个词，我们自己去想它的属性不是很困难吗，所以这个时候就可以交给神经网络了，我们只需要定义我们想要的维度，比如100，然后通过神经网络去学习它的每一个属性的大小，而我们并不用关心到底这个属性代表着什么，我们只需要知道词向量的夹角越小，表示他们之间的语义更加接近。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0])\n",
      "tensor([[-1.6342, -0.0521,  1.1251,  0.4128, -1.3451]])\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {'hello': 0, 'world': 1}\n",
    "# 接着就是word embedding的定义nn.Embedding(2, 5)，这里的2表示有2个词，5表示5维，其实也就是一个2×5的矩阵，\n",
    "# 所以如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)。\n",
    "embeds = nn.Embedding(2, 5)\n",
    "hello_idx = torch.LongTensor([word_to_ix['hello']])\n",
    "hello_idx = Variable(hello_idx)\n",
    "print(hello_idx)\n",
    "hello_embed = embeds(hello_idx)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# GRU   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://blog.csdn.net/thinking_boy1992/article/details/53184237\n",
    "    https://www.datasciencecentral.com/profiles/blogs/gru-implementation-in-tensorflow\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    GRUs are one of the simplest RNNs. Vanilla RNNs are even simpler, but these models suffer from the Vanishing Gradient problem.在基本的LSTM框架下存在一些变体。一个通常的做法是构建窥视孔链接，它允许门不仅仅依赖于之前的隐含层状态St-1,而且依赖于先前的内部状态Ct-1,在门等式上添加一个新项；这里有很多变体，https://arxiv.org/pdf/1503.04069.pdf\n",
    "![Image of Yaktocat](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm.png)   \n",
    "    \n",
    "    GRU层背后的思想和LSTM层背后的思想相似，等式如下：\n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20161116191517555)      \n",
    "    \n",
    "    GRU有两个门,一个重置门r和一个更新门，直观的，重置门决定了如何把新的输入与之前的记忆相结合，更新门决定多少先前的记忆起作用。如果我们把所有reset设置为全1，更新门设置为全0，又达到了普通RNN的形式；使用一个门机制学习长距离依赖的基本思想与LSTM相同，但是有如下不同点： \n",
    "    GRU有两个门 ，LSTM有三个门； \n",
    "    GRU不能处理 \n",
    "\n",
    "    输入和遗忘门能够被更新门z耦合，重置门r能够直接应用于之前的隐含状态。因此，重置门的职责在LSTM中被拆分为r和z; \n",
    "![Image of Yaktocat](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png)\n",
    "\n",
    "    GRU VS LSTM \n",
    "    现在，有两个模型来解决梯度消失的问题，哪个更有效呢？GRUs相当的新，对它的评价没有被完全的探索；根据经验进行评价，相关文章http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf“>part1，part2没有明确的胜利者。在许多任务中，两个模型能产生相当的表现，看起来选择像层数这样的超参数相比与选择框架更重要。GRUs拥有更少的参数，可能训练的更快些或需要更少的数据来训练；如果你有足够多的数据，具有更强的表达能力的LASTs可能导致更好的结果 \n",
    "    \n",
    "    https://zhuanlan.zhihu.com/p/37217734 PyTorch中的循环神经网络相关函数的使用及记录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 40])\n",
      "torch.Size([1, 1, 40])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个输入输出50维的GRU\n",
    "gru = nn.GRU(input_size=50, hidden_size=50, batch_first=True)\n",
    "# 我们将单词与数字一一对应，那么有:\n",
    "# i -> 0\n",
    "# love -> 1\n",
    "# nlp -> 2\n",
    "\n",
    "# 也就是将字符变成了[0, 1, 2]这个列表创建一个字典大小为3，词向量维度为50维的Embedding。\n",
    "embed = nn.Embedding(3, 50) \n",
    "# 创建一个二维LongTensor索引数据（作为索引数据的时候一般都使用LongTensor）\n",
    "x = torch.LongTensor([[0, 1, 2]])  # x.size() --> torch.Size([1, 3])\n",
    "# 将索引映射到向量\n",
    "x_embed = embed(x)  # x_embed.size() --> torch.Size([1, 3, 50])\n",
    "\n",
    "# 单条数据时\n",
    "out, hidden = gru(x_embed)\n",
    "print(out.size())\n",
    "#  =======> torch.Size([1, 3, 50])  循环神经网络会自适应序列长度。比如这里使用的是长度3的序列，那么输出的序列也是长度为3\n",
    "# 输出的out包括了每一个输入位所对应的输出。因而输入输出的格式相同\n",
    "print(hidden.size())\n",
    "#  =======> torch.Size([1, 1, 50] 输出的隐含层向量只返回最后一层所对应的隐含层输出。这是设计的原因是很多时候只需要使用最后\n",
    "# 一层的隐含层输出。比如Sequence to Sequence模型中的encoder一般就只传递最后一层的隐层输出给decoder。如果我们需要使用到中间\n",
    "# 层的隐含层怎么？其实隐含层输出和输出层输出的数值是一样的，唯一的区别就是他们的输出格式可能有区别。在后面会继续说明，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SoftMax 与 LogSoftMax\n",
    "\n",
    "    https://blog.csdn.net/lanchunhui/article/details/51248184 softmax 及 logsoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# torch.matmul和torch.bmm，都能实现对于batch的矩阵乘法\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5612,  3.2501],\n",
      "         [ 2.1804,  3.4205],\n",
      "         [ 2.6273,  3.2887]],\n",
      "\n",
      "        [[ 2.4362,  2.9144],\n",
      "         [ 0.9368,  1.1704],\n",
      "         [ 2.4260,  2.7505]]])\n",
      "tensor([[[ 2.5612,  3.2501],\n",
      "         [ 2.1804,  3.4205],\n",
      "         [ 2.6273,  3.2887]],\n",
      "\n",
      "        [[ 2.4362,  2.9144],\n",
      "         [ 0.9368,  1.1704],\n",
      "         [ 2.4260,  2.7505]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3,10))\n",
    "b = torch.rand((2,2,10))\n",
    "### matmal()\n",
    "res1 = torch.matmul(a,b.transpose(1,2))\n",
    "print(res1)\n",
    "\n",
    "res2 = torch.bmm(a,b.transpose(1,2))\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   2.,   4.],\n",
      "        [  6.,   8.,  10.]])\n",
      "<class 'tuple'>\n",
      "tensor([[  4.,   2.],\n",
      "        [ 10.,   8.]])\n",
      "(tensor([[  4.,   2.],\n",
      "        [ 10.,   8.]]), tensor([[ 2,  1],\n",
      "        [ 2,  1]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0, 10, 6).view(2, 3)\n",
    "print(a)\n",
    "print(type(a.topk(1)))\n",
    "print(a.topk(2)[0])\n",
    "print(a.topk(2))  # 返回 值和 值所在索引号"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
